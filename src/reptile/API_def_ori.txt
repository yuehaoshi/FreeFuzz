paddle.abs(x,          name=None)[source] 
paddle.acos(x,          name=None)[source] 
paddle.add(x,          y,          name=None)[source] 
paddle.add_n(inputs,          name=None)[source] 
paddle.addmm(input,          x,          y,          beta=1.0,          alpha=1.0,          name=None)[source] 
paddle.all(x,          axis=None,          keepdim=False,          name=None)[source] 
paddle.allclose(x,          y,          rtol=1e-05,          atol=1e-08,          equal_nan=False,          name=None)[source] 
paddle.any(x,          axis=None,          keepdim=False,          name=None)[source] 
paddle.arange(start=0,          end=None,          step=1,          dtype=None,          name=None)[source] 
paddle.argmax(x,          axis=None,          keepdim=False,          dtype='int64',          name=None)[source] 
paddle.argmin(x,          axis=None,          keepdim=False,          dtype='int64',          name=None)[source] 
paddle.argsort(x,          axis=- 1,          descending=False,          name=None)[source] 
paddle.asin(x,          name=None)[source] 
paddle.assign(x,          output=None)[source] 
paddle.atan(x,          name=None)[source] 
paddle.atan2(x,          y,          name=None)[source] 
paddle.batch(reader,          batch_size,          drop_last=False)[source] 
paddle.bernoulli(x,          name=None)[source] 
paddle.bincount(x,          weights=None,          minlength=0,          name=None)[source] 
paddle.bitwise_and(x,          y,          out=None,          name=None)[source] 
paddle.bitwise_not(x,          out=None,          name=None)[source] 
paddle.bitwise_or(x,          y,          out=None,          name=None)[source] 
paddle.bitwise_xor(x,          y,          out=None,          name=None)[source] 
paddle.bmm(x,          y,          name=None)[source] 
paddle.broadcast_shape(x_shape,          y_shape)[source] 
paddle.broadcast_tensors(input,          name=None)[source] 
paddle.broadcast_to(x,          shape,          name=None)[source] 
paddle.cast(x,          dtype)[source] 
paddle.ceil(x,          name=None)[source] 
paddle.check_shape(shape,          op_name,          expected_shape_type=(<class 'list'>,          <class 'tuple'>,          <class 'paddle.fluid.framework.Variable'>),          expected_element_type=(<class 'int'>,          <class 'paddle.fluid.framework.Variable'>),          expected_tensor_dtype=('int32',          'int64'))[source] 
paddle.cholesky(x,          upper=False,          name=None)[source] 
paddle.chunk(x,          chunks,          axis=0,          name=None)[source] 
paddle.clip(x,          min=None,          max=None,          name=None)[source] 
paddle.concat(x,          axis=0,          name=None)[source] 
paddle.conj(x,          name=None)[source] 
paddle.cos(x,          name=None)[source] 
paddle.cosh(x,          name=None)[source] 
class paddle.CPUPlace 
paddle.create_parameter(shape,          dtype,          name=None,          attr=None,          is_bias=False,          default_initializer=None)[source] 
paddle.crop(x,          shape=None,          offsets=None,          name=None)[source] 
paddle.cross(x,          y,          axis=None,          name=None)[source] 
class paddle.CUDAPinnedPlace 
class paddle.CUDAPlace get_device_id(self: paddle.fluid.core_avx.CUDAPlace)→ intget_device_id¶ 
paddle.cumprod(x,          dim=None,          dtype=None,          name=None)[source] 
paddle.cumsum(x,          axis=None,          dtype=None,          name=None)[source] 
class paddle.DataParallel(layers,          strategy=None,          comm_buffer_size=25,          last_comm_buffer_size=1,          find_unused_parameters=False)[source] no_sync()no_sync¶ forward(*inputs,            **kwargs)forward¶ scale_loss(loss)scale_loss¶ apply_collective_grads()apply_collective_grads¶ state_dict(destination=None,            include_sublayers=True,            structured_name_prefix='')state_dict¶ add_parameter(name,            parameter)add_parameter¶ add_sublayer(name,            sublayer)add_sublayer¶ apply(fn)apply¶ buffers(include_sublayers=True)buffers¶ children()children¶ clear_gradients()clear_gradients¶ create_parameter(shape,            attr=None,            dtype=None,            is_bias=False,            default_initializer=None)[source]create_parameter¶ create_tensor(name=None,            persistable=None,            dtype=None)create_tensor¶ create_variable(name=None,            persistable=None,            dtype=None)create_variable¶ eval()eval¶ extra_repr()extra_repr¶ full_name()full_name¶ named_buffers(prefix='',            include_sublayers=True)named_buffers¶ named_children()named_children¶ named_parameters(prefix='',            include_sublayers=True)named_parameters¶ named_sublayers(prefix='',            include_self=False,            layers_set=None)named_sublayers¶ parameters(include_sublayers=True)parameters¶ register_buffer(name,            tensor,            persistable=True)register_buffer¶ register_forward_post_hook(hook)register_forward_post_hook¶ register_forward_pre_hook(hook)register_forward_pre_hook¶ set_state_dict(state_dict,            use_structured_name=True)set_state_dict¶ sublayers(include_self=False)sublayers¶ to(device=None,            dtype=None,            blocking=None)to¶ to_static_state_dict(destination=None,            include_sublayers=True,            structured_name_prefix='')to_static_state_dict¶ train()train¶ set_dict(state_dict,            use_structured_name=True)set_dict¶ load_dict(state_dict,            use_structured_name=True)load_dict¶ 
paddle.diag(x,          offset=0,          padding_value=0,          name=None)[source] 
paddle.diagflat(x,          offset=0,          name=None)[source] 
paddle.diagonal(x,          offset=0,          axis1=0,          axis2=1,          name=None)[source] 
paddle.digamma(x,          name=None)[source] 
paddle.disable_signal_handler()[source] 
paddle.disable_static(place=None)[source] 
paddle.dist(x,          y,          p=2,          name=None)[source] 
paddle.divide(x,          y,          name=None)[source] 
paddle.dot(x,          y,          name=None)[source] 
paddle.dtype 
paddle.einsum(equation,          *operands)[source] 
paddle.empty(shape,          dtype=None,          name=None)[source] 
paddle.empty_like(x,          dtype=None,          name=None)[source] 
paddle.enable_static()[source] 
paddle.equal(x,          y,          name=None)[source] 
paddle.equal_all(x,          y,          name=None)[source] 
paddle.erf(x,          name=None)[source] 
paddle.exp(x,          name=None)[source] 
paddle.expand(x,          shape,          name=None)[source] 
paddle.expand_as(x,          y,          name=None)[source] 
paddle.expm1(x,          name=None)[source] 
paddle.eye(num_rows,          num_columns=None,          dtype=None,          name=None)[source] 
paddle.flatten(x,          start_axis=0,          stop_axis=- 1,          name=None)[source] 
paddle.flip(x,          axis,          name=None)[source] 
paddle.floor(x,          name=None)[source] 
paddle.floor_divide(x,          y,          name=None)[source] 
paddle.floor_mod(x,          y,          name=None)[source] 
paddle.flops(net,          input_size,          custom_ops=None,          print_detail=False)[source] 
paddle.full(shape,          fill_value,          dtype=None,          name=None)[source] 
paddle.full_like(x,          fill_value,          dtype=None,          name=None)[source] 
paddle.gather(x,          index,          axis=None,          name=None)[source] 
paddle.gather_nd(x,          index,          name=None)[source] 
paddle.get_cuda_rng_state()[source] 
paddle.get_default_dtype()[source] 
paddle.get_flags(flags)[source] 
paddle.grad(outputs,          inputs,          grad_outputs=None,          retain_graph=None,          create_graph=False,          only_inputs=True,          allow_unused=False,          no_grad_vars=None)[source] 
paddle.greater_equal(x,          y,          name=None)[source] 
paddle.greater_than(x,          y,          name=None)[source] 
paddle.histogram(input,          bins=100,          min=0,          max=0,          name=None)[source] 
paddle.imag(x,          name=None)[source] 
paddle.in_dynamic_mode() 
paddle.increment(x,          value=1.0,          name=None)[source] 
paddle.index_sample(x,          index)[source] 
paddle.index_select(x,          index,          axis=0,          name=None)[source] 
paddle.inverse(x,          name=None)[source] 
paddle.is_empty(x,          name=None)[source] 
paddle.is_tensor(x)[source] 
paddle.isfinite(x,          name=None)[source] 
paddle.isinf(x,          name=None)[source] 
paddle.isnan(x,          name=None)[source] 
paddle.kron(x,          y,          name=None)[source] 
paddle.less_equal(x,          y,          name=None)[source] 
paddle.less_than(x,          y,          name=None)[source] 
paddle.lgamma(x,          name=None)[source] 
paddle.linspace(start,          stop,          num,          dtype=None,          name=None)[source] 
paddle.load(path,          **configs)[source] 
paddle.log(x,          name=None)[source] 
paddle.log10(x,          name=None)[source] 
paddle.log1p(x,          name=None)[source] 
paddle.log2(x,          name=None)[source] 
paddle.logical_and(x,          y,          out=None,          name=None)[source] 
paddle.logical_not(x,          out=None,          name=None)[source] 
paddle.logical_or(x,          y,          out=None,          name=None)[source] 
paddle.logical_xor(x,          y,          out=None,          name=None)[source] 
paddle.logsumexp(x,          axis=None,          keepdim=False,          name=None)[source] 
paddle.masked_select(x,          mask,          name=None)[source] 
paddle.matmul(x,          y,          transpose_x=False,          transpose_y=False,          name=None)[source] 
paddle.max(x,          axis=None,          keepdim=False,          name=None)[source] 
paddle.maximum(x,          y,          name=None)[source] 
paddle.mean(x,          axis=None,          keepdim=False,          name=None)[source] 
paddle.median(x,          axis=None,          keepdim=False,          name=None)[source] 
paddle.meshgrid(*args,          **kwargs)[source] 
paddle.min(x,          axis=None,          keepdim=False,          name=None)[source] 
paddle.minimum(x,          y,          name=None)[source] 
paddle.mm(input,          mat2,          name=None)[source] 
class paddle.Model(network,          inputs=None,          labels=None)[source] train_batch(inputs,            labels=None,            update=True)train_batch¶ eval_batch(inputs,            labels=None)eval_batch¶ predict_batch(inputs)predict_batch¶ save(path,            training=True)[source]save¶ load(path,            skip_mismatch=False,            reset_optimizer=False)[source]load¶ parameters(*args,            **kwargs)parameters¶ prepare(optimizer=None,            loss=None,            metrics=None,            amp_configs=None)prepare¶ fit(train_data=None,            eval_data=None,            batch_size=1,            epochs=1,            eval_freq=1,            log_freq=10,            save_dir=None,            save_freq=1,            verbose=2,            drop_last=False,            shuffle=True,            num_workers=0,            callbacks=None,            accumulate_grad_batches=1,            num_iters=None)fit¶ evaluate(eval_data,            batch_size=1,            log_freq=10,            verbose=2,            num_workers=0,            callbacks=None,            num_iters=None)evaluate¶ predict(test_data,            batch_size=1,            num_workers=0,            stack_outputs=False,            verbose=1,            callbacks=None)predict¶ summary(input_size=None,            dtype=None)[source]summary¶ 
paddle.multinomial(x,          num_samples=1,          replacement=False,          name=None)[source] 
paddle.multiplex(inputs,          index,          name=None)[source] 
paddle.multiply(x,          y,          name=None)[source] 
paddle.mv(x,          vec,          name=None)[source] 
paddle.neg(x,          name=None)[source] 
paddle.no_grad[source] 
paddle.nonzero(x,          as_tuple=False)[source] 
paddle.norm(x,          p='fro',          axis=None,          keepdim=False,          name=None)[source] 
paddle.normal(mean=0.0,          std=1.0,          shape=None,          name=None)[source] 
paddle.not_equal(x,          y,          name=None)[source] 
class paddle.NPUPlace get_device_id(self: paddle.fluid.core_avx.NPUPlace)→ intget_device_id¶ 
paddle.numel(x,          name=None)[source] 
paddle.ones(shape,          dtype=None,          name=None)[source] 
paddle.ones_like(x,          dtype=None,          name=None)[source] 
class paddle.ParamAttr(name=None,          initializer=None,          learning_rate=1.0,          regularizer=None,          trainable=True,          do_model_average=True,          need_clip=True)[source] 
paddle.pow(x,          y,          name=None)[source] 
paddle.prod(x,          axis=None,          keepdim=False,          dtype=None,          name=None)[source] 
paddle.rand(shape,          dtype=None,          name=None)[source] 
paddle.randint(low=0,          high=None,          shape=[1],          dtype=None,          name=None)[source] 
paddle.randn(shape,          dtype=None,          name=None)[source] 
paddle.randperm(n,          dtype='int64',          name=None)[source] 
paddle.rank(input)[source] 
paddle.real(x,          name=None)[source] 
paddle.reciprocal(x,          name=None)[source] 
paddle.reshape(x,          shape,          name=None)[source] 
paddle.reshape_(x,          shape,          name=None)[source] 
paddle.roll(x,          shifts,          axis=None,          name=None)[source] 
paddle.round(x,          name=None)[source] 
paddle.rsqrt(x,          name=None)[source] 
paddle.save(obj,          path,          protocol=4,          **configs)[source] 
paddle.scale(x,          scale=1.0,          bias=0.0,          bias_after_scale=True,          act=None,          name=None)[source] 
paddle.scatter(x,          index,          updates,          overwrite=True,          name=None)[source] 
paddle.scatter_(x,          index,          updates,          overwrite=True,          name=None)[source] 
paddle.scatter_nd(index,          updates,          shape,          name=None)[source] 
paddle.scatter_nd_add(x,          index,          updates,          name=None)[source] 
paddle.searchsorted(sorted_sequence,          values,          out_int32=False,          right=False,          name=None)[source] 
paddle.seed(seed)[source] 
paddle.set_cuda_rng_state(state_list)[source] 
paddle.set_default_dtype(d)[source] 
paddle.set_flags(flags)[source] 
paddle.set_grad_enabled(mode)[source] 
paddle.set_printoptions(precision=None,          threshold=None,          edgeitems=None,          sci_mode=None,          linewidth=None)[source] 
paddle.shape(input)[source] 
paddle.shard_index(input,          index_num,          nshards,          shard_id,          ignore_value=- 1)[source] 
paddle.sign(x,          name=None)[source] 
paddle.sin(x,          name=None)[source] 
paddle.sinh(x,          name=None)[source] 
paddle.slice(input,          axes,          starts,          ends)[source] 
paddle.sort(x,          axis=- 1,          descending=False,          name=None)[source] 
paddle.split(x,          num_or_sections,          axis=0,          name=None)[source] 
paddle.sqrt(x,          name=None)[source] 
paddle.square(x,          name=None)[source] 
paddle.squeeze(x,          axis=None,          name=None)[source] 
paddle.squeeze_(x,          axis=None,          name=None)[source] 
paddle.stack(x,          axis=0,          name=None)[source] 
paddle.standard_normal(shape,          dtype=None,          name=None)[source] 
paddle.stanh(x,          scale_a=0.67,          scale_b=1.7159,          name=None)[source] 
paddle.std(x,          axis=None,          unbiased=True,          keepdim=False,          name=None)[source] 
paddle.strided_slice(x,          axes,          starts,          ends,          strides,          name=None)[source] 
paddle.subtract(x,          y,          name=None)[source] 
paddle.sum(x,          axis=None,          dtype=None,          keepdim=False,          name=None)[source] 
paddle.summary(net,          input_size=None,          dtypes=None,          input=None)[source] 
paddle.t(input,          name=None)[source] 
paddle.tan(x,          name=None)[source] 
paddle.tanh(x,          name=None)[source] 
paddle.tanh_(x,          name=None)[source] 
class paddle.Tensor abs(name=None)[source]abs¶ acos(name=None)[source]acos¶ add(y,            name=None)[source]add¶ add_(y,            name=None)add_¶ add_n(name=None)[source]add_n¶ addmm(x,            y,            beta=1.0,            alpha=1.0,            name=None)[source]addmm¶ all(axis=None,            keepdim=False,            name=None)[source]all¶ allclose(y,            rtol=1e-05,            atol=1e-08,            equal_nan=False,            name=None)[source]allclose¶ any(axis=None,            keepdim=False,            name=None)[source]any¶ argmax(axis=None,            keepdim=False,            dtype='int64',            name=None)[source]argmax¶ argmin(axis=None,            keepdim=False,            dtype='int64',            name=None)[source]argmin¶ argsort(axis=- 1,            descending=False,            name=None)[source]argsort¶ asin(name=None)[source]asin¶ astype(dtype)astype¶ atan(name=None)[source]atan¶ backward(grad_tensor=None,            retain_graph=False)backward¶ bincount(weights=None,            minlength=0,            name=None)[source]bincount¶ bitwise_and(y,            out=None,            name=None)[source]bitwise_and¶ bitwise_not(out=None,            name=None)[source]bitwise_not¶ bitwise_or(y,            out=None,            name=None)[source]bitwise_or¶ bitwise_xor(y,            out=None,            name=None)[source]bitwise_xor¶ bmm(y,            name=None)[source]bmm¶ broadcast_shape(y_shape)[source]broadcast_shape¶ broadcast_tensors(name=None)[source]broadcast_tensors¶ broadcast_to(shape,            name=None)[source]broadcast_to¶ cast(dtype)[source]cast¶ ceil(name=None)[source]ceil¶ ceil_(name=None)ceil_¶ cholesky(upper=False,            name=None)[source]cholesky¶ chunk(chunks,            axis=0,            name=None)[source]chunk¶ clear_grad()clear_grad¶ clear_gradient(self: paddle.fluid.core_avx.VarBase)→ Noneclear_gradient¶ clip(min=None,            max=None,            name=None)[source]clip¶ clip_(min=None,            max=None,            name=None)clip_¶ clone(self: paddle.fluid.core_avx.VarBase)→ paddle.fluid.core_avx.VarBaseclone¶ concat(axis=0,            name=None)[source]concat¶ cond(p=None,            name=None)cond¶ conj(name=None)[source]conj¶ copy_(self: paddle.fluid.core_avx.VarBase,            arg0: paddle.fluid.core_avx.VarBase,            arg1: bool)→ Nonecopy_¶ cos(name=None)[source]cos¶ cosh(name=None)[source]cosh¶ cpu(self: paddle.fluid.core_avx.VarBase)→ paddle.fluid.core_avx.VarBasecpu¶ cross(y,            axis=None,            name=None)[source]cross¶ cuda(self: paddle.fluid.core_avx.VarBase,            device_id: handle = None,            blocking: bool = True)→ paddle.fluid.core_avx.VarBasecuda¶ cumprod(dim=None,            dtype=None,            name=None)[source]cumprod¶ cumsum(axis=None,            dtype=None,            name=None)[source]cumsum¶ detach(self: paddle.fluid.core_avx.VarBase)→ paddle.fluid.core_avx.VarBasedetach¶ diagonal(offset=0,            axis1=0,            axis2=1,            name=None)[source]diagonal¶ digamma(name=None)[source]digamma¶ dist(y,            p=2,            name=None)[source]dist¶ divide(y,            name=None)[source]divide¶ dot(y,            name=None)[source]dot¶ eig(name=None)eig¶ eigvals(name=None)eigvals¶ eigvalsh(UPLO='L',            name=None)[source]eigvalsh¶ equal(y,            name=None)[source]equal¶ equal_all(y,            name=None)[source]equal_all¶ erf(name=None)[source]erf¶ exp(name=None)[source]exp¶ exp_(name=None)exp_¶ expand(shape,            name=None)[source]expand¶ expand_as(y,            name=None)[source]expand_as¶ fill_(value)fill_¶ fill_diagonal_(value,            offset=0,            wrap=False,            name=None)fill_diagonal_¶ fill_diagonal_tensor(y,            offset=0,            dim1=0,            dim2=1,            name=None)fill_diagonal_tensor¶ fill_diagonal_tensor_(y,            offset=0,            dim1=0,            dim2=1,            name=None)fill_diagonal_tensor_¶ flatten(start_axis=0,            stop_axis=- 1,            name=None)[source]flatten¶ flatten_(start_axis=0,            stop_axis=- 1,            name=None)flatten_¶ flip(axis,            name=None)[source]flip¶ floor(name=None)[source]floor¶ floor_(name=None)floor_¶ floor_divide(y,            name=None)[source]floor_divide¶ floor_mod(y,            name=None)[source]floor_mod¶ gather(index,            axis=None,            name=None)[source]gather¶ gather_nd(index,            name=None)[source]gather_nd¶ property grad[source] gradient()gradient¶ greater_equal(y,            name=None)[source]greater_equal¶ greater_than(y,            name=None)[source]greater_than¶ histogram(bins=100,            min=0,            max=0,            name=None)[source]histogram¶ imag(name=None)[source]imag¶ increment(value=1.0,            name=None)[source]increment¶ index_sample(index)[source]index_sample¶ index_select(index,            axis=0,            name=None)[source]index_select¶ property inplace_version inverse(name=None)[source]inverse¶ is_empty(name=None)[source]is_empty¶ property is_leaf is_tensor()[source]is_tensor¶ isfinite(name=None)[source]isfinite¶ isinf(name=None)[source]isinf¶ isnan(name=None)[source]isnan¶ item(*args)item¶ kron(y,            name=None)[source]kron¶ less_equal(y,            name=None)[source]less_equal¶ less_than(y,            name=None)[source]less_than¶ lgamma(name=None)[source]lgamma¶ log(name=None)[source]log¶ log10(name=None)[source]log10¶ log1p(name=None)[source]log1p¶ log2(name=None)[source]log2¶ logical_and(y,            out=None,            name=None)[source]logical_and¶ logical_not(out=None,            name=None)[source]logical_not¶ logical_or(y,            out=None,            name=None)[source]logical_or¶ logical_xor(y,            out=None,            name=None)[source]logical_xor¶ logsumexp(axis=None,            keepdim=False,            name=None)[source]logsumexp¶ masked_select(mask,            name=None)[source]masked_select¶ matmul(y,            transpose_x=False,            transpose_y=False,            name=None)[source]matmul¶ matrix_power(n,            name=None)matrix_power¶ max(axis=None,            keepdim=False,            name=None)[source]max¶ maximum(y,            name=None)[source]maximum¶ mean(axis=None,            keepdim=False,            name=None)[source]mean¶ median(axis=None,            keepdim=False,            name=None)[source]median¶ min(axis=None,            keepdim=False,            name=None)[source]min¶ minimum(y,            name=None)[source]minimum¶ mm(mat2,            name=None)[source]mm¶ mod(y,            name=None)[source]mod¶ multi_dot(name=None)multi_dot¶ multiplex(index,            name=None)[source]multiplex¶ multiply(y,            name=None)[source]multiply¶ mv(vec,            name=None)[source]mv¶ neg(name=None)[source]neg¶ nonzero(as_tuple=False)[source]nonzero¶ norm(p='fro',            axis=None,            keepdim=False,            name=None)[source]norm¶ not_equal(y,            name=None)[source]not_equal¶ numel(name=None)[source]numel¶ numpy(self: paddle.fluid.core_avx.VarBase)→ arraynumpy¶ pin_memory(self: paddle.fluid.core_avx.VarBase)→ paddle.fluid.core_avx.VarBasepin_memory¶ pow(y,            name=None)[source]pow¶ prod(axis=None,            keepdim=False,            dtype=None,            name=None)[source]prod¶ qr(mode='reduced',            name=None)qr¶ rank()[source]rank¶ real(name=None)[source]real¶ reciprocal(name=None)[source]reciprocal¶ reciprocal_(name=None)reciprocal_¶ register_hook(hook)register_hook¶ remainder(y,            name=None)[source]remainder¶ reshape(shape,            name=None)[source]reshape¶ reshape_(shape,            name=None)[source]reshape_¶ reverse(axis,            name=None)[source]reverse¶ roll(shifts,            axis=None,            name=None)[source]roll¶ round(name=None)[source]round¶ round_(name=None)round_¶ rsqrt(name=None)[source]rsqrt¶ rsqrt_(name=None)rsqrt_¶ scale(scale=1.0,            bias=0.0,            bias_after_scale=True,            act=None,            name=None)[source]scale¶ scale_(scale=1.0,            bias=0.0,            bias_after_scale=True,            act=None,            name=None)scale_¶ scatter(index,            updates,            overwrite=True,            name=None)[source]scatter¶ scatter_(index,            updates,            overwrite=True,            name=None)[source]scatter_¶ scatter_nd(updates,            shape,            name=None)[source]scatter_nd¶ scatter_nd_add(index,            updates,            name=None)[source]scatter_nd_add¶ set_value(value)set_value¶ shard_index(index_num,            nshards,            shard_id,            ignore_value=- 1)[source]shard_index¶ sign(name=None)[source]sign¶ sin(name=None)[source]sin¶ sinh(name=None)[source]sinh¶ slice(axes,            starts,            ends)[source]slice¶ solve(y,            name=None)solve¶ sort(axis=- 1,            descending=False,            name=None)[source]sort¶ split(num_or_sections,            axis=0,            name=None)[source]split¶ sqrt(name=None)[source]sqrt¶ sqrt_(name=None)sqrt_¶ square(name=None)[source]square¶ squeeze(axis=None,            name=None)[source]squeeze¶ squeeze_(axis=None,            name=None)[source]squeeze_¶ stack(axis=0,            name=None)[source]stack¶ stanh(scale_a=0.67,            scale_b=1.7159,            name=None)[source]stanh¶ std(axis=None,            unbiased=True,            keepdim=False,            name=None)[source]std¶ strided_slice(axes,            starts,            ends,            strides,            name=None)[source]strided_slice¶ subtract(y,            name=None)[source]subtract¶ subtract_(y,            name=None)subtract_¶ sum(axis=None,            dtype=None,            keepdim=False,            name=None)[source]sum¶ t(name=None)[source]t¶ tanh(name=None)[source]tanh¶ tanh_(name=None)[source]tanh_¶ tensordot(y,            axes=2,            name=None)[source]tensordot¶ tile(repeat_times,            name=None)[source]tile¶ tolist()[source]tolist¶ topk(k,            axis=None,            largest=True,            sorted=True,            name=None)[source]topk¶ trace(offset=0,            axis1=0,            axis2=1,            name=None)[source]trace¶ transpose(perm,            name=None)[source]transpose¶ trunc(name=None)[source]trunc¶ unbind(axis=0)[source]unbind¶ uniform_(min=- 1.0,            max=1.0,            seed=0,            name=None)uniform_¶ unique(return_index=False,            return_inverse=False,            return_counts=False,            axis=None,            dtype='int64',            name=None)[source]unique¶ unique_consecutive(return_inverse=False,            return_counts=False,            axis=None,            dtype='int64',            name=None)[source]unique_consecutive¶ unsqueeze(axis,            name=None)[source]unsqueeze¶ unsqueeze_(axis,            name=None)[source]unsqueeze_¶ unstack(axis=0,            num=None)[source]unstack¶ value(self: paddle.fluid.core_avx.VarBase)→ paddle::framework::Variablevalue¶ var(axis=None,            unbiased=True,            keepdim=False,            name=None)[source]var¶ where(x,            y,            name=None)[source]where¶ zero_()zero_¶ 
paddle.tensordot(x,          y,          axes=2,          name=None)[source] 
paddle.tile(x,          repeat_times,          name=None)[source] 
paddle.to_tensor(data,          dtype=None,          place=None,          stop_gradient=True)[source] 
paddle.tolist(x)[source] 
paddle.topk(x,          k,          axis=None,          largest=True,          sorted=True,          name=None)[source] 
paddle.trace(x,          offset=0,          axis1=0,          axis2=1,          name=None)[source] 
paddle.transpose(x,          perm,          name=None)[source] 
paddle.tril(x,          diagonal=0,          name=None)[source] 
paddle.triu(x,          diagonal=0,          name=None)[source] 
paddle.trunc(input,          name=None)[source] 
paddle.unbind(input,          axis=0)[source] 
paddle.uniform(shape,          dtype=None,          min=- 1.0,          max=1.0,          seed=0,          name=None)[source] 
paddle.unique(x,          return_index=False,          return_inverse=False,          return_counts=False,          axis=None,          dtype='int64',          name=None)[source] 
paddle.unique_consecutive(x,          return_inverse=False,          return_counts=False,          axis=None,          dtype='int64',          name=None)[source] 
paddle.unsqueeze(x,          axis,          name=None)[source] 
paddle.unsqueeze_(x,          axis,          name=None)[source] 
paddle.unstack(x,          axis=0,          num=None)[source] 
paddle.var(x,          axis=None,          unbiased=True,          keepdim=False,          name=None)[source] 
paddle.where(condition,          x,          y,          name=None)[source] 
paddle.zeros(shape,          dtype=None,          name=None)[source] 
paddle.zeros_like(x,          dtype=None,          name=None)[source] 
paddle.amp.auto_cast(enable=True,          custom_white_list=None,          custom_black_list=None,          level='O1')[source] 
paddle.amp.decorate(models,          optimizers=None,          level='O1',          master_weight=None,          save_dtype=None)[source] 
class paddle.amp.GradScaler(enable=True,          init_loss_scaling=32768.0,          incr_ratio=2.0,          decr_ratio=0.5,          incr_every_n_steps=1000,          decr_every_n_nan_or_inf=2,          use_dynamic_loss_scaling=True)[source] scale(var)scale¶ minimize(optimizer,            *args,            **kwargs)minimize¶ step(optimizer)step¶ update()update¶ unscale_(optimizer)unscale_¶ is_enable()is_enable¶ is_use_dynamic_loss_scaling()is_use_dynamic_loss_scaling¶ get_init_loss_scaling()get_init_loss_scaling¶ set_init_loss_scaling(new_init_loss_scaling)set_init_loss_scaling¶ get_incr_ratio()get_incr_ratio¶ set_incr_ratio(new_incr_ratio)set_incr_ratio¶ get_decr_ratio()get_decr_ratio¶ set_decr_ratio(new_decr_ratio)set_decr_ratio¶ get_incr_every_n_steps()get_incr_every_n_steps¶ set_incr_every_n_steps(new_incr_every_n_steps)set_incr_every_n_steps¶ get_decr_every_n_nan_or_inf()get_decr_every_n_nan_or_inf¶ set_decr_every_n_nan_or_inf(new_decr_every_n_nan_or_inf)set_decr_every_n_nan_or_inf¶ state_dict()state_dict¶ load_state_dict(state_dict)load_state_dict¶ 
paddle.autograd.backward(tensors,          grad_tensors=None,          retain_graph=False)[source] 
class paddle.autograd.PyLayer[source] static forward(ctx,            *args,            **kwargs)forward¶ static backward(ctx,            *args,            **kwargs)[source]backward¶ classmethod apply(*args,            **kwargs)apply¶ 
class paddle.autograd.PyLayerContext[source] save_for_backward(*tensors)save_for_backward¶ saved_tensor()saved_tensor¶ 
class paddle.callbacks.Callback[source] set_params(params)set_params¶ set_model(model)set_model¶ on_train_begin(logs=None)on_train_begin¶ on_train_end(logs=None)on_train_end¶ on_eval_begin(logs=None)on_eval_begin¶ on_eval_end(logs=None)on_eval_end¶ on_predict_begin(logs=None)on_predict_begin¶ on_predict_end(logs=None)on_predict_end¶ on_epoch_begin(epoch,            logs=None)on_epoch_begin¶ on_epoch_end(epoch,            logs=None)on_epoch_end¶ on_train_batch_begin(step,            logs=None)on_train_batch_begin¶ on_train_batch_end(step,            logs=None)on_train_batch_end¶ on_eval_batch_begin(step,            logs=None)on_eval_batch_begin¶ on_eval_batch_end(step,            logs=None)on_eval_batch_end¶ on_predict_batch_begin(step,            logs=None)on_predict_batch_begin¶ on_predict_batch_end(step,            logs=None)on_predict_batch_end¶ 
class paddle.callbacks.EarlyStopping(monitor='loss',          mode='auto',          patience=0,          verbose=1,          min_delta=0,          baseline=None,          save_best_model=True)[source] on_train_begin(logs=None)on_train_begin¶ on_eval_end(logs=None)on_eval_end¶ 
class paddle.callbacks.LRScheduler(by_step=True,          by_epoch=False)[source] on_epoch_end(epoch,            logs=None)on_epoch_end¶ on_train_batch_end(step,            logs=None)on_train_batch_end¶ 
class paddle.callbacks.ModelCheckpoint(save_freq=1,          save_dir=None)[source] on_epoch_begin(epoch=None,            logs=None)on_epoch_begin¶ on_epoch_end(epoch,            logs=None)on_epoch_end¶ on_train_end(logs=None)on_train_end¶ 
class paddle.callbacks.ProgBarLogger(log_freq=1,          verbose=2)[source] on_train_begin(logs=None)on_train_begin¶ on_epoch_begin(epoch=None,            logs=None)on_epoch_begin¶ on_train_batch_begin(step,            logs=None)on_train_batch_begin¶ on_train_batch_end(step,            logs=None)on_train_batch_end¶ on_epoch_end(epoch,            logs=None)on_epoch_end¶ on_eval_begin(logs=None)on_eval_begin¶ on_eval_batch_begin(step,            logs=None)on_eval_batch_begin¶ on_eval_batch_end(step,            logs=None)on_eval_batch_end¶ on_predict_begin(logs=None)on_predict_begin¶ on_predict_batch_begin(step,            logs=None)on_predict_batch_begin¶ on_predict_batch_end(step,            logs=None)on_predict_batch_end¶ on_eval_end(logs=None)on_eval_end¶ on_predict_end(logs=None)on_predict_end¶ 
class paddle.callbacks.ReduceLROnPlateau(monitor='loss',          factor=0.1,          patience=10,          verbose=1,          mode='auto',          min_delta=0.0001,          cooldown=0,          min_lr=0)[source] on_train_begin(logs=None)on_train_begin¶ on_eval_end(logs=None)on_eval_end¶ 
class paddle.callbacks.VisualDL(log_dir)[source] on_train_begin(logs=None)on_train_begin¶ on_epoch_begin(epoch=None,            logs=None)on_epoch_begin¶ on_train_batch_end(step,            logs=None)on_train_batch_end¶ on_eval_begin(logs=None)on_eval_begin¶ on_train_end(logs=None)on_train_end¶ on_eval_end(logs=None)on_eval_end¶ 

paddle.device.get_cudnn_version()[source] 
paddle.device.get_device()[source] 
paddle.device.is_compiled_with_cuda()[source] 
paddle.device.is_compiled_with_npu()[source] 
paddle.device.is_compiled_with_rocm()[source] 
paddle.device.is_compiled_with_xpu()[source] 
paddle.device.set_device(device)[source] 
paddle.device.XPUPlace(dev_id)[source] 
paddle.distributed.all_gather(tensor_list,          tensor,          group=None,          use_calc_stream=True)[source] 
paddle.distributed.all_reduce(tensor,          op=0,          group=None,          use_calc_stream=True)[source] 
paddle.distributed.alltoall(in_tensor_list,          out_tensor_list,          group=None,          use_calc_stream=True)[source] 
paddle.distributed.barrier(group=None)[source] 
paddle.distributed.broadcast(tensor,          src,          group=None,          use_calc_stream=True)[source] 
class paddle.distributed.CountFilterEntry(count_filter)[source] 

paddle.distributed.get_group(id=0)[source] 
paddle.distributed.get_rank()[source] 
paddle.distributed.get_world_size()[source] 
paddle.distributed.gloo_barrier()[source] 
paddle.distributed.gloo_init_parallel_env(rank_id,          rank_num,          server_endpoint)[source] 
paddle.distributed.gloo_release()[source] 
paddle.distributed.init_parallel_env()[source] 
class paddle.distributed.InMemoryDataset[source] update_settings(**kwargs)update_settings¶ init(**kwargs)init¶ load_into_memory(is_shuffle=False)load_into_memory¶ preload_into_memory(thread_num=None)preload_into_memory¶ wait_preload_done()wait_preload_done¶ local_shuffle()local_shuffle¶ global_shuffle(fleet=None,            thread_num=12)global_shuffle¶ release_memory()release_memory¶ get_memory_data_size(fleet=None)get_memory_data_size¶ get_shuffle_data_size(fleet=None)get_shuffle_data_size¶ slots_shuffle(slots)slots_shuffle¶ set_filelist(filelist)set_filelist¶ 
paddle.distributed.launch()[source] 
paddle.distributed.new_group(ranks=None,          backend=None)[source] 
class paddle.distributed.ParallelEnv[source] property rank property world_size property device_id property current_endpoint property trainer_endpoints property nrings property local_rank property nranks property dev_id 
class paddle.distributed.ProbabilityEntry(probability)[source] 
class paddle.distributed.QueueDataset[source] init(**kwargs)init¶ set_filelist(filelist)set_filelist¶ 
paddle.distributed.recv(tensor,          src=0,          group=None,          use_calc_stream=True)[source] 
paddle.distributed.reduce(tensor,          dst,          op=0,          group=None,          use_calc_stream=True)[source] 
class paddle.distributed.ReduceOp[source] 
paddle.distributed.scatter(tensor,          tensor_list=None,          src=0,          group=None,          use_calc_stream=True)[source] 
paddle.distributed.send(tensor,          dst=0,          group=None,          use_calc_stream=True)[source] 
paddle.distributed.spawn(func,          args=(),          nprocs=- 1,          join=True,          daemon=False,          **options)[source] 
paddle.distributed.split(x,          size,          operation,          axis=0,          num_partitions=1,          gather_out=True,          weight_attr=None,          bias_attr=None,          name=None)[source] 

paddle.distributed.wait(tensor,          group=None,          use_calc_stream=True)[source] 
class paddle.distribution.Categorical(logits,          name=None)[source] sample(shape)sample¶ kl_divergence(other)kl_divergence¶ entropy()entropy¶ probs(value)probs¶ log_prob(value)log_prob¶ 
class paddle.distribution.Distribution[source] sample()sample¶ entropy()entropy¶ kl_divergence(other)kl_divergence¶ log_prob(value)log_prob¶ probs(value)probs¶ 
class paddle.distribution.Normal(loc,          scale,          name=None)[source] sample(shape,            seed=0)sample¶ entropy()entropy¶ log_prob(value)log_prob¶ probs(value)probs¶ kl_divergence(other)kl_divergence¶ 
class paddle.distribution.Uniform(low,          high,          name=None)[source] sample(shape,            seed=0)sample¶ log_prob(value)log_prob¶ probs(value)probs¶ entropy()entropy¶ kl_divergence(other)kl_divergence¶ 
paddle.fft.fft(x,          n=None,          axis=- 1,          norm='backward',          name=None)[source] 
paddle.fft.fft2(x,          s=None,          axes=(- 2, - 1),          norm='backward',          name=None)[source] 
paddle.fft.fftfreq(n,          d=1.0,          dtype=None,          name=None)[source] 
paddle.fft.fftn(x,          s=None,          axes=None,          norm='backward',          name=None)[source] 
paddle.fft.fftshift(x,          axes=None,          name=None)[source] 
paddle.fft.hfft(x,          n=None,          axis=- 1,          norm='backward',          name=None)[source] 
paddle.fft.hfft2(x,          s=None,          axes=(- 2, - 1),          norm='backward',          name=None)[source] 
paddle.fft.hfftn(x,          s=None,          axes=None,          norm='backward',          name=None)[source] 
paddle.fft.ifft(x,          n=None,          axis=- 1,          norm='backward',          name=None)[source] 
paddle.fft.ifft2(x,          s=None,          axes=(- 2, - 1),          norm='backward',          name=None)[source] 
paddle.fft.ifftn(x,          s=None,          axes=None,          norm='backward',          name=None)[source] 
paddle.fft.ifftshift(x,          axes=None,          name=None)[source] 
paddle.fft.ihfft(x,          n=None,          axis=- 1,          norm='backward',          name=None)[source] 
paddle.fft.ihfft2(x,          s=None,          axes=(- 2, - 1),          norm='backward',          name=None)[source] 
paddle.fft.ihfftn(x,          s=None,          axes=None,          norm='backward',          name=None)[source] 
paddle.fft.irfft(x,          n=None,          axis=- 1,          norm='backward',          name=None)[source] 
paddle.fft.irfft2(x,          s=None,          axes=(- 2, - 1),          norm='backward',          name=None)[source] 
paddle.fft.irfftn(x,          s=None,          axes=None,          norm='backward',          name=None)[source] 
paddle.fft.rfft(x,          n=None,          axis=- 1,          norm='backward',          name=None)[source] 
paddle.fft.rfft2(x,          s=None,          axes=(- 2, - 1),          norm='backward',          name=None)[source] 
paddle.fft.rfftfreq(n,          d=1.0,          dtype=None,          name=None)[source] 
paddle.fft.rfftn(x,          s=None,          axes=None,          norm='backward',          name=None)[source] 




paddle.fluid.data(name,          shape,          dtype='float32',          lod_level=0)[source] 



























paddle.hub.help(repo_dir,          model,          source='github',          force_reload=False)[source] 
paddle.hub.list(repo_dir,          source='github',          force_reload=False)[source] 
paddle.hub.load(repo_dir,          model,          source='github',          force_reload=False,          **kwargs)[source] 
paddle.incubate.graph_send_recv(x,          src_index,          dst_index,          pool_type='sum',          name=None)[source] 
class paddle.incubate.LookAhead(inner_optimizer,          alpha=0.5,          k=5,          name=None)[source] step()step¶ minimize(loss,            startup_program=None,            parameters=None,            no_grad_set=None)minimize¶ append_regularization_ops(parameters_and_grads,            regularization=None)append_regularization_ops¶ apply_gradients(params_grads)apply_gradients¶ backward(loss,            startup_program=None,            parameters=None,            no_grad_set=None,            callbacks=None)backward¶ clear_grad()clear_grad¶ get_lr()get_lr¶ set_lr(value)set_lr¶ set_state_dict(state_dict)set_state_dict¶ state_dict()state_dict¶ 
class paddle.incubate.ModelAverage(average_window_rate,          parameters=None,          min_average_window=10000,          max_average_window=10000,          name=None)[source] minimize(loss,            startup_program=None,            parameters=None,            no_grad_set=None)minimize¶ step()step¶ apply(executor=None,            need_restore=True)apply¶ restore(executor=None)restore¶ append_regularization_ops(parameters_and_grads,            regularization=None)append_regularization_ops¶ apply_gradients(params_grads)apply_gradients¶ backward(loss,            startup_program=None,            parameters=None,            no_grad_set=None,            callbacks=None)backward¶ clear_grad()clear_grad¶ get_lr()get_lr¶ set_lr(value)set_lr¶ set_state_dict(state_dict)set_state_dict¶ state_dict()state_dict¶ 


paddle.incubate.segment_max(data,          segment_ids,          name=None)[source] 
paddle.incubate.segment_mean(data,          segment_ids,          name=None)[source] 
paddle.incubate.segment_min(data,          segment_ids,          name=None)[source] 
paddle.incubate.segment_sum(data,          segment_ids,          name=None)[source] 
paddle.incubate.softmax_mask_fuse(x,          mask,          name=None)[source] 
paddle.incubate.softmax_mask_fuse_upper_triangle(x)[source] 
paddle.inference.Config 
paddle.inference.DataType 
paddle.inference.PlaceType 
paddle.inference.PrecisionType 
paddle.inference.Predictor 
class paddle.inference.PredictorPool retrive(self: paddle.fluid.core_avx.PredictorPool,            arg0: int)→ paddle.fluid.core_avx.PaddleInferPredictorretrive¶ 
paddle.inference.Tensor 
class paddle.io.BatchSampler(dataset=None,          sampler=None,          shuffle=False,          batch_size=1,          drop_last=False)[source] 
class paddle.io.ChainDataset(datasets)[source] 
class paddle.io.ComposeDataset(datasets)[source] 
class paddle.io.DataLoader(dataset,          feed_list=None,          places=None,          return_list=True,          batch_sampler=None,          batch_size=1,          shuffle=False,          drop_last=False,          collate_fn=None,          num_workers=0,          use_buffer_reader=True,          use_shared_memory=True,          timeout=0,          worker_init_fn=None,          persistent_workers=False)[source] static from_generator(feed_list=None,            capacity=None,            use_double_buffer=True,            iterable=True,            return_list=False,            use_multiprocess=False,            drop_last=True)from_generator¶ static from_dataset(dataset,            places,            drop_last=True)from_dataset¶ 
class paddle.io.Dataset[source] 
class paddle.io.DistributedBatchSampler(dataset,          batch_size,          num_replicas=None,          rank=None,          shuffle=False,          drop_last=False)[source] set_epoch(epoch)set_epoch¶ 
paddle.io.get_worker_info()[source] 
class paddle.io.IterableDataset[source] 
paddle.io.random_split(dataset,          lengths,          generator=None)[source] 
class paddle.io.RandomSampler(data_source,          replacement=False,          num_samples=None,          generator=None)[source] 
class paddle.io.Sampler(data_source=None)[source] 
class paddle.io.SequenceSampler(data_source)[source] 
class paddle.io.Subset(dataset,          indices)[source] 
class paddle.io.TensorDataset(tensors)[source] 
class paddle.io.WeightedRandomSampler(weights,          num_samples,          replacement=True)[source] 
paddle.jit.load(path,          **configs)[source] 
paddle.jit.not_to_static(func=None)[source] 
class paddle.jit.ProgramTranslator(**kwargs)[source] enable(enable_to_static)enable¶ get_output(dygraph_func,            *args,            **kwargs)get_output¶ get_func(dygraph_func)get_func¶ get_program(dygraph_func,            *args,            **kwargs)get_program¶ get_code(dygraph_func)get_code¶ get_program_cache()get_program_cache¶ 
paddle.jit.save(layer,          path,          input_spec=None,          **configs)[source] 
paddle.jit.set_code_level(level=100,          also_to_stdout=False)[source] 
paddle.jit.set_verbosity(level=0,          also_to_stdout=False)[source] 
paddle.jit.to_static(function=None,          input_spec=None,          build_strategy=None)[source] 
class paddle.jit.TracedLayer(program,          parameters,          feed_names,          fetch_names)[source] static trace(layer,            inputs)trace¶ set_strategy(build_strategy=None,            exec_strategy=None)set_strategy¶ save_inference_model(path,            feed=None,            fetch=None,            **kwargs)save_inference_model¶ 
class paddle.jit.TranslatedLayer(programs,          persistable_vars)[source] train()train¶ eval()eval¶ program(method_name='forward')program¶ add_parameter(name,            parameter)add_parameter¶ add_sublayer(name,            sublayer)add_sublayer¶ apply(fn)apply¶ buffers(include_sublayers=True)buffers¶ children()children¶ clear_gradients()clear_gradients¶ create_parameter(shape,            attr=None,            dtype=None,            is_bias=False,            default_initializer=None)create_parameter¶ create_tensor(name=None,            persistable=None,            dtype=None)create_tensor¶ create_variable(name=None,            persistable=None,            dtype=None)create_variable¶ extra_repr()extra_repr¶ forward(*inputs,            **kwargs)forward¶ full_name()full_name¶ load_dict(state_dict,            use_structured_name=True)load_dict¶ named_buffers(prefix='',            include_sublayers=True)named_buffers¶ named_children()named_children¶ named_parameters(prefix='',            include_sublayers=True)named_parameters¶ named_sublayers(prefix='',            include_self=False,            layers_set=None)named_sublayers¶ parameters(include_sublayers=True)parameters¶ register_buffer(name,            tensor,            persistable=True)register_buffer¶ register_forward_post_hook(hook)register_forward_post_hook¶ register_forward_pre_hook(hook)register_forward_pre_hook¶ set_dict(state_dict,            use_structured_name=True)set_dict¶ set_state_dict(state_dict,            use_structured_name=True)set_state_dict¶ state_dict(destination=None,            include_sublayers=True,            structured_name_prefix='')state_dict¶ sublayers(include_self=False)sublayers¶ to(device=None,            dtype=None,            blocking=None)to¶ to_static_state_dict(destination=None,            include_sublayers=True,            structured_name_prefix='')to_static_state_dict¶ 
paddle.linalg.cond(x,          p=None,          name=None)[source] 
paddle.linalg.det(x,          name=None)[source] 
paddle.linalg.eig(x,          name=None)[source] 
paddle.linalg.eigh(x,          UPLO='L',          name=None)[source] 
paddle.linalg.eigvals(x,          name=None)[source] 
paddle.linalg.eigvalsh(x,          UPLO='L',          name=None)[source] 
paddle.linalg.matrix_power(x,          n,          name=None)[source] 
paddle.linalg.matrix_rank(x,          tol=None,          hermitian=False,          name=None)[source] 
paddle.linalg.multi_dot(x,          name=None)[source] 
paddle.linalg.pinv(x,          rcond=1e-15,          hermitian=False,          name=None)[source] 
paddle.linalg.qr(x,          mode='reduced',          name=None)[source] 
paddle.linalg.slogdet(x,          name=None)[source] 
paddle.linalg.solve(x,          y,          name=None)[source] 
paddle.linalg.svd(x,          full_matrices=False,          name=None)[source] 
paddle.linalg.triangular_solve(x,          y,          upper=True,          transpose=False,          unitriangular=False,          name=None)[source] 
class paddle.metric.Accuracy(topk=(1,),          name=None,          *args,          **kwargs)[source] compute(pred,            label,            *args)compute¶ update(correct,            *args)update¶ reset()reset¶ accumulate()accumulate¶ name()name¶ 
paddle.metric.accuracy(input,          label,          k=1,          correct=None,          total=None,          name=None)[source] 
class paddle.metric.Auc(curve='ROC',          num_thresholds=4095,          name='auc',          *args,          **kwargs)[source] update(preds,            labels)update¶ accumulate()accumulate¶ reset()reset¶ name()name¶ compute(*args)compute¶ 
class paddle.metric.Metric[source] abstract reset()reset¶ abstract update(*args)update¶ abstract accumulate()accumulate¶ abstract name()name¶ compute(*args)compute¶ 
class paddle.metric.Precision(name='precision',          *args,          **kwargs)[source] update(preds,            labels)update¶ reset()reset¶ accumulate()accumulate¶ name()name¶ compute(*args)compute¶ 
class paddle.metric.Recall(name='recall',          *args,          **kwargs)[source] update(preds,            labels)update¶ accumulate()accumulate¶ reset()reset¶ name()name¶ compute(*args)compute¶ 
class paddle.nn.AdaptiveAvgPool1D(output_size,          name=None)[source] forward(input)forward¶ extra_repr()extra_repr¶ 
class paddle.nn.AdaptiveAvgPool2D(output_size,          data_format='NCHW',          name=None)[source] forward(x)forward¶ extra_repr()extra_repr¶ 
class paddle.nn.AdaptiveAvgPool3D(output_size,          data_format='NCDHW',          name=None)[source] forward(x)forward¶ extra_repr()extra_repr¶ 
class paddle.nn.AdaptiveMaxPool1D(output_size,          return_mask=False,          name=None)[source] forward(input)forward¶ extra_repr()extra_repr¶ 
class paddle.nn.AdaptiveMaxPool2D(output_size,          return_mask=False,          name=None)[source] forward(x)forward¶ extra_repr()extra_repr¶ 
class paddle.nn.AdaptiveMaxPool3D(output_size,          return_mask=False,          name=None)[source] forward(x)forward¶ extra_repr()extra_repr¶ 
class paddle.nn.AlphaDropout(p=0.5,          name=None)[source] forward(input)forward¶ extra_repr()extra_repr¶ 
class paddle.nn.AvgPool1D(kernel_size,          stride=None,          padding=0,          exclusive=True,          ceil_mode=False,          name=None)[source] forward(x)forward¶ extra_repr()extra_repr¶ 
class paddle.nn.AvgPool2D(kernel_size,          stride=None,          padding=0,          ceil_mode=False,          exclusive=True,          divisor_override=None,          data_format='NCHW',          name=None)[source] forward(x)forward¶ extra_repr()extra_repr¶ 
class paddle.nn.AvgPool3D(kernel_size,          stride=None,          padding=0,          ceil_mode=False,          exclusive=True,          divisor_override=None,          data_format='NCDHW',          name=None)[source] forward(x)forward¶ extra_repr()extra_repr¶ 
class paddle.nn.BatchNorm(num_channels,          act=None,          is_test=False,          momentum=0.9,          epsilon=1e-05,          param_attr=None,          bias_attr=None,          dtype='float32',          data_layout='NCHW',          in_place=False,          moving_mean_name=None,          moving_variance_name=None,          do_model_average_for_mean_and_var=True,          use_global_stats=False,          trainable_statistics=False)[source] forward(input)forward¶ 
class paddle.nn.BatchNorm1D(num_features,          momentum=0.9,          epsilon=1e-05,          weight_attr=None,          bias_attr=None,          data_format='NCL',          use_global_stats=None,          name=None)[source] 
class paddle.nn.BatchNorm2D(num_features,          momentum=0.9,          epsilon=1e-05,          weight_attr=None,          bias_attr=None,          data_format='NCHW',          use_global_stats=None,          name=None)[source] 
class paddle.nn.BatchNorm3D(num_features,          momentum=0.9,          epsilon=1e-05,          weight_attr=None,          bias_attr=None,          data_format='NCDHW',          use_global_stats=None,          name=None)[source] 
class paddle.nn.BCELoss(weight=None,          reduction='mean',          name=None)[source] forward(input,            label)forward¶ 
class paddle.nn.BCEWithLogitsLoss(weight=None,          reduction='mean',          pos_weight=None,          name=None)[source] forward(logit,            label)forward¶ 
class paddle.nn.BeamSearchDecoder(cell,          start_token,          end_token,          beam_size,          embedding_fn=None,          output_fn=None)[source] static tile_beam_merge_with_batch(x,            beam_size)tile_beam_merge_with_batch¶ class OutputWrapper(scores,            predicted_ids,            parent_ids) class StateWrapper(cell_states,            log_probs,            finished,            lengths) initialize(initial_cell_states)initialize¶ step(time,            inputs,            states,            **kwargs)step¶ finalize(outputs,            final_states,            sequence_lengths)finalize¶ property tracks_own_finished 
class paddle.nn.Bilinear(in1_features,          in2_features,          out_features,          weight_attr=None,          bias_attr=None,          name=None)[source] forward(x1,            x2)forward¶ extra_repr()extra_repr¶ 
class paddle.nn.BiRNN(cell_fw,          cell_bw,          time_major=False)[source] forward(inputs,            initial_states=None,            sequence_length=None,            **kwargs)forward¶ 
class paddle.nn.ClipGradByGlobalNorm(clip_norm,          group_name='default_group')[source] 
class paddle.nn.ClipGradByNorm(clip_norm)[source] 
class paddle.nn.ClipGradByValue(max,          min=None)[source] 
class paddle.nn.Conv1D(in_channels,          out_channels,          kernel_size,          stride=1,          padding=0,          dilation=1,          groups=1,          padding_mode='zeros',          weight_attr=None,          bias_attr=None,          data_format='NCL')[source] forward(x)forward¶ 
class paddle.nn.Conv1DTranspose(in_channels,          out_channels,          kernel_size,          stride=1,          padding=0,          output_padding=0,          groups=1,          dilation=1,          weight_attr=None,          bias_attr=None,          data_format='NCL')[source] forward(x,            output_size=None)forward¶ 
class paddle.nn.Conv2D(in_channels,          out_channels,          kernel_size,          stride=1,          padding=0,          dilation=1,          groups=1,          padding_mode='zeros',          weight_attr=None,          bias_attr=None,          data_format='NCHW')[source] forward(x)forward¶ 
class paddle.nn.Conv2DTranspose(in_channels,          out_channels,          kernel_size,          stride=1,          padding=0,          output_padding=0,          dilation=1,          groups=1,          weight_attr=None,          bias_attr=None,          data_format='NCHW')[source] forward(x,            output_size=None)forward¶ 
class paddle.nn.Conv3D(in_channels,          out_channels,          kernel_size,          stride=1,          padding=0,          dilation=1,          groups=1,          padding_mode='zeros',          weight_attr=None,          bias_attr=None,          data_format='NCDHW')[source] forward(x)forward¶ 
class paddle.nn.Conv3DTranspose(in_channels,          out_channels,          kernel_size,          stride=1,          padding=0,          output_padding=0,          dilation=1,          groups=1,          weight_attr=None,          bias_attr=None,          data_format='NCDHW')[source] forward(x,            output_size=None)forward¶ 
class paddle.nn.CosineSimilarity(axis=1,          eps=1e-08)[source] forward(x1,            x2)forward¶ extra_repr()extra_repr¶ 
class paddle.nn.CrossEntropyLoss(weight=None,          ignore_index=- 100,          reduction='mean',          soft_label=False,          axis=- 1,          use_softmax=True,          name=None)[source] forward(input,            label)forward¶ 
class paddle.nn.CTCLoss(blank=0,          reduction='mean')[source] forward(log_probs,            labels,            input_lengths,            label_lengths,            norm_by_times=False)forward¶ 
class paddle.nn.Dropout(p=0.5,          axis=None,          mode='upscale_in_train',          name=None)[source] forward(input)forward¶ extra_repr()extra_repr¶ 
class paddle.nn.Dropout2D(p=0.5,          data_format='NCHW',          name=None)[source] forward(input)forward¶ extra_repr()extra_repr¶ 
class paddle.nn.Dropout3D(p=0.5,          data_format='NCDHW',          name=None)[source] forward(input)forward¶ extra_repr()extra_repr¶ 
paddle.nn.dynamic_decode(decoder,          inits=None,          max_step_num=None,          output_time_major=False,          impute_finished=False,          is_test=False,          return_length=False,          **kwargs)[source] 
class paddle.nn.ELU(alpha=1.0,          name=None)[source] forward(x)forward¶ extra_repr()extra_repr¶ 
class paddle.nn.Embedding(num_embeddings,          embedding_dim,          padding_idx=None,          sparse=False,          weight_attr=None,          name=None)[source] forward(x)forward¶ extra_repr()extra_repr¶ 
class paddle.nn.Flatten(start_axis=1,          stop_axis=- 1)[source] forward(input)forward¶ 

class paddle.nn.GELU(approximate=False,          name=None)[source] forward(x)forward¶ extra_repr()extra_repr¶ 
class paddle.nn.GroupNorm(num_groups,          num_channels,          epsilon=1e-05,          weight_attr=None,          bias_attr=None,          data_format='NCHW',          name=None)[source] forward(input)forward¶ extra_repr()extra_repr¶ 
class paddle.nn.GRU(input_size,          hidden_size,          num_layers=1,          direction='forward',          time_major=False,          dropout=0.0,          weight_ih_attr=None,          weight_hh_attr=None,          bias_ih_attr=None,          bias_hh_attr=None,          name=None)[source] 
class paddle.nn.GRUCell(input_size,          hidden_size,          weight_ih_attr=None,          weight_hh_attr=None,          bias_ih_attr=None,          bias_hh_attr=None,          name=None)[source] forward(inputs,            states=None)forward¶ property state_shape extra_repr()extra_repr¶ 
class paddle.nn.Hardshrink(threshold=0.5,          name=None)[source] forward(x)forward¶ extra_repr()extra_repr¶ 
class paddle.nn.Hardsigmoid(name=None)[source] forward(x)forward¶ extra_repr()extra_repr¶ 
class paddle.nn.Hardswish(name=None)[source] forward(x)forward¶ extra_repr()extra_repr¶ 
class paddle.nn.Hardtanh(min=- 1.0,          max=1.0,          name=None)[source] forward(x)forward¶ extra_repr()extra_repr¶ 
class paddle.nn.HSigmoidLoss(feature_size,          num_classes,          weight_attr=None,          bias_attr=None,          is_custom=False,          is_sparse=False,          name=None)[source] forward(input,            label,            path_table=None,            path_code=None)forward¶ 

class paddle.nn.InstanceNorm1D(num_features,          epsilon=1e-05,          momentum=0.9,          weight_attr=None,          bias_attr=None,          data_format='NCHW',          name=None)[source] 
class paddle.nn.InstanceNorm2D(num_features,          epsilon=1e-05,          momentum=0.9,          weight_attr=None,          bias_attr=None,          data_format='NCHW',          name=None)[source] 
class paddle.nn.InstanceNorm3D(num_features,          epsilon=1e-05,          momentum=0.9,          weight_attr=None,          bias_attr=None,          data_format='NCHW',          name=None)[source] 
class paddle.nn.KLDivLoss(reduction='mean')[source] forward(input,            label)forward¶ 
class paddle.nn.L1Loss(reduction='mean',          name=None)[source] forward(input,            label)forward¶ 
class paddle.nn.Layer(name_scope=None,          dtype='float32')[source] train()train¶ eval()eval¶ apply(fn)apply¶ full_name()full_name¶ register_forward_post_hook(hook)register_forward_post_hook¶ register_forward_pre_hook(hook)register_forward_pre_hook¶ create_parameter(shape,            attr=None,            dtype=None,            is_bias=False,            default_initializer=None)create_parameter¶ create_variable(name=None,            persistable=None,            dtype=None)create_variable¶ create_tensor(name=None,            persistable=None,            dtype=None)create_tensor¶ parameters(include_sublayers=True)parameters¶ children()children¶ named_children()named_children¶ sublayers(include_self=False)sublayers¶ named_parameters(prefix='',            include_sublayers=True)named_parameters¶ named_sublayers(prefix='',            include_self=False,            layers_set=None)named_sublayers¶ register_buffer(name,            tensor,            persistable=True)register_buffer¶ buffers(include_sublayers=True)buffers¶ named_buffers(prefix='',            include_sublayers=True)named_buffers¶ clear_gradients()clear_gradients¶ forward(*inputs,            **kwargs)forward¶ add_sublayer(name,            sublayer)add_sublayer¶ add_parameter(name,            parameter)add_parameter¶ extra_repr()extra_repr¶ to_static_state_dict(destination=None,            include_sublayers=True,            structured_name_prefix='')to_static_state_dict¶ state_dict(destination=None,            include_sublayers=True,            structured_name_prefix='')state_dict¶ set_state_dict(state_dict,            use_structured_name=True)set_state_dict¶ to(device=None,            dtype=None,            blocking=None)to¶ set_dict(state_dict,            use_structured_name=True)set_dict¶ load_dict(state_dict,            use_structured_name=True)load_dict¶ 
class paddle.nn.LayerDict(sublayers=None)[source] clear()clear¶ pop(key)pop¶ keys()keys¶ items()items¶ values()values¶ update(sublayers)update¶ 
class paddle.nn.LayerList(sublayers=None)[source] append(sublayer)append¶ insert(index,            sublayer)insert¶ extend(sublayers)extend¶ 
class paddle.nn.LayerNorm(normalized_shape,          epsilon=1e-05,          weight_attr=None,          bias_attr=None,          name=None)[source] forward(input)forward¶ extra_repr()extra_repr¶ 
class paddle.nn.LeakyReLU(negative_slope=0.01,          name=None)[source] forward(x)forward¶ extra_repr()extra_repr¶ 
class paddle.nn.Linear(in_features,          out_features,          weight_attr=None,          bias_attr=None,          name=None)[source] forward(input)forward¶ extra_repr()extra_repr¶ 
class paddle.nn.LocalResponseNorm(size,          alpha=0.0001,          beta=0.75,          k=1.0,          data_format='NCHW',          name=None)[source] forward(input)forward¶ extra_repr()extra_repr¶ 
class paddle.nn.LogSigmoid(name=None)[source] forward(x)forward¶ extra_repr()extra_repr¶ 
class paddle.nn.LogSoftmax(axis=- 1,          name=None)[source] forward(x)forward¶ extra_repr()extra_repr¶ 
class paddle.nn.LSTM(input_size,          hidden_size,          num_layers=1,          direction='forward',          time_major=False,          dropout=0.0,          weight_ih_attr=None,          weight_hh_attr=None,          bias_ih_attr=None,          bias_hh_attr=None,          name=None)[source] 
class paddle.nn.LSTMCell(input_size,          hidden_size,          weight_ih_attr=None,          weight_hh_attr=None,          bias_ih_attr=None,          bias_hh_attr=None,          name=None)[source] forward(inputs,            states=None)forward¶ property state_shape extra_repr()extra_repr¶ 
class paddle.nn.MarginRankingLoss(margin=0.0,          reduction='mean',          name=None)[source] forward(input,            other,            label)forward¶ 
class paddle.nn.Maxout(groups,          axis=1,          name=None)[source] forward(x)forward¶ extra_repr()extra_repr¶ 
class paddle.nn.MaxPool1D(kernel_size,          stride=None,          padding=0,          return_mask=False,          ceil_mode=False,          name=None)[source] forward(input)forward¶ extra_repr()extra_repr¶ 
class paddle.nn.MaxPool2D(kernel_size,          stride=None,          padding=0,          return_mask=False,          ceil_mode=False,          data_format='NCHW',          name=None)[source] forward(x)forward¶ extra_repr()extra_repr¶ 
class paddle.nn.MaxPool3D(kernel_size,          stride=None,          padding=0,          return_mask=False,          ceil_mode=False,          data_format='NCDHW',          name=None)[source] forward(x)forward¶ extra_repr()extra_repr¶ 
class paddle.nn.Mish(name=None)[source] forward(x)forward¶ extra_repr()extra_repr¶ 
class paddle.nn.MSELoss(reduction='mean')[source] forward(input,            label)forward¶ 
class paddle.nn.MultiHeadAttention(embed_dim,          num_heads,          dropout=0.0,          kdim=None,          vdim=None,          need_weights=False,          weight_attr=None,          bias_attr=None)[source] class Cache(k,            v) k v class StaticCache(k,            v) k v compute_kv(key,            value)compute_kv¶ gen_cache(key,            value=None,            type=<class 'paddle.nn.layer.transformer.Cache'>)gen_cache¶ forward(query,            key=None,            value=None,            attn_mask=None,            cache=None)forward¶ 
class paddle.nn.NLLLoss(weight=None,          ignore_index=- 100,          reduction='mean',          name=None)[source] forward(input,            label)forward¶ 
class paddle.nn.Pad1D(padding,          mode='constant',          value=0.0,          data_format='NCL',          name=None)[source] forward(x)forward¶ extra_repr()extra_repr¶ 
class paddle.nn.Pad2D(padding,          mode='constant',          value=0.0,          data_format='NCHW',          name=None)[source] forward(x)forward¶ extra_repr()extra_repr¶ 
class paddle.nn.Pad3D(padding,          mode='constant',          value=0.0,          data_format='NCDHW',          name=None)[source] forward(x)forward¶ extra_repr()extra_repr¶ 
class paddle.nn.PairwiseDistance(p=2.0,          epsilon=1e-06,          keepdim=False,          name=None)[source] forward(x,            y)forward¶ extra_repr()extra_repr¶ 
class paddle.nn.ParameterList(parameters=None)[source] append(parameter)append¶ 
class paddle.nn.PixelShuffle(upscale_factor,          data_format='NCHW',          name=None)[source] forward(x)forward¶ extra_repr()extra_repr¶ 
class paddle.nn.PReLU(num_parameters=1,          init=0.25,          weight_attr=None,          data_format='NCHW',          name=None)[source] forward(x)forward¶ extra_repr()extra_repr¶ 

class paddle.nn.ReLU(name=None)[source] forward(x)forward¶ extra_repr()extra_repr¶ 
class paddle.nn.ReLU6(name=None)[source] forward(x)forward¶ extra_repr()extra_repr¶ 
class paddle.nn.RNN(cell,          is_reverse=False,          time_major=False)[source] forward(inputs,            initial_states=None,            sequence_length=None,            **kwargs)forward¶ 
class paddle.nn.RNNCellBase(name_scope=None,          dtype='float32')[source] get_initial_states(batch_ref,            shape=None,            dtype=None,            init_value=0.0,            batch_dim_idx=0)get_initial_states¶ property state_shape property state_dtype 
class paddle.nn.SELU(scale=1.0507009873554805,          alpha=1.6732632423543772,          name=None)[source] forward(x)forward¶ extra_repr()extra_repr¶ 
class paddle.nn.Sequential(*layers)[source] forward(input)forward¶ 
class paddle.nn.Sigmoid(name=None)[source] forward(x)forward¶ extra_repr()extra_repr¶ 
class paddle.nn.Silu(name=None)[source] forward(x)forward¶ extra_repr()extra_repr¶ 
class paddle.nn.SimpleRNN(input_size,          hidden_size,          num_layers=1,          direction='forward',          time_major=False,          dropout=0.0,          activation='tanh',          weight_ih_attr=None,          weight_hh_attr=None,          bias_ih_attr=None,          bias_hh_attr=None,          name=None)[source] 
class paddle.nn.SimpleRNNCell(input_size,          hidden_size,          activation='tanh',          weight_ih_attr=None,          weight_hh_attr=None,          bias_ih_attr=None,          bias_hh_attr=None,          name=None)[source] forward(inputs,            states=None)forward¶ property state_shape extra_repr()extra_repr¶ 
class paddle.nn.SmoothL1Loss(reduction='mean',          delta=1.0,          name=None)[source] forward(input,            label)forward¶ 
class paddle.nn.Softmax(axis=- 1,          name=None)[source] forward(x)forward¶ extra_repr()extra_repr¶ 
class paddle.nn.Softplus(beta=1,          threshold=20,          name=None)[source] forward(x)forward¶ extra_repr()extra_repr¶ 
class paddle.nn.Softshrink(threshold=0.5,          name=None)[source] forward(x)forward¶ extra_repr()extra_repr¶ 
class paddle.nn.Softsign(name=None)[source] forward(x)forward¶ extra_repr()extra_repr¶ 
class paddle.nn.SpectralNorm(weight_shape,          dim=0,          power_iters=1,          eps=1e-12,          dtype='float32')[source] forward(weight)forward¶ 
class paddle.nn.Swish(name=None)[source] forward(x)forward¶ extra_repr()extra_repr¶ 
class paddle.nn.SyncBatchNorm(num_features,          momentum=0.9,          epsilon=1e-05,          weight_attr=None,          bias_attr=None,          data_format='NCHW',          name=None)[source] forward(x)forward¶ classmethod convert_sync_batchnorm(layer)convert_sync_batchnorm¶ 
class paddle.nn.Tanh(name=None)[source] forward(x)forward¶ extra_repr()extra_repr¶ 
class paddle.nn.Tanhshrink(name=None)[source] forward(x)forward¶ extra_repr()extra_repr¶ 
class paddle.nn.ThresholdedReLU(threshold=1.0,          name=None)[source] forward(x)forward¶ extra_repr()extra_repr¶ 
class paddle.nn.Transformer(d_model=512,          nhead=8,          num_encoder_layers=6,          num_decoder_layers=6,          dim_feedforward=2048,          dropout=0.1,          activation='relu',          attn_dropout=None,          act_dropout=None,          normalize_before=False,          weight_attr=None,          bias_attr=None,          custom_encoder=None,          custom_decoder=None)[source] forward(src,            tgt,            src_mask=None,            tgt_mask=None,            memory_mask=None)forward¶ generate_square_subsequent_mask(length)generate_square_subsequent_mask¶ 
class paddle.nn.TransformerDecoder(decoder_layer,          num_layers,          norm=None)[source] forward(tgt,            memory,            tgt_mask=None,            memory_mask=None,            cache=None)forward¶ gen_cache(memory,            do_zip=False)gen_cache¶ 
class paddle.nn.TransformerDecoderLayer(d_model,          nhead,          dim_feedforward,          dropout=0.1,          activation='relu',          attn_dropout=None,          act_dropout=None,          normalize_before=False,          weight_attr=None,          bias_attr=None)[source] forward(tgt,            memory,            tgt_mask=None,            memory_mask=None,            cache=None)forward¶ gen_cache(memory)gen_cache¶ 
class paddle.nn.TransformerEncoder(encoder_layer,          num_layers,          norm=None)[source] forward(src,            src_mask=None,            cache=None)forward¶ gen_cache(src)gen_cache¶ 
class paddle.nn.TransformerEncoderLayer(d_model,          nhead,          dim_feedforward,          dropout=0.1,          activation='relu',          attn_dropout=None,          act_dropout=None,          normalize_before=False,          weight_attr=None,          bias_attr=None)[source] forward(src,            src_mask=None,            cache=None)forward¶ gen_cache(src)gen_cache¶ 
class paddle.nn.Unfold(kernel_sizes,          dilations=1,          paddings=0,          strides=1,          name=None)[source] forward(input)forward¶ extra_repr()extra_repr¶ 
class paddle.nn.Upsample(size=None,          scale_factor=None,          mode='nearest',          align_corners=False,          align_mode=0,          data_format='NCHW',          name=None)[source] forward(x)forward¶ extra_repr()extra_repr¶ 
class paddle.nn.UpsamplingBilinear2D(size=None,          scale_factor=None,          data_format='NCHW',          name=None)[source] forward(x)forward¶ extra_repr()extra_repr¶ 
class paddle.nn.UpsamplingNearest2D(size=None,          scale_factor=None,          data_format='NCHW',          name=None)[source] forward(x)forward¶ extra_repr()extra_repr¶ 

paddle.onnx.export(layer,          path,          input_spec=None,          opset_version=9,          **configs)[source] 
class paddle.optimizer.Adadelta(learning_rate=0.001,          epsilon=1e-06,          rho=0.95,          parameters=None,          weight_decay=None,          grad_clip=None,          name=None)[source] append_regularization_ops(parameters_and_grads,            regularization=None)append_regularization_ops¶ clear_grad()clear_grad¶ get_lr()get_lr¶ minimize(loss,            startup_program=None,            parameters=None,            no_grad_set=None)minimize¶ set_lr(value)set_lr¶ set_state_dict(state_dict)set_state_dict¶ state_dict()state_dict¶ step()step¶ 
class paddle.optimizer.Adagrad(learning_rate,          epsilon=1e-06,          parameters=None,          weight_decay=None,          grad_clip=None,          name=None,          initial_accumulator_value=0.0)[source] append_regularization_ops(parameters_and_grads,            regularization=None)append_regularization_ops¶ clear_grad()clear_grad¶ get_lr()get_lr¶ minimize(loss,            startup_program=None,            parameters=None,            no_grad_set=None)minimize¶ set_lr(value)set_lr¶ set_state_dict(state_dict)set_state_dict¶ state_dict()state_dict¶ step()step¶ 
class paddle.optimizer.Adam(learning_rate=0.001,          beta1=0.9,          beta2=0.999,          epsilon=1e-08,          parameters=None,          weight_decay=None,          grad_clip=None,          lazy_mode=False,          multi_precision=False,          name=None)[source] step()step¶ append_regularization_ops(parameters_and_grads,            regularization=None)append_regularization_ops¶ clear_grad()clear_grad¶ get_lr()get_lr¶ minimize(loss,            startup_program=None,            parameters=None,            no_grad_set=None)minimize¶ set_lr(value)set_lr¶ set_state_dict(state_dict)set_state_dict¶ state_dict()state_dict¶ 
class paddle.optimizer.Adamax(learning_rate=0.001,          beta1=0.9,          beta2=0.999,          epsilon=1e-08,          parameters=None,          weight_decay=None,          grad_clip=None,          name=None)[source] append_regularization_ops(parameters_and_grads,            regularization=None)append_regularization_ops¶ clear_grad()clear_grad¶ get_lr()get_lr¶ minimize(loss,            startup_program=None,            parameters=None,            no_grad_set=None)minimize¶ set_lr(value)set_lr¶ set_state_dict(state_dict)set_state_dict¶ state_dict()state_dict¶ step()step¶ 
class paddle.optimizer.AdamW(learning_rate=0.001,          beta1=0.9,          beta2=0.999,          epsilon=1e-08,          parameters=None,          weight_decay=0.01,          lr_ratio=None,          apply_decay_param_fun=None,          grad_clip=None,          lazy_mode=False,          multi_precision=False,          name=None)[source] append_regularization_ops(parameters_and_grads,            regularization=None)append_regularization_ops¶ clear_grad()clear_grad¶ get_lr()get_lr¶ minimize(loss,            startup_program=None,            parameters=None,            no_grad_set=None)minimize¶ set_lr(value)set_lr¶ set_state_dict(state_dict)set_state_dict¶ state_dict()state_dict¶ step()step¶ 
class paddle.optimizer.Lamb(learning_rate=0.001,          lamb_weight_decay=0.01,          beta1=0.9,          beta2=0.999,          epsilon=1e-06,          parameters=None,          grad_clip=None,          exclude_from_weight_decay_fn=None,          name=None)[source] append_regularization_ops(parameters_and_grads,            regularization=None)append_regularization_ops¶ clear_grad()clear_grad¶ get_lr()get_lr¶ minimize(loss,            startup_program=None,            parameters=None,            no_grad_set=None)minimize¶ set_lr(value)set_lr¶ set_state_dict(state_dict)set_state_dict¶ state_dict()state_dict¶ step()step¶ 

class paddle.optimizer.Momentum(learning_rate=0.001,          momentum=0.9,          parameters=None,          use_nesterov=False,          weight_decay=None,          grad_clip=None,          multi_precision=False,          rescale_grad=1.0,          name=None)[source] append_regularization_ops(parameters_and_grads,            regularization=None)append_regularization_ops¶ clear_grad()clear_grad¶ get_lr()get_lr¶ minimize(loss,            startup_program=None,            parameters=None,            no_grad_set=None)minimize¶ set_lr(value)set_lr¶ set_state_dict(state_dict)set_state_dict¶ state_dict()state_dict¶ step()step¶ 
class paddle.optimizer.Optimizer(learning_rate,          parameters=None,          weight_decay=None,          grad_clip=None,          name=None)[source] state_dict()state_dict¶ set_state_dict(state_dict)set_state_dict¶ set_lr(value)set_lr¶ get_lr()get_lr¶ append_regularization_ops(parameters_and_grads,            regularization=None)append_regularization_ops¶ clear_grad()clear_grad¶ minimize(loss,            startup_program=None,            parameters=None,            no_grad_set=None)minimize¶ step()step¶ 
class paddle.optimizer.RMSProp(learning_rate,          rho=0.95,          epsilon=1e-06,          momentum=0.0,          centered=False,          parameters=None,          weight_decay=None,          grad_clip=None,          name=None)[source] append_regularization_ops(parameters_and_grads,            regularization=None)append_regularization_ops¶ clear_grad()clear_grad¶ get_lr()get_lr¶ minimize(loss,            startup_program=None,            parameters=None,            no_grad_set=None)minimize¶ set_lr(value)set_lr¶ set_state_dict(state_dict)set_state_dict¶ state_dict()state_dict¶ step()step¶ 
class paddle.optimizer.SGD(learning_rate=0.001,          parameters=None,          weight_decay=None,          grad_clip=None,          name=None)[source] append_regularization_ops(parameters_and_grads,            regularization=None)append_regularization_ops¶ clear_grad()clear_grad¶ get_lr()get_lr¶ minimize(loss,            startup_program=None,            parameters=None,            no_grad_set=None)minimize¶ set_lr(value)set_lr¶ set_state_dict(state_dict)set_state_dict¶ state_dict()state_dict¶ step()step¶ 
class paddle.regularizer.L1Decay(coeff=0.0)[source] 
class paddle.regularizer.L2Decay(coeff=0.0)[source] 
paddle.signal.istft(x,          n_fft,          hop_length=None,          win_length=None,          window=None,          center=True,          normalized=False,          onesided=True,          length=None,          return_complex=False,          name=None)[source] 
paddle.signal.stft(x,          n_fft,          hop_length=None,          win_length=None,          window=None,          center=True,          pad_mode='reflect',          normalized=False,          onesided=True,          name=None)[source] 
paddle.static.accuracy(input,          label,          k=1,          correct=None,          total=None)[source] 
paddle.static.append_backward(loss,          parameter_list=None,          no_grad_set=None,          callbacks=None,          checkpoints=None)[source] 
paddle.static.auc(input,          label,          curve='ROC',          num_thresholds=4095,          topk=1,          slide_steps=1)[source] 
class paddle.static.BuildStrategy class GradientScaleStrategy property name class ReduceStrategy property name property debug_graphviz_path property enable_auto_fusion property enable_sequential_execution property fuse_bn_act_ops property fuse_bn_add_act_ops property fuse_broadcast_ops property fuse_elewise_add_act_ops property fuse_relu_depthwise_conv property gradient_scale_strategy property memory_optimize property reduce_strategy property remove_unnecessary_lock property sync_batch_norm 
class paddle.static.CompiledProgram(program_or_graph,          build_strategy=None)[source] with_data_parallel(loss_name=None,            build_strategy=None,            exec_strategy=None,            share_vars_from=None,            places=None)with_data_parallel¶ 
paddle.static.cpu_places(device_count=None)[source] 
paddle.static.create_global_var(shape,          value,          dtype,          persistable=False,          force_cpu=False,          name=None)[source] 
paddle.static.cuda_places(device_ids=None)[source] 
paddle.static.data(name,          shape,          dtype=None,          lod_level=0)[source] 
paddle.static.default_main_program()[source] 
paddle.static.default_startup_program()[source] 
paddle.static.deserialize_persistables(program,          data,          executor)[source] 
paddle.static.deserialize_program(data)[source] 
paddle.static.device_guard(device=None)[source] 
class paddle.static.ExecutionStrategy property allow_op_delay property num_iteration_per_drop_scope property num_iteration_per_run property num_threads property use_thread_barrier 
class paddle.static.Executor(place=None)[source] close()close¶ run(program=None,            feed=None,            fetch_list=None,            feed_var_name='feed',            fetch_var_name='fetch',            scope=None,            return_numpy=True,            use_program_cache=False,            return_merged=True,            use_prune=False)run¶ infer_from_dataset(program=None,            dataset=None,            scope=None,            thread=0,            debug=False,            fetch_list=None,            fetch_info=None,            print_period=100,            fetch_handler=None)infer_from_dataset¶ train_from_dataset(program=None,            dataset=None,            scope=None,            thread=0,            debug=False,            fetch_list=None,            fetch_info=None,            print_period=100,            fetch_handler=None)train_from_dataset¶ 
class paddle.static.ExponentialMovingAverage(decay=0.999,          thres_steps=None,          name=None)[source] update()update¶ apply(executor,            need_restore=True)apply¶ restore(executor)restore¶ 
paddle.static.global_scope()[source] 
paddle.static.gradients(targets,          inputs,          target_gradients=None,          no_grad_set=None)[source] 
class paddle.static.InputSpec(shape,          dtype='float32',          name=None)[source] classmethod from_tensor(tensor,            name=None)from_tensor¶ classmethod from_numpy(ndarray,            name=None)from_numpy¶ batch(batch_size)batch¶ unbatch()unbatch¶ 
paddle.static.load(program,          model_path,          executor=None,          var_list=None)[source] 
paddle.static.load_from_file(path)[source] 
paddle.static.load_inference_model(path_prefix,          executor,          **kwargs)[source] 
paddle.static.load_program_state(model_path,          var_list=None)[source] 
paddle.static.name_scope(prefix=None)[source] 

paddle.static.normalize_program(program,          feed_vars,          fetch_vars)[source] 
class paddle.static.ParallelExecutor(use_cuda,          loss_name=None,          main_program=None,          share_vars_from=None,          exec_strategy=None,          build_strategy=None,          num_trainers=1,          trainer_id=0,          scope=None)[source] run(fetch_list,            feed=None,            feed_dict=None,            return_numpy=True)run¶ drop_local_exe_scopes()drop_local_exe_scopes¶ 
paddle.static.Print(input,          first_n=- 1,          message=None,          summarize=20,          print_tensor_name=True,          print_tensor_type=True,          print_tensor_shape=True,          print_tensor_layout=True,          print_tensor_lod=True,          print_phase='both')[source] 
class paddle.static.Program[source] global_seed(seed=0)global_seed¶ to_string(throw_on_error,            with_details=False)to_string¶ clone(for_test=False)clone¶ static parse_from_string(binary_str)parse_from_string¶ property num_blocks property random_seed global_block()global_block¶ block(index)block¶ current_block()current_block¶ list_vars()list_vars¶ all_parameters()all_parameters¶ state_dict(mode='all',            scope=None)state_dict¶ set_state_dict(state_dict,            scope=None)set_state_dict¶ 
paddle.static.program_guard(main_program,          startup_program=None)[source] 
paddle.static.py_func(func,          x,          out,          backward_func=None,          skip_vars_in_backward_input=None)[source] 
paddle.static.save(program,          model_path,          protocol=4,          **configs)[source] 
paddle.static.save_inference_model(path_prefix,          feed_vars,          fetch_vars,          executor,          **kwargs)[source] 
paddle.static.save_to_file(path,          content)[source] 
paddle.static.scope_guard(scope)[source] 
paddle.static.serialize_persistables(feed_vars,          fetch_vars,          executor,          **kwargs)[source] 
paddle.static.serialize_program(feed_vars,          fetch_vars,          **kwargs)[source] 
paddle.static.set_program_state(program,          state_dict)[source] 

class paddle.static.Variable(block,          type=VarType.LOD_TENSOR,          name=None,          shape=None,          dtype=None,          lod_level=None,          capacity=None,          persistable=None,          error_clip=None,          stop_gradient=False,          is_data=False,          need_check_feed=False,          belong_to_optimizer=False,          **kwargs)[source] detach()detach¶ numpy()numpy¶ backward(retain_graph=False)backward¶ gradient()gradient¶ clear_gradient()clear_gradient¶ to_string(throw_on_error,            with_details=False)to_string¶ property stop_gradient property persistable property is_parameter property grad_name property name property shape property dtype property lod_level property type property T clone()clone¶ get_value(scope=None)get_value¶ set_value(value,            scope=None)set_value¶ size()size¶ property attr_names property process_mesh property shard_mask property offload_device abs(name=None)abs¶ acos(name=None)acos¶ add(y,            name=None)add¶ add_(y,            name=None)add_¶ add_n(name=None)add_n¶ addmm(x,            y,            beta=1.0,            alpha=1.0,            name=None)addmm¶ all(axis=None,            keepdim=False,            name=None)all¶ allclose(y,            rtol=1e-05,            atol=1e-08,            equal_nan=False,            name=None)allclose¶ any(axis=None,            keepdim=False,            name=None)any¶ append(var)append¶ argmax(axis=None,            keepdim=False,            dtype='int64',            name=None)argmax¶ argmin(axis=None,            keepdim=False,            dtype='int64',            name=None)argmin¶ argsort(axis=- 1,            descending=False,            name=None)argsort¶ asin(name=None)asin¶ astype(dtype)astype¶ atan(name=None)atan¶ bincount(weights=None,            minlength=0,            name=None)bincount¶ bitwise_and(y,            out=None,            name=None)bitwise_and¶ bitwise_not(out=None,            name=None)bitwise_not¶ bitwise_or(y,            out=None,            name=None)bitwise_or¶ bitwise_xor(y,            out=None,            name=None)bitwise_xor¶ bmm(y,            name=None)bmm¶ broadcast_shape(y_shape)broadcast_shape¶ broadcast_tensors(name=None)broadcast_tensors¶ broadcast_to(shape,            name=None)broadcast_to¶ cast(dtype)cast¶ ceil(name=None)ceil¶ ceil_(name=None)ceil_¶ cholesky(upper=False,            name=None)cholesky¶ chunk(chunks,            axis=0,            name=None)chunk¶ clip(min=None,            max=None,            name=None)clip¶ clip_(min=None,            max=None,            name=None)clip_¶ concat(axis=0,            name=None)concat¶ cond(p=None,            name=None)cond¶ conj(name=None)conj¶ cos(name=None)cos¶ cosh(name=None)cosh¶ cross(y,            axis=None,            name=None)cross¶ cumprod(dim=None,            dtype=None,            name=None)cumprod¶ cumsum(axis=None,            dtype=None,            name=None)cumsum¶ diagonal(offset=0,            axis1=0,            axis2=1,            name=None)diagonal¶ digamma(name=None)digamma¶ dist(y,            p=2,            name=None)dist¶ divide(y,            name=None)divide¶ dot(y,            name=None)dot¶ eig(name=None)eig¶ eigvals(name=None)eigvals¶ eigvalsh(UPLO='L',            name=None)eigvalsh¶ equal(y,            name=None)equal¶ equal_all(y,            name=None)equal_all¶ erf(name=None)erf¶ exp(name=None)exp¶ exp_(name=None)exp_¶ expand(shape,            name=None)expand¶ expand_as(y,            name=None)expand_as¶ flatten(start_axis=0,            stop_axis=- 1,            name=None)flatten¶ flatten_(start_axis=0,            stop_axis=- 1,            name=None)flatten_¶ flip(axis,            name=None)flip¶ floor(name=None)floor¶ floor_(name=None)floor_¶ floor_divide(y,            name=None)floor_divide¶ floor_mod(y,            name=None)floor_mod¶ gather(index,            axis=None,            name=None)gather¶ gather_nd(index,            name=None)gather_nd¶ greater_equal(y,            name=None)greater_equal¶ greater_than(y,            name=None)greater_than¶ histogram(bins=100,            min=0,            max=0,            name=None)histogram¶ imag(name=None)imag¶ increment(value=1.0,            name=None)increment¶ index_sample(index)index_sample¶ index_select(index,            axis=0,            name=None)index_select¶ inverse(name=None)inverse¶ is_empty(name=None)is_empty¶ is_tensor()is_tensor¶ isfinite(name=None)isfinite¶ isinf(name=None)isinf¶ isnan(name=None)isnan¶ kron(y,            name=None)kron¶ less_equal(y,            name=None)less_equal¶ less_than(y,            name=None)less_than¶ lgamma(name=None)lgamma¶ log(name=None)log¶ log10(name=None)log10¶ log1p(name=None)log1p¶ log2(name=None)log2¶ logical_and(y,            out=None,            name=None)logical_and¶ logical_not(out=None,            name=None)logical_not¶ logical_or(y,            out=None,            name=None)logical_or¶ logical_xor(y,            out=None,            name=None)logical_xor¶ logsumexp(axis=None,            keepdim=False,            name=None)logsumexp¶ masked_select(mask,            name=None)masked_select¶ matmul(y,            transpose_x=False,            transpose_y=False,            name=None)matmul¶ matrix_power(n,            name=None)matrix_power¶ max(axis=None,            keepdim=False,            name=None)max¶ maximum(y,            name=None)maximum¶ mean(axis=None,            keepdim=False,            name=None)mean¶ median(axis=None,            keepdim=False,            name=None)median¶ min(axis=None,            keepdim=False,            name=None)min¶ minimum(y,            name=None)minimum¶ mm(mat2,            name=None)mm¶ mod(y,            name=None)mod¶ multi_dot(name=None)multi_dot¶ multiplex(index,            name=None)multiplex¶ multiply(y,            name=None)multiply¶ mv(vec,            name=None)mv¶ property ndim neg(name=None)neg¶ nonzero(as_tuple=False)nonzero¶ norm(p='fro',            axis=None,            keepdim=False,            name=None)norm¶ not_equal(y,            name=None)not_equal¶ numel(name=None)numel¶ pow(y,            name=None)pow¶ prod(axis=None,            keepdim=False,            dtype=None,            name=None)prod¶ qr(mode='reduced',            name=None)qr¶ rank()rank¶ real(name=None)real¶ reciprocal(name=None)reciprocal¶ reciprocal_(name=None)reciprocal_¶ remainder(y,            name=None)remainder¶ reshape(shape,            name=None)reshape¶ reshape_(shape,            name=None)reshape_¶ reverse(axis,            name=None)reverse¶ roll(shifts,            axis=None,            name=None)roll¶ round(name=None)round¶ round_(name=None)round_¶ rsqrt(name=None)rsqrt¶ rsqrt_(name=None)rsqrt_¶ scale(scale=1.0,            bias=0.0,            bias_after_scale=True,            act=None,            name=None)scale¶ scale_(scale=1.0,            bias=0.0,            bias_after_scale=True,            act=None,            name=None)scale_¶ scatter(index,            updates,            overwrite=True,            name=None)scatter¶ scatter_(index,            updates,            overwrite=True,            name=None)scatter_¶ scatter_nd(updates,            shape,            name=None)scatter_nd¶ scatter_nd_add(index,            updates,            name=None)scatter_nd_add¶ shard_index(index_num,            nshards,            shard_id,            ignore_value=- 1)shard_index¶ sign(name=None)sign¶ sin(name=None)sin¶ sinh(name=None)sinh¶ slice(axes,            starts,            ends)slice¶ solve(y,            name=None)solve¶ sort(axis=- 1,            descending=False,            name=None)sort¶ split(num_or_sections,            axis=0,            name=None)split¶ sqrt(name=None)sqrt¶ sqrt_(name=None)sqrt_¶ square(name=None)square¶ squeeze(axis=None,            name=None)squeeze¶ squeeze_(axis=None,            name=None)squeeze_¶ stack(axis=0,            name=None)stack¶ stanh(scale_a=0.67,            scale_b=1.7159,            name=None)stanh¶ std(axis=None,            unbiased=True,            keepdim=False,            name=None)std¶ strided_slice(axes,            starts,            ends,            strides,            name=None)strided_slice¶ subtract(y,            name=None)subtract¶ subtract_(y,            name=None)subtract_¶ sum(axis=None,            dtype=None,            keepdim=False,            name=None)sum¶ t(name=None)t¶ tanh(name=None)tanh¶ tanh_(name=None)tanh_¶ tensordot(y,            axes=2,            name=None)tensordot¶ tile(repeat_times,            name=None)tile¶ topk(k,            axis=None,            largest=True,            sorted=True,            name=None)topk¶ trace(offset=0,            axis1=0,            axis2=1,            name=None)trace¶ transpose(perm,            name=None)transpose¶ trunc(name=None)trunc¶ unbind(axis=0)unbind¶ uniform_(min=- 1.0,            max=1.0,            seed=0,            name=None)uniform_¶ unique(return_index=False,            return_inverse=False,            return_counts=False,            axis=None,            dtype='int64',            name=None)unique¶ unique_consecutive(return_inverse=False,            return_counts=False,            axis=None,            dtype='int64',            name=None)unique_consecutive¶ unsqueeze(axis,            name=None)unsqueeze¶ unsqueeze_(axis,            name=None)unsqueeze_¶ unstack(axis=0,            num=None)unstack¶ var(axis=None,            unbiased=True,            keepdim=False,            name=None)var¶ where(x,            y,            name=None)where¶ 
class paddle.static.WeightNormParamAttr(dim=None,          name=None,          initializer=None,          learning_rate=1.0,          regularizer=None,          trainable=True,          do_model_average=False,          need_clip=True)[source] 
paddle.static.xpu_places(device_ids=None)[source] 
paddle.sysconfig.get_include()[source] 
paddle.sysconfig.get_lib()[source] 

paddle.Tensor.add_(x,          y,          name=None) 
paddle.Tensor.astype(self,          dtype) 
paddle.Tensor.backward(self,          grad_tensor=None,          retain_graph=False) 
paddle.Tensor.ceil_(x,          name=None) 
paddle.Tensor.clear_grad(self) 
paddle.Tensor.clip_(x,          min=None,          max=None,          name=None) 
paddle.Tensor.dim(x) 
paddle.Tensor.exp_(x,          name=None) 
paddle.Tensor.fill_(x,          value) 
paddle.Tensor.fill_diagonal_(x,          value,          offset=0,          wrap=False,          name=None) 
paddle.Tensor.fill_diagonal_tensor(x,          y,          offset=0,          dim1=0,          dim2=1,          name=None) 
paddle.Tensor.fill_diagonal_tensor_(x,          y,          offset=0,          dim1=0,          dim2=1,          name=None) 
paddle.Tensor.flatten_(x,          start_axis=0,          stop_axis=- 1,          name=None) 
paddle.Tensor.floor_(x,          name=None) 
paddle.Tensor.gradient(self) 
paddle.Tensor.item(self,          *args) 
paddle.Tensor.ndimension(x) 
paddle.Tensor.reciprocal_(x,          name=None) 
paddle.Tensor.register_hook(self,          hook) 
paddle.Tensor.round_(x,          name=None) 
paddle.Tensor.rsqrt_(x,          name=None) 
paddle.Tensor.scale_(x,          scale=1.0,          bias=0.0,          bias_after_scale=True,          act=None,          name=None) 
paddle.Tensor.set_value(self,          value) 
paddle.Tensor.sqrt_(x,          name=None) 
paddle.Tensor.subtract_(x,          y,          name=None) 
paddle.Tensor.uniform_(x,          min=- 1.0,          max=1.0,          seed=0,          name=None) 
paddle.Tensor.zero_(x) 
class paddle.text.Conll05st(data_file=None,          word_dict_file=None,          verb_dict_file=None,          target_dict_file=None,          emb_file=None,          download=True)[source] get_dict()get_dict¶ get_embedding()get_embedding¶ 
class paddle.text.Imdb(data_file=None,          mode='train',          cutoff=150,          download=True)[source] 
class paddle.text.Imikolov(data_file=None,          data_type='NGRAM',          window_size=- 1,          mode='train',          min_word_freq=50,          download=True)[source] 
class paddle.text.Movielens(data_file=None,          mode='train',          test_ratio=0.1,          rand_seed=0,          download=True)[source] 
class paddle.text.UCIHousing(data_file=None,          mode='train',          download=True)[source] 
paddle.text.viterbi_decode(potentials,          transition_params,          lengths,          include_bos_eos_tag=True,          name=None)[source] 
class paddle.text.ViterbiDecoder(transitions,          include_bos_eos_tag=True,          name=None)[source] forward(potentials,            lengths)forward¶ add_parameter(name,            parameter)add_parameter¶ add_sublayer(name,            sublayer)add_sublayer¶ apply(fn)apply¶ buffers(include_sublayers=True)buffers¶ children()children¶ clear_gradients()clear_gradients¶ create_parameter(shape,            attr=None,            dtype=None,            is_bias=False,            default_initializer=None)create_parameter¶ create_tensor(name=None,            persistable=None,            dtype=None)create_tensor¶ create_variable(name=None,            persistable=None,            dtype=None)create_variable¶ eval()eval¶ extra_repr()extra_repr¶ full_name()full_name¶ load_dict(state_dict,            use_structured_name=True)load_dict¶ named_buffers(prefix='',            include_sublayers=True)named_buffers¶ named_children()named_children¶ named_parameters(prefix='',            include_sublayers=True)named_parameters¶ named_sublayers(prefix='',            include_self=False,            layers_set=None)named_sublayers¶ parameters(include_sublayers=True)parameters¶ register_buffer(name,            tensor,            persistable=True)register_buffer¶ register_forward_post_hook(hook)register_forward_post_hook¶ register_forward_pre_hook(hook)register_forward_pre_hook¶ set_dict(state_dict,            use_structured_name=True)set_dict¶ set_state_dict(state_dict,            use_structured_name=True)set_state_dict¶ state_dict(destination=None,            include_sublayers=True,            structured_name_prefix='')state_dict¶ sublayers(include_self=False)sublayers¶ to(device=None,            dtype=None,            blocking=None)to¶ to_static_state_dict(destination=None,            include_sublayers=True,            structured_name_prefix='')to_static_state_dict¶ train()train¶ 
class paddle.text.WMT14(data_file=None,          mode='train',          dict_size=- 1,          download=True)[source] get_dict(reverse=False)get_dict¶ 
class paddle.text.WMT16(data_file=None,          mode='train',          src_dict_size=- 1,          trg_dict_size=- 1,          lang='en',          download=True)[source] get_dict(lang,            reverse=False)get_dict¶ 

paddle.utils.deprecated(update_to='',          since='',          reason='',          level=0)[source] 



paddle.utils.require_version(min_version,          max_version=None)[source] 
paddle.utils.run_check()[source] 
paddle.utils.try_import(module_name)[source] 

paddle.version.cuda()[source] 
paddle.version.cudnn()[source] 

paddle.vision.get_image_backend()[source] 
paddle.vision.image_load(path,          backend=None)[source] 


paddle.vision.set_image_backend(backend)[source] 

