paddle.abs(x,          name=None)[source] 
paddle.acos(x,          name=None)[source] 
paddle.acosh(x,          name=None)[source] 
paddle.add(x,          y,          name=None)[source] 
paddle.add_n(inputs,          name=None)[source] 
paddle.addmm(input,          x,          y,          beta=1.0,          alpha=1.0,          name=None)[source] 
paddle.all(x,          axis=None,          keepdim=False,          name=None)[source] 
paddle.allclose(x,          y,          rtol=1e-05,          atol=1e-08,          equal_nan=False,          name=None)[source] 
paddle.amax(x,          axis=None,          keepdim=False,          name=None)[source] 
paddle.amin(x,          axis=None,          keepdim=False,          name=None)[source] 
paddle.angle(x,          name=None)[source] 
paddle.any(x,          axis=None,          keepdim=False,          name=None)[source] 
paddle.arange(start=0,          end=None,          step=1,          dtype=None,          name=None)[source] 
paddle.argmax(x,          axis=None,          keepdim=False,          dtype='int64',          name=None)[source] 
paddle.argmin(x,          axis=None,          keepdim=False,          dtype='int64',          name=None)[source] 
paddle.argsort(x,          axis=- 1,          descending=False,          name=None)[source] 
paddle.as_complex(x,          name=None)[source] 
paddle.as_real(x,          name=None)[source] 
paddle.asin(x,          name=None)[source] 
paddle.asinh(x,          name=None)[source] 
paddle.assign(x,          output=None)[source] 
paddle.atan(x,          name=None)[source] 
paddle.atan2(x,          y,          name=None)[source] 
paddle.atanh(x,          name=None)[source] 
paddle.batch(reader,          batch_size,          drop_last=False)[source] 
paddle.bernoulli(x,          name=None)[source] 
paddle.bincount(x,          weights=None,          minlength=0,          name=None)[source] 
paddle.bitwise_and(x,          y,          out=None,          name=None)[source] 
paddle.bitwise_not(x,          out=None,          name=None)[source] 
paddle.bitwise_or(x,          y,          out=None,          name=None)[source] 
paddle.bitwise_xor(x,          y,          out=None,          name=None)[source] 
paddle.bmm(x,          y,          name=None)[source] 
paddle.broadcast_shape(x_shape,          y_shape)[source] 
paddle.broadcast_tensors(input,          name=None)[source] 
paddle.broadcast_to(x,          shape,          name=None)[source] 
paddle.bucketize(x,          sorted_sequence,          out_int32=False,          right=False,          name=None)[source] 
paddle.cast(x,          dtype)[source] 
paddle.ceil(x,          name=None)[source] 
paddle.check_shape(shape,          op_name,          expected_shape_type=(<class 'list'>,          <class 'tuple'>,          <class 'paddle.fluid.framework.Variable'>),          expected_element_type=(<class 'int'>,          <class 'paddle.fluid.framework.Variable'>),          expected_tensor_dtype=('int32',          'int64'))[source] 
paddle.chunk(x,          chunks,          axis=0,          name=None)[source] 
paddle.clip(x,          min=None,          max=None,          name=None)[source] 
paddle.clone(x,          name=None)[source] 
paddle.complex(real,          imag,          name=None)[source] 
paddle.concat(x,          axis=0,          name=None)[source] 
paddle.conj(x,          name=None)[source] 
paddle.cos(x,          name=None)[source] 
paddle.cosh(x,          name=None)[source] 
paddle.count_nonzero(x,          axis=None,          keepdim=False,          name=None)[source] 
class paddle.CPUPlace 
paddle.create_parameter(shape,          dtype,          name=None,          attr=None,          is_bias=False,          default_initializer=None)[source] 
paddle.crop(x,          shape=None,          offsets=None,          name=None)[source] 
paddle.cross(x,          y,          axis=9,          name=None)[source] 
class paddle.CUDAPinnedPlace 
class paddle.CUDAPlace 
paddle.cumprod(x,          dim=None,          dtype=None,          name=None)[source] 
paddle.cumsum(x,          axis=None,          dtype=None,          name=None)[source] 
class paddle.DataParallel(layers,          strategy=None,          comm_buffer_size=25,          last_comm_buffer_size=1,          find_unused_parameters=False,          group=None)[source] no_sync()no_sync¶ add_parameter(name,            parameter)add_parameter¶ add_sublayer(name,            sublayer)add_sublayer¶ apply(fn)apply¶ buffers(include_sublayers=True)buffers¶ children()children¶ clear_gradients()clear_gradients¶ create_parameter(shape,            attr=None,            dtype=None,            is_bias=False,            default_initializer=None)[source]create_parameter¶ create_tensor(name=None,            persistable=None,            dtype=None)create_tensor¶ create_variable(name=None,            persistable=None,            dtype=None)create_variable¶ eval()eval¶ extra_repr()extra_repr¶ forward(*inputs,            **kwargs)forward¶ full_name()full_name¶ named_buffers(prefix='',            include_sublayers=True)named_buffers¶ named_children()named_children¶ named_parameters(prefix='',            include_sublayers=True)named_parameters¶ named_sublayers(prefix='',            include_self=False,            layers_set=None)named_sublayers¶ parameters(include_sublayers=True)parameters¶ register_buffer(name,            tensor,            persistable=True)register_buffer¶ register_forward_post_hook(hook)register_forward_post_hook¶ register_forward_pre_hook(hook)register_forward_pre_hook¶ sublayers(include_self=False)sublayers¶ to(device=None,            dtype=None,            blocking=None)to¶ to_static_state_dict(destination=None,            include_sublayers=True,            structured_name_prefix='',            use_hook=True)to_static_state_dict¶ train()train¶ scale_loss(loss)scale_loss¶ apply_collective_grads()apply_collective_grads¶ state_dict(destination=None,            include_sublayers=True,            structured_name_prefix='')state_dict¶ set_state_dict(state_dict,            use_structured_name=True)set_state_dict¶ set_dict(state_dict,            use_structured_name=True)set_dict¶ load_dict(state_dict,            use_structured_name=True)load_dict¶ 
paddle.deg2rad(x,          name=None)[source] 
paddle.diag(x,          offset=0,          padding_value=0,          name=None)[source] 
paddle.diagflat(x,          offset=0,          name=None)[source] 
paddle.diagonal(x,          offset=0,          axis1=0,          axis2=1,          name=None)[source] 
paddle.diff(x,          n=1,          axis=- 1,          prepend=None,          append=None,          name=None)[source] 
paddle.digamma(x,          name=None)[source] 
paddle.disable_signal_handler()[source] 
paddle.disable_static(place=None)[source] 
paddle.dist(x,          y,          p=2,          name=None)[source] 
paddle.divide(x,          y,          name=None)[source] 
paddle.dot(x,          y,          name=None)[source] 
paddle.dtype 
paddle.einsum(equation,          *operands)[source] 
paddle.empty(shape,          dtype=None,          name=None)[source] 
paddle.empty_like(x,          dtype=None,          name=None)[source] 
paddle.enable_static()[source] 
paddle.equal(x,          y,          name=None)[source] 
paddle.equal_all(x,          y,          name=None)[source] 
paddle.erf(x,          name=None)[source] 
paddle.erfinv(x,          name=None)[source] 
paddle.exp(x,          name=None)[source] 
paddle.expand(x,          shape,          name=None)[source] 
paddle.expand_as(x,          y,          name=None)[source] 
paddle.expm1(x,          name=None)[source] 
paddle.eye(num_rows,          num_columns=None,          dtype=None,          name=None)[source] 
paddle.flatten(x,          start_axis=0,          stop_axis=- 1,          name=None)[source] 
paddle.flip(x,          axis,          name=None)[source] 
paddle.floor(x,          name=None)[source] 
paddle.floor_divide(x,          y,          name=None)[source] 
paddle.floor_mod(x,          y,          name=None)[source] 
paddle.flops(net,          input_size,          custom_ops=None,          print_detail=False)[source] 
paddle.fmax(x,          y,          name=None)[source] 
paddle.fmin(x,          y,          name=None)[source] 
paddle.frac(x,          name=None)[source] 
paddle.full(shape,          fill_value,          dtype=None,          name=None)[source] 
paddle.full_like(x,          fill_value,          dtype=None,          name=None)[source] 
paddle.gather(x,          index,          axis=None,          name=None)[source] 
paddle.gather_nd(x,          index,          name=None)[source] 
paddle.gcd(x,          y,          name=None)[source] 
paddle.get_cuda_rng_state()[source] 
paddle.get_default_dtype()[source] 
paddle.get_flags(flags)[source] 
paddle.grad(outputs,          inputs,          grad_outputs=None,          retain_graph=None,          create_graph=False,          only_inputs=True,          allow_unused=False,          no_grad_vars=None)[source] 
paddle.greater_equal(x,          y,          name=None)[source] 
paddle.greater_than(x,          y,          name=None)[source] 
paddle.heaviside(x,          y,          name=None)[source] 
paddle.histogram(input,          bins=100,          min=0,          max=0,          name=None)[source] 
paddle.iinfo(dtype)[source] 
paddle.imag(x,          name=None)[source] 
paddle.in_dynamic_mode() 
paddle.increment(x,          value=1.0,          name=None)[source] 
paddle.index_add(x,          index,          axis,          value,          name=None)[source] 
paddle.index_add_(x,          index,          axis,          value,          name=None)[source] 
paddle.index_sample(x,          index)[source] 
paddle.index_select(x,          index,          axis=0,          name=None)[source] 
paddle.inner(x,          y,          name=None)[source] 
paddle.is_complex(x)[source] 
paddle.is_empty(x,          name=None)[source] 
paddle.is_floating_point(x)[source] 
paddle.is_grad_enabled()[source] 
paddle.is_integer(x)[source] 
paddle.is_tensor(x)[source] 
paddle.isclose(x,          y,          rtol=1e-05,          atol=1e-08,          equal_nan=False,          name=None)[source] 
paddle.isfinite(x,          name=None)[source] 
paddle.isinf(x,          name=None)[source] 
paddle.isnan(x,          name=None)[source] 
paddle.kron(x,          y,          name=None)[source] 
paddle.kthvalue(x,          k,          axis=None,          keepdim=False,          name=None)[source] 
class paddle.LazyGuard[source] 
paddle.lcm(x,          y,          name=None)[source] 
paddle.lerp(x,          y,          weight,          name=None)[source] 
paddle.less_equal(x,          y,          name=None)[source] 
paddle.less_than(x,          y,          name=None)[source] 
paddle.lgamma(x,          name=None)[source] 
paddle.linspace(start,          stop,          num,          dtype=None,          name=None)[source] 
paddle.load(path,          **configs)[source] 
paddle.log(x,          name=None)[source] 
paddle.log10(x,          name=None)[source] 
paddle.log1p(x,          name=None)[source] 
paddle.log2(x,          name=None)[source] 
paddle.logcumsumexp(x,          axis=None,          dtype=None,          name=None)[source] 
paddle.logical_and(x,          y,          out=None,          name=None)[source] 
paddle.logical_not(x,          out=None,          name=None)[source] 
paddle.logical_or(x,          y,          out=None,          name=None)[source] 
paddle.logical_xor(x,          y,          out=None,          name=None)[source] 
paddle.logit(x,          eps=None,          name=None)[source] 
paddle.logspace(start,          stop,          num,          base=10.0,          dtype=None,          name=None)[source] 
paddle.logsumexp(x,          axis=None,          keepdim=False,          name=None)[source] 
paddle.masked_select(x,          mask,          name=None)[source] 
paddle.matmul(x,          y,          transpose_x=False,          transpose_y=False,          name=None)[source] 
paddle.max(x,          axis=None,          keepdim=False,          name=None)[source] 
paddle.maximum(x,          y,          name=None)[source] 
paddle.mean(x,          axis=None,          keepdim=False,          name=None)[source] 
paddle.median(x,          axis=None,          keepdim=False,          name=None)[source] 
paddle.meshgrid(*args,          **kwargs)[source] 
paddle.min(x,          axis=None,          keepdim=False,          name=None)[source] 
paddle.minimum(x,          y,          name=None)[source] 
paddle.mm(input,          mat2,          name=None)[source] 
paddle.mode(x,          axis=- 1,          keepdim=False,          name=None)[source] 
class paddle.Model(network,          inputs=None,          labels=None)[source] train_batch(inputs,            labels=None,            update=True)train_batch¶ eval_batch(inputs,            labels=None)eval_batch¶ predict_batch(inputs)predict_batch¶ save(path,            training=True)[source]save¶ load(path,            skip_mismatch=False,            reset_optimizer=False)[source]load¶ parameters(*args,            **kwargs)parameters¶ prepare(optimizer=None,            loss=None,            metrics=None,            amp_configs=None)prepare¶ fit(train_data=None,            eval_data=None,            batch_size=1,            epochs=1,            eval_freq=1,            log_freq=10,            save_dir=None,            save_freq=1,            verbose=2,            drop_last=False,            shuffle=True,            num_workers=0,            callbacks=None,            accumulate_grad_batches=1,            num_iters=None)fit¶ evaluate(eval_data,            batch_size=1,            log_freq=10,            verbose=2,            num_workers=0,            callbacks=None,            num_iters=None)evaluate¶ predict(test_data,            batch_size=1,            num_workers=0,            stack_outputs=False,            verbose=1,            callbacks=None)predict¶ summary(input_size=None,            dtype=None)[source]summary¶ 
paddle.moveaxis(x,          source,          destination,          name=None)[source] 
paddle.multinomial(x,          num_samples=1,          replacement=False,          name=None)[source] 
paddle.multiplex(inputs,          index,          name=None)[source] 
paddle.multiply(x,          y,          name=None)[source] 
paddle.mv(x,          vec,          name=None)[source] 
paddle.nanmean(x,          axis=None,          keepdim=False,          name=None)[source] 
paddle.nanmedian(x,          axis=None,          keepdim=True,          name=None)[source] 
paddle.nanquantile(x,          q,          axis=None,          keepdim=False)[source] 
paddle.nansum(x,          axis=None,          dtype=None,          keepdim=False,          name=None)[source] 
paddle.neg(x,          name=None)[source] 
paddle.no_grad[source] 
paddle.nonzero(x,          as_tuple=False)[source] 
paddle.normal(mean=0.0,          std=1.0,          shape=None,          name=None)[source] 
paddle.not_equal(x,          y,          name=None)[source] 
class paddle.NPUPlace get_device_id(self: paddle.fluid.libpaddle.NPUPlace)→ intget_device_id¶ 
paddle.numel(x,          name=None)[source] 
paddle.ones(shape,          dtype=None,          name=None)[source] 
paddle.ones_like(x,          dtype=None,          name=None)[source] 
paddle.outer(x,          y,          name=None)[source] 
class paddle.ParamAttr(name=None,          initializer=None,          learning_rate=1.0,          regularizer=None,          trainable=True,          do_model_average=True,          need_clip=True)[source] 
paddle.poisson(x,          name=None)[source] 
paddle.pow(x,          y,          name=None)[source] 
paddle.prod(x,          axis=None,          keepdim=False,          dtype=None,          name=None)[source] 
paddle.put_along_axis(arr,          indices,          values,          axis,          reduce='assign')[source] 
paddle.quantile(x,          q,          axis=None,          keepdim=False)[source] 
paddle.rad2deg(x,          name=None)[source] 
paddle.rand(shape,          dtype=None,          name=None)[source] 
paddle.randint(low=0,          high=None,          shape=[1],          dtype=None,          name=None)[source] 
paddle.randint_like(x,          low=0,          high=None,          dtype=None,          name=None)[source] 
paddle.randn(shape,          dtype=None,          name=None)[source] 
paddle.randperm(n,          dtype='int64',          name=None)[source] 
paddle.rank(input)[source] 
paddle.real(x,          name=None)[source] 
paddle.reciprocal(x,          name=None)[source] 
paddle.renorm(x,          p,          axis,          max_norm)[source] 
paddle.repeat_interleave(x,          repeats,          axis=None,          name=None)[source] 
paddle.reshape(x,          shape,          name=None)[source] 
paddle.reshape_(x,          shape,          name=None)[source] 
paddle.roll(x,          shifts,          axis=None,          name=None)[source] 
paddle.rot90(x,          k=1,          axes=[0, 1],          name=None)[source] 
paddle.round(x,          name=None)[source] 
paddle.rsqrt(x,          name=None)[source] 
paddle.save(obj,          path,          protocol=4,          **configs)[source] 
paddle.scale(x,          scale=1.0,          bias=0.0,          bias_after_scale=True,          act=None,          name=None)[source] 
paddle.scatter(x,          index,          updates,          overwrite=True,          name=None)[source] 
paddle.scatter_(x,          index,          updates,          overwrite=True,          name=None)[source] 
paddle.scatter_nd(index,          updates,          shape,          name=None)[source] 
paddle.scatter_nd_add(x,          index,          updates,          name=None)[source] 
paddle.searchsorted(sorted_sequence,          values,          out_int32=False,          right=False,          name=None)[source] 
paddle.seed(seed)[source] 
paddle.set_cuda_rng_state(state_list)[source] 
paddle.set_default_dtype(d)[source] 
paddle.set_flags(flags)[source] 
paddle.set_grad_enabled(mode)[source] 
paddle.set_printoptions(precision=None,          threshold=None,          edgeitems=None,          sci_mode=None,          linewidth=None)[source] 
paddle.sgn(x,          name=None)[source] 
paddle.shape(input)[source] 
paddle.shard_index(input,          index_num,          nshards,          shard_id,          ignore_value=- 1)[source] 
paddle.sign(x,          name=None)[source] 
paddle.sin(x,          name=None)[source] 
paddle.sinh(x,          name=None)[source] 
paddle.slice(input,          axes,          starts,          ends)[source] 
paddle.sort(x,          axis=- 1,          descending=False,          name=None)[source] 
paddle.split(x,          num_or_sections,          axis=0,          name=None)[source] 
paddle.sqrt(x,          name=None)[source] 
paddle.square(x,          name=None)[source] 
paddle.squeeze(x,          axis=None,          name=None)[source] 
paddle.squeeze_(x,          axis=None,          name=None)[source] 
paddle.stack(x,          axis=0,          name=None)[source] 
paddle.standard_normal(shape,          dtype=None,          name=None)[source] 
paddle.stanh(x,          scale_a=0.67,          scale_b=1.7159,          name=None)[source] 
paddle.std(x,          axis=None,          unbiased=True,          keepdim=False,          name=None)[source] 
paddle.strided_slice(x,          axes,          starts,          ends,          strides,          name=None)[source] 
paddle.subtract(x,          y,          name=None)[source] 
paddle.sum(x,          axis=None,          dtype=None,          keepdim=False,          name=None)[source] 
paddle.summary(net,          input_size=None,          dtypes=None,          input=None)[source] 
paddle.t(input,          name=None)[source] 
paddle.take(x,          index,          mode='raise',          name=None)[source] 
paddle.take_along_axis(arr,          indices,          axis)[source] 
paddle.tan(x,          name=None)[source] 
paddle.tanh(x,          name=None)[source] 
paddle.tanh_(x,          name=None)[source] 
class paddle.Tensor abs(name=None)[source]abs¶ acos(name=None)[source]acos¶ acosh(name=None)[source]acosh¶ add(y,            name=None)[source]add¶ add_(y,            name=None)add_¶ add_n(name=None)[source]add_n¶ addmm(x,            y,            beta=1.0,            alpha=1.0,            name=None)[source]addmm¶ all(axis=None,            keepdim=False,            name=None)[source]all¶ allclose(y,            rtol=1e-05,            atol=1e-08,            equal_nan=False,            name=None)[source]allclose¶ amax(axis=None,            keepdim=False,            name=None)[source]amax¶ amin(axis=None,            keepdim=False,            name=None)[source]amin¶ angle(name=None)[source]angle¶ any(axis=None,            keepdim=False,            name=None)[source]any¶ argmax(axis=None,            keepdim=False,            dtype='int64',            name=None)[source]argmax¶ argmin(axis=None,            keepdim=False,            dtype='int64',            name=None)[source]argmin¶ argsort(axis=- 1,            descending=False,            name=None)[source]argsort¶ as_complex(name=None)[source]as_complex¶ as_real(name=None)[source]as_real¶ asin(name=None)[source]asin¶ asinh(name=None)[source]asinh¶ astype(dtype)astype¶ atan(name=None)[source]atan¶ atanh(name=None)[source]atanh¶ backward(grad_tensor=None,            retain_graph=False)backward¶ bincount(weights=None,            minlength=0,            name=None)[source]bincount¶ bitwise_and(y,            out=None,            name=None)[source]bitwise_and¶ bitwise_not(out=None,            name=None)[source]bitwise_not¶ bitwise_or(y,            out=None,            name=None)[source]bitwise_or¶ bitwise_xor(y,            out=None,            name=None)[source]bitwise_xor¶ bmm(y,            name=None)[source]bmm¶ broadcast_shape(y_shape)[source]broadcast_shape¶ broadcast_tensors(name=None)[source]broadcast_tensors¶ broadcast_to(shape,            name=None)[source]broadcast_to¶ bucketize(sorted_sequence,            out_int32=False,            right=False,            name=None)[source]bucketize¶ cast(dtype)[source]cast¶ ceil(name=None)[source]ceil¶ ceil_(name=None)ceil_¶ cholesky(upper=False,            name=None)[source]cholesky¶ cholesky_solve(y,            upper=False,            name=None)cholesky_solve¶ chunk(chunks,            axis=0,            name=None)[source]chunk¶ clear_grad()clear_grad¶ clip(min=None,            max=None,            name=None)[source]clip¶ clip_(min=None,            max=None,            name=None)clip_¶ concat(axis=0,            name=None)[source]concat¶ cond(p=None,            name=None)cond¶ conj(name=None)[source]conj¶ corrcoef(rowvar=True,            name=None)corrcoef¶ cos(name=None)[source]cos¶ cosh(name=None)[source]cosh¶ count_nonzero(axis=None,            keepdim=False,            name=None)[source]count_nonzero¶ cov(rowvar=True,            ddof=True,            fweights=None,            aweights=None,            name=None)cov¶ cross(y,            axis=9,            name=None)[source]cross¶ cumprod(dim=None,            dtype=None,            name=None)[source]cumprod¶ cumsum(axis=None,            dtype=None,            name=None)[source]cumsum¶ deg2rad(name=None)[source]deg2rad¶ diagonal(offset=0,            axis1=0,            axis2=1,            name=None)[source]diagonal¶ diff(n=1,            axis=- 1,            prepend=None,            append=None,            name=None)[source]diff¶ digamma(name=None)[source]digamma¶ dist(y,            p=2,            name=None)[source]dist¶ divide(y,            name=None)[source]divide¶ dot(y,            name=None)[source]dot¶ eig(name=None)eig¶ eigvals(name=None)eigvals¶ eigvalsh(UPLO='L',            name=None)[source]eigvalsh¶ equal(y,            name=None)[source]equal¶ equal_all(y,            name=None)[source]equal_all¶ erf(name=None)[source]erf¶ erfinv(name=None)[source]erfinv¶ erfinv_(name=None)erfinv_¶ exp(name=None)[source]exp¶ exp_(name=None)exp_¶ expand(shape,            name=None)[source]expand¶ expand_as(y,            name=None)[source]expand_as¶ exponential_(lam=1.0,            name=None)exponential_¶ fill_(value)fill_¶ fill_diagonal_(value,            offset=0,            wrap=False,            name=None)fill_diagonal_¶ fill_diagonal_tensor(y,            offset=0,            dim1=0,            dim2=1,            name=None)fill_diagonal_tensor¶ fill_diagonal_tensor_(y,            offset=0,            dim1=0,            dim2=1,            name=None)fill_diagonal_tensor_¶ flatten(start_axis=0,            stop_axis=- 1,            name=None)[source]flatten¶ flatten_(start_axis=0,            stop_axis=- 1,            name=None)flatten_¶ flip(axis,            name=None)[source]flip¶ floor(name=None)[source]floor¶ floor_(name=None)floor_¶ floor_divide(y,            name=None)[source]floor_divide¶ floor_mod(y,            name=None)[source]floor_mod¶ fmax(y,            name=None)[source]fmax¶ fmin(y,            name=None)[source]fmin¶ frac(name=None)[source]frac¶ gather(index,            axis=None,            name=None)[source]gather¶ gather_nd(index,            name=None)[source]gather_nd¶ gcd(y,            name=None)[source]gcd¶ gradient()gradient¶ greater_equal(y,            name=None)[source]greater_equal¶ greater_than(y,            name=None)[source]greater_than¶ heaviside(y,            name=None)[source]heaviside¶ histogram(bins=100,            min=0,            max=0,            name=None)[source]histogram¶ imag(name=None)[source]imag¶ increment(value=1.0,            name=None)[source]increment¶ index_add(index,            axis,            value,            name=None)[source]index_add¶ index_add_(index,            axis,            value,            name=None)[source]index_add_¶ index_sample(index)[source]index_sample¶ index_select(index,            axis=0,            name=None)[source]index_select¶ inner(y,            name=None)[source]inner¶ property inplace_version inverse(name=None)[source]inverse¶ is_complex()[source]is_complex¶ is_empty(name=None)[source]is_empty¶ is_floating_point()[source]is_floating_point¶ is_integer()[source]is_integer¶ is_tensor()[source]is_tensor¶ isclose(y,            rtol=1e-05,            atol=1e-08,            equal_nan=False,            name=None)[source]isclose¶ isfinite(name=None)[source]isfinite¶ isinf(name=None)[source]isinf¶ isnan(name=None)[source]isnan¶ item(*args)item¶ kron(y,            name=None)[source]kron¶ kthvalue(k,            axis=None,            keepdim=False,            name=None)[source]kthvalue¶ lcm(y,            name=None)[source]lcm¶ lerp(y,            weight,            name=None)[source]lerp¶ lerp_(y,            weight,            name=None)lerp_¶ less_equal(y,            name=None)[source]less_equal¶ less_than(y,            name=None)[source]less_than¶ lgamma(name=None)[source]lgamma¶ log(name=None)[source]log¶ log10(name=None)[source]log10¶ log1p(name=None)[source]log1p¶ log2(name=None)[source]log2¶ logcumsumexp(axis=None,            dtype=None,            name=None)[source]logcumsumexp¶ logical_and(y,            out=None,            name=None)[source]logical_and¶ logical_not(out=None,            name=None)[source]logical_not¶ logical_or(y,            out=None,            name=None)[source]logical_or¶ logical_xor(y,            out=None,            name=None)[source]logical_xor¶ logit(eps=None,            name=None)[source]logit¶ logsumexp(axis=None,            keepdim=False,            name=None)[source]logsumexp¶ lstsq(y,            rcond=None,            driver=None,            name=None)lstsq¶ lu(pivot=True,            get_infos=False,            name=None)lu¶ lu_unpack(y,            unpack_ludata=True,            unpack_pivots=True,            name=None)lu_unpack¶ masked_select(mask,            name=None)[source]masked_select¶ matmul(y,            transpose_x=False,            transpose_y=False,            name=None)[source]matmul¶ matrix_power(n,            name=None)matrix_power¶ max(axis=None,            keepdim=False,            name=None)[source]max¶ maximum(y,            name=None)[source]maximum¶ mean(axis=None,            keepdim=False,            name=None)[source]mean¶ median(axis=None,            keepdim=False,            name=None)[source]median¶ min(axis=None,            keepdim=False,            name=None)[source]min¶ minimum(y,            name=None)[source]minimum¶ mm(mat2,            name=None)[source]mm¶ mod(y,            name=None)[source]mod¶ mode(axis=- 1,            keepdim=False,            name=None)[source]mode¶ moveaxis(source,            destination,            name=None)[source]moveaxis¶ multi_dot(name=None)multi_dot¶ multiplex(index,            name=None)[source]multiplex¶ multiply(y,            name=None)[source]multiply¶ mv(vec,            name=None)[source]mv¶ nanmean(axis=None,            keepdim=False,            name=None)[source]nanmean¶ nanmedian(axis=None,            keepdim=True,            name=None)[source]nanmedian¶ nanquantile(q,            axis=None,            keepdim=False)[source]nanquantile¶ nansum(axis=None,            dtype=None,            keepdim=False,            name=None)[source]nansum¶ neg(name=None)[source]neg¶ nonzero(as_tuple=False)[source]nonzero¶ norm(p='fro',            axis=None,            keepdim=False,            name=None)[source]norm¶ not_equal(y,            name=None)[source]not_equal¶ numel(name=None)[source]numel¶ outer(y,            name=None)[source]outer¶ pow(y,            name=None)[source]pow¶ prod(axis=None,            keepdim=False,            dtype=None,            name=None)[source]prod¶ put_along_axis(indices,            values,            axis,            reduce='assign')[source]put_along_axis¶ put_along_axis_(indices,            values,            axis,            reduce='assign')put_along_axis_¶ qr(mode='reduced',            name=None)qr¶ quantile(q,            axis=None,            keepdim=False)[source]quantile¶ rad2deg(name=None)[source]rad2deg¶ rank()[source]rank¶ real(name=None)[source]real¶ reciprocal(name=None)[source]reciprocal¶ reciprocal_(name=None)reciprocal_¶ register_hook(hook)register_hook¶ remainder(y,            name=None)[source]remainder¶ remainder_(y,            name=None)[source]remainder_¶ repeat_interleave(repeats,            axis=None,            name=None)[source]repeat_interleave¶ reshape(shape,            name=None)[source]reshape¶ reshape_(shape,            name=None)[source]reshape_¶ reverse(axis,            name=None)[source]reverse¶ roll(shifts,            axis=None,            name=None)[source]roll¶ rot90(k=1,            axes=[0, 1],            name=None)[source]rot90¶ round(name=None)[source]round¶ round_(name=None)round_¶ rsqrt(name=None)[source]rsqrt¶ rsqrt_(name=None)rsqrt_¶ scale(scale=1.0,            bias=0.0,            bias_after_scale=True,            act=None,            name=None)[source]scale¶ scale_(scale=1.0,            bias=0.0,            bias_after_scale=True,            act=None,            name=None)scale_¶ scatter(index,            updates,            overwrite=True,            name=None)[source]scatter¶ scatter_(index,            updates,            overwrite=True,            name=None)[source]scatter_¶ scatter_nd(updates,            shape,            name=None)[source]scatter_nd¶ scatter_nd_add(index,            updates,            name=None)[source]scatter_nd_add¶ set_value(value)set_value¶ sgn(name=None)[source]sgn¶ shard_index(index_num,            nshards,            shard_id,            ignore_value=- 1)[source]shard_index¶ sign(name=None)[source]sign¶ sin(name=None)[source]sin¶ sinh(name=None)[source]sinh¶ slice(axes,            starts,            ends)[source]slice¶ solve(y,            name=None)solve¶ sort(axis=- 1,            descending=False,            name=None)[source]sort¶ split(num_or_sections,            axis=0,            name=None)[source]split¶ sqrt(name=None)[source]sqrt¶ sqrt_(name=None)sqrt_¶ square(name=None)[source]square¶ squeeze(axis=None,            name=None)[source]squeeze¶ squeeze_(axis=None,            name=None)[source]squeeze_¶ stack(axis=0,            name=None)[source]stack¶ stanh(scale_a=0.67,            scale_b=1.7159,            name=None)[source]stanh¶ std(axis=None,            unbiased=True,            keepdim=False,            name=None)[source]std¶ strided_slice(axes,            starts,            ends,            strides,            name=None)[source]strided_slice¶ subtract(y,            name=None)[source]subtract¶ subtract_(y,            name=None)subtract_¶ sum(axis=None,            dtype=None,            keepdim=False,            name=None)[source]sum¶ t(name=None)[source]t¶ take(index,            mode='raise',            name=None)[source]take¶ take_along_axis(indices,            axis)[source]take_along_axis¶ tanh(name=None)[source]tanh¶ tanh_(name=None)[source]tanh_¶ tensordot(y,            axes=2,            name=None)[source]tensordot¶ tile(repeat_times,            name=None)[source]tile¶ to_dense()to_dense¶ to_sparse_coo(sparse_dim)to_sparse_coo¶ tolist()[source]tolist¶ topk(k,            axis=None,            largest=True,            sorted=True,            name=None)[source]topk¶ trace(offset=0,            axis1=0,            axis2=1,            name=None)[source]trace¶ transpose(perm,            name=None)[source]transpose¶ trunc(name=None)[source]trunc¶ unbind(axis=0)[source]unbind¶ uniform_(min=- 1.0,            max=1.0,            seed=0,            name=None)uniform_¶ unique(return_index=False,            return_inverse=False,            return_counts=False,            axis=None,            dtype='int64',            name=None)[source]unique¶ unique_consecutive(return_inverse=False,            return_counts=False,            axis=None,            dtype='int64',            name=None)[source]unique_consecutive¶ unsqueeze(axis,            name=None)[source]unsqueeze¶ unsqueeze_(axis,            name=None)[source]unsqueeze_¶ unstack(axis=0,            num=None)[source]unstack¶ values()values¶ var(axis=None,            unbiased=True,            keepdim=False,            name=None)[source]var¶ where(x=None,            y=None,            name=None)[source]where¶ zero_()zero_¶ 
paddle.tensordot(x,          y,          axes=2,          name=None)[source] 
paddle.tile(x,          repeat_times,          name=None)[source] 
paddle.to_tensor(data,          dtype=None,          place=None,          stop_gradient=True)[source] 
paddle.tolist(x)[source] 
paddle.topk(x,          k,          axis=None,          largest=True,          sorted=True,          name=None)[source] 
paddle.trace(x,          offset=0,          axis1=0,          axis2=1,          name=None)[source] 
paddle.transpose(x,          perm,          name=None)[source] 
paddle.tril(x,          diagonal=0,          name=None)[source] 
paddle.tril_indices(row,          col,          offset=0,          dtype='int64')[source] 
paddle.triu(x,          diagonal=0,          name=None)[source] 
paddle.triu_indices(row,          col=None,          offset=0,          dtype='int64')[source] 
paddle.trunc(input,          name=None)[source] 
paddle.unbind(input,          axis=0)[source] 
paddle.uniform(shape,          dtype=None,          min=- 1.0,          max=1.0,          seed=0,          name=None)[source] 
paddle.unique(x,          return_index=False,          return_inverse=False,          return_counts=False,          axis=None,          dtype='int64',          name=None)[source] 
paddle.unique_consecutive(x,          return_inverse=False,          return_counts=False,          axis=None,          dtype='int64',          name=None)[source] 
paddle.unsqueeze(x,          axis,          name=None)[source] 
paddle.unsqueeze_(x,          axis,          name=None)[source] 
paddle.unstack(x,          axis=0,          num=None)[source] 
paddle.var(x,          axis=None,          unbiased=True,          keepdim=False,          name=None)[source] 
paddle.where(condition,          x=None,          y=None,          name=None)[source] 
paddle.zeros(shape,          dtype=None,          name=None)[source] 
paddle.zeros_like(x,          dtype=None,          name=None)[source] 
paddle.amp.auto_cast(enable=True,          custom_white_list=None,          custom_black_list=None,          level='O1',          dtype='float16')[source] 
paddle.amp.decorate(models,          optimizers=None,          level='O1',          master_weight=None,          save_dtype=None)[source] 
class paddle.amp.GradScaler(enable=True,          init_loss_scaling=32768.0,          incr_ratio=2.0,          decr_ratio=0.5,          incr_every_n_steps=1000,          decr_every_n_nan_or_inf=2,          use_dynamic_loss_scaling=True)[source] scale(var)scale¶ minimize(optimizer,            *args,            **kwargs)minimize¶ step(optimizer)step¶ update()update¶ unscale_(optimizer)unscale_¶ is_enable()is_enable¶ is_use_dynamic_loss_scaling()is_use_dynamic_loss_scaling¶ get_init_loss_scaling()get_init_loss_scaling¶ set_init_loss_scaling(new_init_loss_scaling)set_init_loss_scaling¶ get_incr_ratio()get_incr_ratio¶ set_incr_ratio(new_incr_ratio)set_incr_ratio¶ get_decr_ratio()get_decr_ratio¶ set_decr_ratio(new_decr_ratio)set_decr_ratio¶ get_incr_every_n_steps()get_incr_every_n_steps¶ set_incr_every_n_steps(new_incr_every_n_steps)set_incr_every_n_steps¶ get_decr_every_n_nan_or_inf()get_decr_every_n_nan_or_inf¶ set_decr_every_n_nan_or_inf(new_decr_every_n_nan_or_inf)set_decr_every_n_nan_or_inf¶ state_dict()state_dict¶ load_state_dict(state_dict)load_state_dict¶ 




paddle.audio.info(filepath: str)→ paddle.audio.backends.backend.AudioInfo[source] 
paddle.audio.load(filepath: Union[str, pathlib.Path],          frame_offset: int = 0,          num_frames: int = - 1,          normalize: bool = True,          channels_first: bool = True)→ Tuple[paddle.Tensor, int][source] 
paddle.audio.save(filepath: str,          src: paddle.Tensor,          sample_rate: int,          channels_first: bool = True,          encoding: Optional[str] = None,          bits_per_sample: Optional[int] = 16)[source] 
paddle.autograd.backward(tensors,          grad_tensors=None,          retain_graph=False)[source] 
paddle.autograd.PyLayer[source] 
paddle.autograd.PyLayerContext[source] 
class paddle.callbacks.Callback[source] set_params(params)set_params¶ set_model(model)set_model¶ on_train_begin(logs=None)on_train_begin¶ on_train_end(logs=None)on_train_end¶ on_eval_begin(logs=None)on_eval_begin¶ on_eval_end(logs=None)on_eval_end¶ on_predict_begin(logs=None)on_predict_begin¶ on_predict_end(logs=None)on_predict_end¶ on_epoch_begin(epoch,            logs=None)on_epoch_begin¶ on_epoch_end(epoch,            logs=None)on_epoch_end¶ on_train_batch_begin(step,            logs=None)on_train_batch_begin¶ on_train_batch_end(step,            logs=None)on_train_batch_end¶ on_eval_batch_begin(step,            logs=None)on_eval_batch_begin¶ on_eval_batch_end(step,            logs=None)on_eval_batch_end¶ on_predict_batch_begin(step,            logs=None)on_predict_batch_begin¶ on_predict_batch_end(step,            logs=None)on_predict_batch_end¶ 
class paddle.callbacks.EarlyStopping(monitor='loss',          mode='auto',          patience=0,          verbose=1,          min_delta=0,          baseline=None,          save_best_model=True)[source] on_train_begin(logs=None)on_train_begin¶ on_eval_end(logs=None)on_eval_end¶ 
class paddle.callbacks.LRScheduler(by_step=True,          by_epoch=False)[source] on_epoch_end(epoch,            logs=None)on_epoch_end¶ on_train_batch_end(step,            logs=None)on_train_batch_end¶ 
class paddle.callbacks.ModelCheckpoint(save_freq=1,          save_dir=None)[source] on_epoch_begin(epoch=None,            logs=None)on_epoch_begin¶ on_epoch_end(epoch,            logs=None)on_epoch_end¶ on_train_end(logs=None)on_train_end¶ 
class paddle.callbacks.ProgBarLogger(log_freq=1,          verbose=2)[source] on_train_begin(logs=None)on_train_begin¶ on_epoch_begin(epoch=None,            logs=None)on_epoch_begin¶ on_train_batch_begin(step,            logs=None)on_train_batch_begin¶ on_train_batch_end(step,            logs=None)on_train_batch_end¶ on_epoch_end(epoch,            logs=None)on_epoch_end¶ on_eval_begin(logs=None)on_eval_begin¶ on_eval_batch_begin(step,            logs=None)on_eval_batch_begin¶ on_eval_batch_end(step,            logs=None)on_eval_batch_end¶ on_predict_begin(logs=None)on_predict_begin¶ on_predict_batch_begin(step,            logs=None)on_predict_batch_begin¶ on_predict_batch_end(step,            logs=None)on_predict_batch_end¶ on_eval_end(logs=None)on_eval_end¶ on_predict_end(logs=None)on_predict_end¶ 
class paddle.callbacks.ReduceLROnPlateau(monitor='loss',          factor=0.1,          patience=10,          verbose=1,          mode='auto',          min_delta=0.0001,          cooldown=0,          min_lr=0)[source] on_train_begin(logs=None)on_train_begin¶ on_eval_end(logs=None)on_eval_end¶ 
class paddle.callbacks.VisualDL(log_dir)[source] on_train_begin(logs=None)on_train_begin¶ on_epoch_begin(epoch=None,            logs=None)on_epoch_begin¶ on_train_batch_end(step,            logs=None)on_train_batch_end¶ on_eval_begin(logs=None)on_eval_begin¶ on_train_end(logs=None)on_train_end¶ on_eval_end(logs=None)on_eval_end¶ 

paddle.device.get_all_custom_device_type()[source] 
paddle.device.get_all_device_type()[source] 
paddle.device.get_available_custom_device()[source] 
paddle.device.get_available_device()[source] 
paddle.device.get_cudnn_version()[source] 
paddle.device.get_device()[source] 
paddle.device.IPUPlace()[source] 
paddle.device.is_compiled_with_cinn()[source] 
paddle.device.is_compiled_with_cuda()[source] 
paddle.device.is_compiled_with_ipu()[source] 
paddle.device.is_compiled_with_mlu()[source] 
paddle.device.is_compiled_with_npu()[source] 
paddle.device.is_compiled_with_rocm()[source] 
paddle.device.is_compiled_with_xpu()[source] 
paddle.device.MLUPlace(dev_id)[source] 
paddle.device.set_device(device)[source] 
paddle.device.XPUPlace(dev_id)[source] 
paddle.distributed.all_gather(tensor_list,          tensor,          group=None,          sync_op=True)[source] 
paddle.distributed.all_gather_object(object_list,          obj,          group=None)[source] 
paddle.distributed.all_reduce(tensor,          op=0,          group=None,          sync_op=True)[source] 
paddle.distributed.alltoall(in_tensor_list,          out_tensor_list,          group=None,          sync_op=True)[source] 
paddle.distributed.alltoall_single(in_tensor,          out_tensor,          in_split_sizes=None,          out_split_sizes=None,          group=None,          sync_op=True)[source] 
paddle.distributed.barrier(group=None)[source] 
paddle.distributed.broadcast(tensor,          src,          group=None,          sync_op=True)[source] 

class paddle.distributed.CountFilterEntry(count_filter)[source] 
paddle.distributed.destroy_process_group(group=None)[source] 

paddle.distributed.get_group(id=0)[source] 
paddle.distributed.get_rank(group=None)[source] 
paddle.distributed.get_world_size(group=None)[source] 
paddle.distributed.gloo_barrier()[source] 
paddle.distributed.gloo_init_parallel_env(rank_id,          rank_num,          server_endpoint)[source] 
paddle.distributed.gloo_release()[source] 
paddle.distributed.init_parallel_env()[source] 
class paddle.distributed.InMemoryDataset[source] update_settings(**kwargs)update_settings¶ init(**kwargs)init¶ set_date(date)set_date¶ load_into_memory(is_shuffle=False)load_into_memory¶ preload_into_memory(thread_num=None)preload_into_memory¶ wait_preload_done()wait_preload_done¶ local_shuffle()local_shuffle¶ global_shuffle(fleet=None,            thread_num=12)global_shuffle¶ release_memory()release_memory¶ get_memory_data_size(fleet=None)get_memory_data_size¶ get_shuffle_data_size(fleet=None)get_shuffle_data_size¶ slots_shuffle(slots)slots_shuffle¶ set_filelist(filelist)set_filelist¶ 
paddle.distributed.irecv(tensor,          src=None,          group=None)[source] 
paddle.distributed.is_initialized()[source] 
paddle.distributed.isend(tensor,          dst,          group=None)[source] 
paddle.distributed.launch()[source] 
paddle.distributed.new_group(ranks=None,          backend=None,          timeout=datetime.timedelta(seconds=1800))[source] 
class paddle.distributed.ParallelEnv[source] property rank property world_size property device_id property device_type property current_endpoint property trainer_endpoints property nrings property local_rank property nranks property dev_id 
class paddle.distributed.ParallelMode[source] 

class paddle.distributed.ProbabilityEntry(probability)[source] 

class paddle.distributed.QueueDataset[source] init(**kwargs)init¶ set_filelist(filelist)set_filelist¶ 
paddle.distributed.recv(tensor,          src=0,          group=None,          sync_op=True)[source] 
paddle.distributed.reduce(tensor,          dst,          op=0,          group=None,          sync_op=True)[source] 
paddle.distributed.reduce_scatter(tensor,          tensor_list,          op=0,          group=None,          sync_op=True)[source] 
class paddle.distributed.ReduceOp[source] 
paddle.distributed.scatter(tensor,          tensor_list=None,          src=0,          group=None,          sync_op=True)[source] 
paddle.distributed.send(tensor,          dst=0,          group=None,          sync_op=True)[source] 

class paddle.distributed.ShowClickEntry(show_name,          click_name)[source] 
paddle.distributed.spawn(func,          args=(),          nprocs=- 1,          join=True,          daemon=False,          **options)[source] 
paddle.distributed.split(x,          size,          operation,          axis=0,          num_partitions=1,          gather_out=True,          weight_attr=None,          bias_attr=None,          name=None)[source] 
paddle.distributed.wait(tensor,          group=None,          use_calc_stream=True)[source] 
class paddle.distribution.AbsTransform[source] forward(x)forward¶ forward_log_det_jacobian(x)forward_log_det_jacobian¶ forward_shape(shape)forward_shape¶ inverse(y)inverse¶ inverse_log_det_jacobian(y)inverse_log_det_jacobian¶ inverse_shape(shape)inverse_shape¶ 
class paddle.distribution.AffineTransform(loc,          scale)[source] forward(x)forward¶ forward_log_det_jacobian(x)forward_log_det_jacobian¶ forward_shape(shape)forward_shape¶ inverse(y)inverse¶ inverse_log_det_jacobian(y)inverse_log_det_jacobian¶ inverse_shape(shape)inverse_shape¶ 
class paddle.distribution.Beta(alpha,          beta)[source] property mean property variance prob(value)prob¶ log_prob(value)log_prob¶ sample(shape=())sample¶ entropy()entropy¶ property batch_shape property event_shape kl_divergence(other)[source]kl_divergence¶ probs(value)probs¶ rsample(shape=())rsample¶ 
class paddle.distribution.Categorical(logits,          name=None)[source] sample(shape)sample¶ property batch_shape property event_shape kl_divergence(other)[source]kl_divergence¶ property mean prob(value)prob¶ rsample(shape=())rsample¶ property variance entropy()entropy¶ probs(value)probs¶ log_prob(value)log_prob¶ 
class paddle.distribution.ChainTransform(transforms)[source] forward(x)forward¶ forward_log_det_jacobian(x)forward_log_det_jacobian¶ forward_shape(shape)forward_shape¶ inverse(y)inverse¶ inverse_log_det_jacobian(y)inverse_log_det_jacobian¶ inverse_shape(shape)inverse_shape¶ 
class paddle.distribution.Dirichlet(concentration)[source] property mean property variance sample(shape=())sample¶ prob(value)prob¶ log_prob(value)log_prob¶ entropy()entropy¶ property batch_shape property event_shape kl_divergence(other)[source]kl_divergence¶ probs(value)probs¶ rsample(shape=())rsample¶ 
class paddle.distribution.Distribution(batch_shape=(),          event_shape=())[source] property batch_shape property event_shape property mean property variance sample(shape=())sample¶ rsample(shape=())rsample¶ entropy()entropy¶ kl_divergence(other)[source]kl_divergence¶ prob(value)prob¶ log_prob(value)log_prob¶ probs(value)probs¶ 
class paddle.distribution.ExponentialFamily(batch_shape=(),          event_shape=())[source] entropy()entropy¶ property batch_shape property event_shape kl_divergence(other)[source]kl_divergence¶ log_prob(value)log_prob¶ property mean prob(value)prob¶ probs(value)probs¶ rsample(shape=())rsample¶ sample(shape=())sample¶ property variance 
class paddle.distribution.ExpTransform[source] forward(x)forward¶ forward_log_det_jacobian(x)forward_log_det_jacobian¶ forward_shape(shape)forward_shape¶ inverse(y)inverse¶ inverse_log_det_jacobian(y)inverse_log_det_jacobian¶ inverse_shape(shape)inverse_shape¶ 
class paddle.distribution.Independent(base,          reinterpreted_batch_rank)[source] property mean property variance sample(shape=())sample¶ log_prob(value)log_prob¶ prob(value)prob¶ entropy()entropy¶ property batch_shape property event_shape kl_divergence(other)[source]kl_divergence¶ probs(value)probs¶ rsample(shape=())rsample¶ 
class paddle.distribution.IndependentTransform(base,          reinterpreted_batch_rank)[source] forward(x)forward¶ forward_log_det_jacobian(x)forward_log_det_jacobian¶ forward_shape(shape)forward_shape¶ inverse(y)inverse¶ inverse_log_det_jacobian(y)inverse_log_det_jacobian¶ inverse_shape(shape)inverse_shape¶ 
paddle.distribution.kl_divergence(p,          q)[source] 
class paddle.distribution.Multinomial(total_count,          probs)[source] probs(value)probs¶ property mean property variance prob(value)prob¶ log_prob(value)log_prob¶ sample(shape=())sample¶ entropy()entropy¶ property batch_shape property event_shape kl_divergence(other)[source]kl_divergence¶ rsample(shape=())rsample¶ 
class paddle.distribution.Normal(loc,          scale,          name=None)[source] property batch_shape property event_shape property mean prob(value)prob¶ rsample(shape=())rsample¶ property variance sample(shape,            seed=0)sample¶ entropy()entropy¶ log_prob(value)log_prob¶ probs(value)probs¶ kl_divergence(other)[source]kl_divergence¶ 
class paddle.distribution.PowerTransform(power)[source] forward(x)forward¶ forward_log_det_jacobian(x)forward_log_det_jacobian¶ forward_shape(shape)forward_shape¶ inverse(y)inverse¶ inverse_log_det_jacobian(y)inverse_log_det_jacobian¶ inverse_shape(shape)inverse_shape¶ 
paddle.distribution.register_kl(cls_p,          cls_q)[source] 
class paddle.distribution.ReshapeTransform(in_event_shape,          out_event_shape)[source] forward(x)forward¶ forward_log_det_jacobian(x)forward_log_det_jacobian¶ forward_shape(shape)forward_shape¶ inverse(y)inverse¶ inverse_log_det_jacobian(y)inverse_log_det_jacobian¶ inverse_shape(shape)inverse_shape¶ 
class paddle.distribution.SigmoidTransform[source] forward(x)forward¶ forward_log_det_jacobian(x)forward_log_det_jacobian¶ forward_shape(shape)forward_shape¶ inverse(y)inverse¶ inverse_log_det_jacobian(y)inverse_log_det_jacobian¶ inverse_shape(shape)inverse_shape¶ 
class paddle.distribution.SoftmaxTransform[source] forward(x)forward¶ forward_log_det_jacobian(x)forward_log_det_jacobian¶ forward_shape(shape)forward_shape¶ inverse(y)inverse¶ inverse_log_det_jacobian(y)inverse_log_det_jacobian¶ inverse_shape(shape)inverse_shape¶ 
class paddle.distribution.StackTransform(transforms,          axis=0)[source] forward(x)forward¶ forward_log_det_jacobian(x)forward_log_det_jacobian¶ forward_shape(shape)forward_shape¶ inverse(y)inverse¶ inverse_log_det_jacobian(y)inverse_log_det_jacobian¶ inverse_shape(shape)inverse_shape¶ 
class paddle.distribution.StickBreakingTransform[source] forward(x)forward¶ forward_log_det_jacobian(x)forward_log_det_jacobian¶ forward_shape(shape)forward_shape¶ inverse(y)inverse¶ inverse_log_det_jacobian(y)inverse_log_det_jacobian¶ inverse_shape(shape)inverse_shape¶ 
class paddle.distribution.TanhTransform[source] forward(x)forward¶ forward_log_det_jacobian(x)forward_log_det_jacobian¶ forward_shape(shape)forward_shape¶ inverse(y)inverse¶ inverse_log_det_jacobian(y)inverse_log_det_jacobian¶ inverse_shape(shape)inverse_shape¶ 
class paddle.distribution.Transform[source] forward(x)forward¶ inverse(y)inverse¶ forward_log_det_jacobian(x)forward_log_det_jacobian¶ inverse_log_det_jacobian(y)inverse_log_det_jacobian¶ forward_shape(shape)forward_shape¶ inverse_shape(shape)inverse_shape¶ 
class paddle.distribution.TransformedDistribution(base,          transforms)[source] sample(shape=())sample¶ log_prob(value)log_prob¶ property batch_shape entropy()entropy¶ property event_shape kl_divergence(other)[source]kl_divergence¶ property mean prob(value)prob¶ probs(value)probs¶ rsample(shape=())rsample¶ property variance 
class paddle.distribution.Uniform(low,          high,          name=None)[source] property batch_shape property event_shape kl_divergence(other)[source]kl_divergence¶ property mean prob(value)prob¶ rsample(shape=())rsample¶ property variance sample(shape,            seed=0)sample¶ log_prob(value)log_prob¶ probs(value)probs¶ entropy()entropy¶ 
paddle.fft.fft(x,          n=None,          axis=- 1,          norm='backward',          name=None)[source] 
paddle.fft.fft2(x,          s=None,          axes=(- 2, - 1),          norm='backward',          name=None)[source] 
paddle.fft.fftfreq(n,          d=1.0,          dtype=None,          name=None)[source] 
paddle.fft.fftn(x,          s=None,          axes=None,          norm='backward',          name=None)[source] 
paddle.fft.fftshift(x,          axes=None,          name=None)[source] 
paddle.fft.hfft(x,          n=None,          axis=- 1,          norm='backward',          name=None)[source] 
paddle.fft.hfft2(x,          s=None,          axes=(- 2, - 1),          norm='backward',          name=None)[source] 
paddle.fft.hfftn(x,          s=None,          axes=None,          norm='backward',          name=None)[source] 
paddle.fft.ifft(x,          n=None,          axis=- 1,          norm='backward',          name=None)[source] 
paddle.fft.ifft2(x,          s=None,          axes=(- 2, - 1),          norm='backward',          name=None)[source] 
paddle.fft.ifftn(x,          s=None,          axes=None,          norm='backward',          name=None)[source] 
paddle.fft.ifftshift(x,          axes=None,          name=None)[source] 
paddle.fft.ihfft(x,          n=None,          axis=- 1,          norm='backward',          name=None)[source] 
paddle.fft.ihfft2(x,          s=None,          axes=(- 2, - 1),          norm='backward',          name=None)[source] 
paddle.fft.ihfftn(x,          s=None,          axes=None,          norm='backward',          name=None)[source] 
paddle.fft.irfft(x,          n=None,          axis=- 1,          norm='backward',          name=None)[source] 
paddle.fft.irfft2(x,          s=None,          axes=(- 2, - 1),          norm='backward',          name=None)[source] 
paddle.fft.irfftn(x,          s=None,          axes=None,          norm='backward',          name=None)[source] 
paddle.fft.rfft(x,          n=None,          axis=- 1,          norm='backward',          name=None)[source] 
paddle.fft.rfft2(x,          s=None,          axes=(- 2, - 1),          norm='backward',          name=None)[source] 
paddle.fft.rfftfreq(n,          d=1.0,          dtype=None,          name=None)[source] 
paddle.fft.rfftn(x,          s=None,          axes=None,          norm='backward',          name=None)[source] 




paddle.fluid.data(name,          shape,          dtype='float32',          lod_level=0)[source] 




























paddle.geometric.reindex_graph(x,          neighbors,          count,          value_buffer=None,          index_buffer=None,          name=None)[source] 
paddle.geometric.reindex_heter_graph(x,          neighbors,          count,          value_buffer=None,          index_buffer=None,          name=None)[source] 
paddle.geometric.sample_neighbors(row,          colptr,          input_nodes,          sample_size=- 1,          eids=None,          return_eids=False,          perm_buffer=None,          name=None)[source] 
paddle.geometric.segment_max(data,          segment_ids,          name=None)[source] 
paddle.geometric.segment_mean(data,          segment_ids,          name=None)[source] 
paddle.geometric.segment_min(data,          segment_ids,          name=None)[source] 
paddle.geometric.segment_sum(data,          segment_ids,          name=None)[source] 
paddle.geometric.send_u_recv(x,          src_index,          dst_index,          reduce_op='sum',          out_size=None,          name=None)[source] 
paddle.geometric.send_ue_recv(x,          y,          src_index,          dst_index,          message_op='add',          reduce_op='sum',          out_size=None,          name=None)[source] 
paddle.geometric.send_uv(x,          y,          src_index,          dst_index,          message_op='add',          name=None)[source] 
paddle.hub.help(repo_dir,          model,          source='github',          force_reload=False)[source] 
paddle.hub.list(repo_dir,          source='github',          force_reload=False)[source] 
paddle.hub.load(repo_dir,          model,          source='github',          force_reload=False,          **kwargs)[source] 



paddle.incubate.graph_khop_sampler(row,          colptr,          input_nodes,          sample_sizes,          sorted_eids=None,          return_eids=False,          name=None)[source] 
paddle.incubate.graph_reindex(x,          neighbors,          count,          value_buffer=None,          index_buffer=None,          flag_buffer_hashtable=False,          name=None)[source] 
paddle.incubate.graph_sample_neighbors(row,          colptr,          input_nodes,          eids=None,          perm_buffer=None,          sample_size=- 1,          return_eids=False,          flag_perm_buffer=False,          name=None)[source] 
paddle.incubate.graph_send_recv(x,          src_index,          dst_index,          pool_type='sum',          out_size=None,          name=None)[source] 
paddle.incubate.identity_loss(x,          reduction='none')[source] 
class paddle.incubate.LookAhead(inner_optimizer,          alpha=0.5,          k=5,          name=None)[source] step()step¶ minimize(loss,            startup_program=None,            parameters=None,            no_grad_set=None)minimize¶ append_regularization_ops(parameters_and_grads,            regularization=None)append_regularization_ops¶ apply_gradients(params_grads)apply_gradients¶ backward(loss,            startup_program=None,            parameters=None,            no_grad_set=None,            callbacks=None)backward¶ clear_grad(set_to_zero=True)clear_grad¶ get_lr()get_lr¶ set_lr(value)set_lr¶ set_state_dict(state_dict)set_state_dict¶ state_dict()state_dict¶ 
class paddle.incubate.ModelAverage(average_window_rate,          parameters=None,          min_average_window=10000,          max_average_window=10000,          name=None)[source] minimize(loss,            startup_program=None,            parameters=None,            no_grad_set=None)minimize¶ step()step¶ apply(executor=None,            need_restore=True)apply¶ restore(executor=None)restore¶ append_regularization_ops(parameters_and_grads,            regularization=None)append_regularization_ops¶ apply_gradients(params_grads)apply_gradients¶ backward(loss,            startup_program=None,            parameters=None,            no_grad_set=None,            callbacks=None)backward¶ clear_grad(set_to_zero=True)clear_grad¶ get_lr()get_lr¶ set_lr(value)set_lr¶ set_state_dict(state_dict)set_state_dict¶ state_dict()state_dict¶ 


paddle.incubate.segment_max(data,          segment_ids,          name=None)[source] 
paddle.incubate.segment_mean(data,          segment_ids,          name=None)[source] 
paddle.incubate.segment_min(data,          segment_ids,          name=None)[source] 
paddle.incubate.segment_sum(data,          segment_ids,          name=None)[source] 
paddle.incubate.softmax_mask_fuse(x,          mask,          name=None)[source] 
paddle.incubate.softmax_mask_fuse_upper_triangle(x)[source] 

paddle.inference.Config 
paddle.inference.convert_to_mixed_precision(model_file: str,          params_file: str,          mixed_model_file: str,          mixed_params_file: str,          mixed_precision: paddle.fluid.libpaddle.AnalysisConfig.Precision,          backend: paddle.fluid.libpaddle.PaddlePlace,          keep_io_types: bool = True,          black_list: Set = {})[source] 
paddle.inference.DataType 
paddle.inference.PlaceType 
paddle.inference.PrecisionType 
paddle.inference.Predictor 
class paddle.inference.PredictorPool retrive(self: paddle.fluid.libpaddle.PredictorPool,            arg0: int)→ paddle.fluid.libpaddle.PaddleInferPredictorretrive¶ 
paddle.inference.Tensor 
class paddle.io.BatchSampler(dataset=None,          sampler=None,          shuffle=False,          batch_size=1,          drop_last=False)[source] 
class paddle.io.ChainDataset(datasets)[source] 
class paddle.io.ComposeDataset(datasets)[source] 
class paddle.io.DataLoader(dataset,          feed_list=None,          places=None,          return_list=True,          batch_sampler=None,          batch_size=1,          shuffle=False,          drop_last=False,          collate_fn=None,          num_workers=0,          use_buffer_reader=True,          prefetch_factor=2,          use_shared_memory=True,          timeout=0,          worker_init_fn=None,          persistent_workers=False)[source] static from_generator(feed_list=None,            capacity=None,            use_double_buffer=True,            iterable=True,            return_list=False,            use_multiprocess=False,            drop_last=True)from_generator¶ static from_dataset(dataset,            places,            drop_last=True)from_dataset¶ 
class paddle.io.Dataset[source] 
class paddle.io.DistributedBatchSampler(dataset,          batch_size,          num_replicas=None,          rank=None,          shuffle=False,          drop_last=False)[source] set_epoch(epoch)set_epoch¶ 
paddle.io.get_worker_info()[source] 
class paddle.io.IterableDataset[source] 
paddle.io.random_split(dataset,          lengths,          generator=None)[source] 
class paddle.io.RandomSampler(data_source,          replacement=False,          num_samples=None,          generator=None)[source] 
class paddle.io.Sampler(data_source=None)[source] 
class paddle.io.SequenceSampler(data_source)[source] 
class paddle.io.Subset(dataset,          indices)[source] 
class paddle.io.TensorDataset(tensors)[source] 
class paddle.io.WeightedRandomSampler(weights,          num_samples,          replacement=True)[source] 
paddle.jit.load(path,          **configs)[source] 
paddle.jit.not_to_static(func=None)[source] 
class paddle.jit.ProgramTranslator(**kwargs)[source] enable(enable_to_static)enable¶ get_output(dygraph_func,            *args,            **kwargs)get_output¶ get_func(dygraph_func)get_func¶ get_program(dygraph_func,            *args,            **kwargs)get_program¶ get_code(dygraph_func)get_code¶ get_program_cache()get_program_cache¶ 
paddle.jit.save(layer,          path,          input_spec=None,          **configs)[source] 
paddle.jit.set_code_level(level=100,          also_to_stdout=False)[source] 
paddle.jit.set_verbosity(level=0,          also_to_stdout=False)[source] 
paddle.jit.to_static(function=None,          input_spec=None,          build_strategy=None,          property=False)[source] 
class paddle.jit.TracedLayer(program,          parameters,          feed_names,          fetch_names)[source] static trace(layer,            inputs)trace¶ set_strategy(build_strategy=None,            exec_strategy=None)set_strategy¶ save_inference_model(path,            feed=None,            fetch=None,            **kwargs)save_inference_model¶ 
class paddle.jit.TranslatedLayer(programs,          persistable_vars)[source] train()train¶ eval()eval¶ program(method_name='forward')program¶ add_parameter(name,            parameter)add_parameter¶ add_sublayer(name,            sublayer)add_sublayer¶ apply(fn)apply¶ buffers(include_sublayers=True)buffers¶ children()children¶ clear_gradients()clear_gradients¶ create_parameter(shape,            attr=None,            dtype=None,            is_bias=False,            default_initializer=None)create_parameter¶ create_tensor(name=None,            persistable=None,            dtype=None)create_tensor¶ create_variable(name=None,            persistable=None,            dtype=None)create_variable¶ extra_repr()extra_repr¶ forward(*inputs,            **kwargs)forward¶ full_name()full_name¶ load_dict(state_dict,            use_structured_name=True)load_dict¶ named_buffers(prefix='',            include_sublayers=True)named_buffers¶ named_children()named_children¶ named_parameters(prefix='',            include_sublayers=True)named_parameters¶ named_sublayers(prefix='',            include_self=False,            layers_set=None)named_sublayers¶ parameters(include_sublayers=True)parameters¶ register_buffer(name,            tensor,            persistable=True)register_buffer¶ register_forward_post_hook(hook)register_forward_post_hook¶ register_forward_pre_hook(hook)register_forward_pre_hook¶ set_dict(state_dict,            use_structured_name=True)set_dict¶ set_state_dict(state_dict,            use_structured_name=True)set_state_dict¶ state_dict(destination=None,            include_sublayers=True,            structured_name_prefix='',            use_hook=True)state_dict¶ sublayers(include_self=False)sublayers¶ to(device=None,            dtype=None,            blocking=None)to¶ to_static_state_dict(destination=None,            include_sublayers=True,            structured_name_prefix='',            use_hook=True)to_static_state_dict¶ 
paddle.linalg.cholesky(x,          upper=False,          name=None)[source] 
paddle.linalg.cholesky_solve(x,          y,          upper=False,          name=None)[source] 
paddle.linalg.cond(x,          p=None,          name=None)[source] 
paddle.linalg.corrcoef(x,          rowvar=True,          name=None)[source] 
paddle.linalg.cov(x,          rowvar=True,          ddof=True,          fweights=None,          aweights=None,          name=None)[source] 
paddle.linalg.det(x,          name=None)[source] 
paddle.linalg.eig(x,          name=None)[source] 
paddle.linalg.eigh(x,          UPLO='L',          name=None)[source] 
paddle.linalg.eigvals(x,          name=None)[source] 
paddle.linalg.eigvalsh(x,          UPLO='L',          name=None)[source] 
paddle.linalg.inv(x,          name=None)[source] 
paddle.linalg.lstsq(x,          y,          rcond=None,          driver=None,          name=None)[source] 
paddle.linalg.lu(x,          pivot=True,          get_infos=False,          name=None)[source] 
paddle.linalg.lu_unpack(x,          y,          unpack_ludata=True,          unpack_pivots=True,          name=None)[source] 
paddle.linalg.matrix_power(x,          n,          name=None)[source] 
paddle.linalg.matrix_rank(x,          tol=None,          hermitian=False,          name=None)[source] 
paddle.linalg.multi_dot(x,          name=None)[source] 
paddle.linalg.norm(x,          p='fro',          axis=None,          keepdim=False,          name=None)[source] 
paddle.linalg.pinv(x,          rcond=1e-15,          hermitian=False,          name=None)[source] 
paddle.linalg.qr(x,          mode='reduced',          name=None)[source] 
paddle.linalg.slogdet(x,          name=None)[source] 
paddle.linalg.solve(x,          y,          name=None)[source] 
paddle.linalg.svd(x,          full_matrices=False,          name=None)[source] 
paddle.linalg.triangular_solve(x,          y,          upper=True,          transpose=False,          unitriangular=False,          name=None)[source] 
class paddle.metric.Accuracy(topk=(1,),          name=None,          *args,          **kwargs)[source] compute(pred,            label,            *args)compute¶ update(correct,            *args)update¶ reset()reset¶ accumulate()accumulate¶ name()name¶ 
paddle.metric.accuracy(input,          label,          k=1,          correct=None,          total=None,          name=None)[source] 
class paddle.metric.Auc(curve='ROC',          num_thresholds=4095,          name='auc',          *args,          **kwargs)[source] update(preds,            labels)update¶ accumulate()accumulate¶ reset()reset¶ name()name¶ compute(*args)compute¶ 
class paddle.metric.Metric[source] abstract reset()reset¶ abstract update(*args)update¶ abstract accumulate()accumulate¶ abstract name()name¶ compute(*args)compute¶ 
class paddle.metric.Precision(name='precision',          *args,          **kwargs)[source] update(preds,            labels)update¶ reset()reset¶ accumulate()accumulate¶ name()name¶ compute(*args)compute¶ 
class paddle.metric.Recall(name='recall',          *args,          **kwargs)[source] update(preds,            labels)update¶ accumulate()accumulate¶ reset()reset¶ name()name¶ compute(*args)compute¶ 
class paddle.nn.AdaptiveAvgPool1D(output_size,          name=None)[source] forward(input)forward¶ extra_repr()extra_repr¶ 
class paddle.nn.AdaptiveAvgPool2D(output_size,          data_format='NCHW',          name=None)[source] forward(x)forward¶ extra_repr()extra_repr¶ 
class paddle.nn.AdaptiveAvgPool3D(output_size,          data_format='NCDHW',          name=None)[source] forward(x)forward¶ extra_repr()extra_repr¶ 
class paddle.nn.AdaptiveMaxPool1D(output_size,          return_mask=False,          name=None)[source] forward(input)forward¶ extra_repr()extra_repr¶ 
class paddle.nn.AdaptiveMaxPool2D(output_size,          return_mask=False,          name=None)[source] forward(x)forward¶ extra_repr()extra_repr¶ 
class paddle.nn.AdaptiveMaxPool3D(output_size,          return_mask=False,          name=None)[source] forward(x)forward¶ extra_repr()extra_repr¶ 
class paddle.nn.AlphaDropout(p=0.5,          name=None)[source] forward(input)forward¶ extra_repr()extra_repr¶ 
class paddle.nn.AvgPool1D(kernel_size,          stride=None,          padding=0,          exclusive=True,          ceil_mode=False,          name=None)[source] forward(x)forward¶ extra_repr()extra_repr¶ 
class paddle.nn.AvgPool2D(kernel_size,          stride=None,          padding=0,          ceil_mode=False,          exclusive=True,          divisor_override=None,          data_format='NCHW',          name=None)[source] forward(x)forward¶ extra_repr()extra_repr¶ 
class paddle.nn.AvgPool3D(kernel_size,          stride=None,          padding=0,          ceil_mode=False,          exclusive=True,          divisor_override=None,          data_format='NCDHW',          name=None)[source] forward(x)forward¶ extra_repr()extra_repr¶ 
class paddle.nn.BatchNorm(num_channels,          act=None,          is_test=False,          momentum=0.9,          epsilon=1e-05,          param_attr=None,          bias_attr=None,          dtype='float32',          data_layout='NCHW',          in_place=False,          moving_mean_name=None,          moving_variance_name=None,          do_model_average_for_mean_and_var=True,          use_global_stats=False,          trainable_statistics=False)[source] forward(input)forward¶ 
class paddle.nn.BatchNorm1D(num_features,          momentum=0.9,          epsilon=1e-05,          weight_attr=None,          bias_attr=None,          data_format='NCL',          use_global_stats=None,          name=None)[source] 
class paddle.nn.BatchNorm2D(num_features,          momentum=0.9,          epsilon=1e-05,          weight_attr=None,          bias_attr=None,          data_format='NCHW',          use_global_stats=None,          name=None)[source] 
class paddle.nn.BatchNorm3D(num_features,          momentum=0.9,          epsilon=1e-05,          weight_attr=None,          bias_attr=None,          data_format='NCDHW',          use_global_stats=None,          name=None)[source] 
class paddle.nn.BCELoss(weight=None,          reduction='mean',          name=None)[source] forward(input,            label)forward¶ 
class paddle.nn.BCEWithLogitsLoss(weight=None,          reduction='mean',          pos_weight=None,          name=None)[source] forward(logit,            label)forward¶ 
class paddle.nn.BeamSearchDecoder(cell,          start_token,          end_token,          beam_size,          embedding_fn=None,          output_fn=None)[source] static tile_beam_merge_with_batch(x,            beam_size)tile_beam_merge_with_batch¶ class OutputWrapper(scores,            predicted_ids,            parent_ids) class StateWrapper(cell_states,            log_probs,            finished,            lengths) initialize(initial_cell_states)initialize¶ step(time,            inputs,            states,            **kwargs)step¶ finalize(outputs,            final_states,            sequence_lengths)finalize¶ property tracks_own_finished 
class paddle.nn.Bilinear(in1_features,          in2_features,          out_features,          weight_attr=None,          bias_attr=None,          name=None)[source] forward(x1,            x2)forward¶ extra_repr()extra_repr¶ 
class paddle.nn.BiRNN(cell_fw,          cell_bw,          time_major=False)[source] forward(inputs,            initial_states=None,            sequence_length=None,            **kwargs)forward¶ 
class paddle.nn.CELU(alpha=1.0,          name=None)[source] forward(x)forward¶ extra_repr()extra_repr¶ 
class paddle.nn.ChannelShuffle(groups,          data_format='NCHW',          name=None)[source] forward(x)forward¶ extra_repr()extra_repr¶ 
class paddle.nn.ClipGradByGlobalNorm(clip_norm,          group_name='default_group',          auto_skip_clip=False)[source] 
class paddle.nn.ClipGradByNorm(clip_norm)[source] 
class paddle.nn.ClipGradByValue(max,          min=None)[source] 
class paddle.nn.Conv1D(in_channels,          out_channels,          kernel_size,          stride=1,          padding=0,          dilation=1,          groups=1,          padding_mode='zeros',          weight_attr=None,          bias_attr=None,          data_format='NCL')[source] forward(x)forward¶ 
class paddle.nn.Conv1DTranspose(in_channels,          out_channels,          kernel_size,          stride=1,          padding=0,          output_padding=0,          groups=1,          dilation=1,          weight_attr=None,          bias_attr=None,          data_format='NCL')[source] forward(x,            output_size=None)forward¶ 
class paddle.nn.Conv2D(in_channels,          out_channels,          kernel_size,          stride=1,          padding=0,          dilation=1,          groups=1,          padding_mode='zeros',          weight_attr=None,          bias_attr=None,          data_format='NCHW')[source] forward(x)forward¶ 
class paddle.nn.Conv2DTranspose(in_channels,          out_channels,          kernel_size,          stride=1,          padding=0,          output_padding=0,          dilation=1,          groups=1,          weight_attr=None,          bias_attr=None,          data_format='NCHW')[source] forward(x,            output_size=None)forward¶ 
class paddle.nn.Conv3D(in_channels,          out_channels,          kernel_size,          stride=1,          padding=0,          dilation=1,          groups=1,          padding_mode='zeros',          weight_attr=None,          bias_attr=None,          data_format='NCDHW')[source] forward(x)forward¶ 
class paddle.nn.Conv3DTranspose(in_channels,          out_channels,          kernel_size,          stride=1,          padding=0,          output_padding=0,          dilation=1,          groups=1,          weight_attr=None,          bias_attr=None,          data_format='NCDHW')[source] forward(x,            output_size=None)forward¶ 
class paddle.nn.CosineEmbeddingLoss(margin=0,          reduction='mean',          name=None)[source] forward(input1,            input2,            label)forward¶ 
class paddle.nn.CosineSimilarity(axis=1,          eps=1e-08)[source] forward(x1,            x2)forward¶ extra_repr()extra_repr¶ 
class paddle.nn.CrossEntropyLoss(weight=None,          ignore_index=- 100,          reduction='mean',          soft_label=False,          axis=- 1,          use_softmax=True,          name=None)[source] forward(input,            label)forward¶ 
class paddle.nn.CTCLoss(blank=0,          reduction='mean')[source] forward(log_probs,            labels,            input_lengths,            label_lengths,            norm_by_times=False)forward¶ 
class paddle.nn.Dropout(p=0.5,          axis=None,          mode='upscale_in_train',          name=None)[source] forward(input)forward¶ extra_repr()extra_repr¶ 
class paddle.nn.Dropout2D(p=0.5,          data_format='NCHW',          name=None)[source] forward(input)forward¶ extra_repr()extra_repr¶ 
class paddle.nn.Dropout3D(p=0.5,          data_format='NCDHW',          name=None)[source] forward(input)forward¶ extra_repr()extra_repr¶ 
paddle.nn.dynamic_decode(decoder,          inits=None,          max_step_num=None,          output_time_major=False,          impute_finished=False,          is_test=False,          return_length=False,          **kwargs)[source] 
class paddle.nn.ELU(alpha=1.0,          name=None)[source] forward(x)forward¶ extra_repr()extra_repr¶ 
class paddle.nn.Embedding(num_embeddings,          embedding_dim,          padding_idx=None,          sparse=False,          weight_attr=None,          name=None)[source] forward(x)forward¶ extra_repr()extra_repr¶ 
class paddle.nn.Flatten(start_axis=1,          stop_axis=- 1)[source] forward(input)forward¶ 
class paddle.nn.Fold(output_sizes,          kernel_sizes,          dilations=1,          paddings=0,          strides=1,          name=None)[source] forward(input)forward¶ extra_repr()extra_repr¶ 

class paddle.nn.GELU(approximate=False,          name=None)[source] forward(x)forward¶ extra_repr()extra_repr¶ 
class paddle.nn.GroupNorm(num_groups,          num_channels,          epsilon=1e-05,          weight_attr=None,          bias_attr=None,          data_format='NCHW',          name=None)[source] forward(input)forward¶ extra_repr()extra_repr¶ 
class paddle.nn.GRU(input_size,          hidden_size,          num_layers=1,          direction='forward',          time_major=False,          dropout=0.0,          weight_ih_attr=None,          weight_hh_attr=None,          bias_ih_attr=None,          bias_hh_attr=None,          name=None)[source] 
class paddle.nn.GRUCell(input_size,          hidden_size,          weight_ih_attr=None,          weight_hh_attr=None,          bias_ih_attr=None,          bias_hh_attr=None,          name=None)[source] forward(inputs,            states=None)forward¶ property state_shape extra_repr()extra_repr¶ 
class paddle.nn.Hardshrink(threshold=0.5,          name=None)[source] forward(x)forward¶ extra_repr()extra_repr¶ 
class paddle.nn.Hardsigmoid(name=None)[source] forward(x)forward¶ extra_repr()extra_repr¶ 
class paddle.nn.Hardswish(name=None)[source] forward(x)forward¶ extra_repr()extra_repr¶ 
class paddle.nn.Hardtanh(min=- 1.0,          max=1.0,          name=None)[source] forward(x)forward¶ extra_repr()extra_repr¶ 
class paddle.nn.HingeEmbeddingLoss(margin=1.0,          reduction='mean',          name=None)[source] forward(input,            label)forward¶ 
class paddle.nn.HSigmoidLoss(feature_size,          num_classes,          weight_attr=None,          bias_attr=None,          is_custom=False,          is_sparse=False,          name=None)[source] forward(input,            label,            path_table=None,            path_code=None)forward¶ 
class paddle.nn.Identity(*args,          **kwargs)[source] forward(input)forward¶ 

class paddle.nn.InstanceNorm1D(num_features,          epsilon=1e-05,          momentum=0.9,          weight_attr=None,          bias_attr=None,          data_format='NCHW',          name=None)[source] 
class paddle.nn.InstanceNorm2D(num_features,          epsilon=1e-05,          momentum=0.9,          weight_attr=None,          bias_attr=None,          data_format='NCHW',          name=None)[source] 
class paddle.nn.InstanceNorm3D(num_features,          epsilon=1e-05,          momentum=0.9,          weight_attr=None,          bias_attr=None,          data_format='NCHW',          name=None)[source] 
class paddle.nn.KLDivLoss(reduction='mean')[source] forward(input,            label)forward¶ 
class paddle.nn.L1Loss(reduction='mean',          name=None)[source] forward(input,            label)forward¶ 
class paddle.nn.Layer(name_scope=None,          dtype='float32')[source] train()train¶ eval()eval¶ apply(fn)apply¶ full_name()full_name¶ register_forward_post_hook(hook)register_forward_post_hook¶ register_forward_pre_hook(hook)register_forward_pre_hook¶ create_parameter(shape,            attr=None,            dtype=None,            is_bias=False,            default_initializer=None)create_parameter¶ create_variable(name=None,            persistable=None,            dtype=None)create_variable¶ create_tensor(name=None,            persistable=None,            dtype=None)create_tensor¶ parameters(include_sublayers=True)parameters¶ children()children¶ named_children()named_children¶ sublayers(include_self=False)sublayers¶ named_parameters(prefix='',            include_sublayers=True)named_parameters¶ named_sublayers(prefix='',            include_self=False,            layers_set=None)named_sublayers¶ register_buffer(name,            tensor,            persistable=True)register_buffer¶ buffers(include_sublayers=True)buffers¶ named_buffers(prefix='',            include_sublayers=True)named_buffers¶ clear_gradients()clear_gradients¶ forward(*inputs,            **kwargs)forward¶ add_sublayer(name,            sublayer)add_sublayer¶ add_parameter(name,            parameter)add_parameter¶ extra_repr()extra_repr¶ to_static_state_dict(destination=None,            include_sublayers=True,            structured_name_prefix='',            use_hook=True)to_static_state_dict¶ state_dict(destination=None,            include_sublayers=True,            structured_name_prefix='',            use_hook=True)state_dict¶ set_state_dict(state_dict,            use_structured_name=True)set_state_dict¶ to(device=None,            dtype=None,            blocking=None)to¶ set_dict(state_dict,            use_structured_name=True)set_dict¶ load_dict(state_dict,            use_structured_name=True)load_dict¶ 
class paddle.nn.LayerDict(sublayers=None)[source] clear()clear¶ pop(key)pop¶ keys()keys¶ items()items¶ values()values¶ update(sublayers)update¶ 
class paddle.nn.LayerList(sublayers=None)[source] append(sublayer)append¶ insert(index,            sublayer)insert¶ extend(sublayers)extend¶ 
class paddle.nn.LayerNorm(normalized_shape,          epsilon=1e-05,          weight_attr=None,          bias_attr=None,          name=None)[source] forward(input)forward¶ extra_repr()extra_repr¶ 
class paddle.nn.LeakyReLU(negative_slope=0.01,          name=None)[source] forward(x)forward¶ extra_repr()extra_repr¶ 
class paddle.nn.Linear(in_features,          out_features,          weight_attr=None,          bias_attr=None,          name=None)[source] forward(input)forward¶ extra_repr()extra_repr¶ 
class paddle.nn.LocalResponseNorm(size,          alpha=0.0001,          beta=0.75,          k=1.0,          data_format='NCHW',          name=None)[source] forward(input)forward¶ extra_repr()extra_repr¶ 
class paddle.nn.LogSigmoid(name=None)[source] forward(x)forward¶ extra_repr()extra_repr¶ 
class paddle.nn.LogSoftmax(axis=- 1,          name=None)[source] forward(x)forward¶ extra_repr()extra_repr¶ 
class paddle.nn.LSTM(input_size,          hidden_size,          num_layers=1,          direction='forward',          time_major=False,          dropout=0.0,          weight_ih_attr=None,          weight_hh_attr=None,          bias_ih_attr=None,          bias_hh_attr=None,          name=None)[source] 
class paddle.nn.LSTMCell(input_size,          hidden_size,          weight_ih_attr=None,          weight_hh_attr=None,          bias_ih_attr=None,          bias_hh_attr=None,          name=None)[source] forward(inputs,            states=None)forward¶ property state_shape extra_repr()extra_repr¶ 
class paddle.nn.MarginRankingLoss(margin=0.0,          reduction='mean',          name=None)[source] forward(input,            other,            label)forward¶ 
class paddle.nn.Maxout(groups,          axis=1,          name=None)[source] forward(x)forward¶ extra_repr()extra_repr¶ 
class paddle.nn.MaxPool1D(kernel_size,          stride=None,          padding=0,          return_mask=False,          ceil_mode=False,          name=None)[source] forward(input)forward¶ extra_repr()extra_repr¶ 
class paddle.nn.MaxPool2D(kernel_size,          stride=None,          padding=0,          return_mask=False,          ceil_mode=False,          data_format='NCHW',          name=None)[source] forward(x)forward¶ extra_repr()extra_repr¶ 
class paddle.nn.MaxPool3D(kernel_size,          stride=None,          padding=0,          return_mask=False,          ceil_mode=False,          data_format='NCDHW',          name=None)[source] forward(x)forward¶ extra_repr()extra_repr¶ 
class paddle.nn.MaxUnPool1D(kernel_size,          stride=None,          padding=0,          data_format='NCL',          output_size=None,          name=None)[source] forward(x,            indices)forward¶ extra_repr()extra_repr¶ 
class paddle.nn.MaxUnPool2D(kernel_size,          stride=None,          padding=0,          data_format='NCHW',          output_size=None,          name=None)[source] forward(x,            indices)forward¶ extra_repr()extra_repr¶ 
class paddle.nn.MaxUnPool3D(kernel_size,          stride=None,          padding=0,          data_format='NCDHW',          output_size=None,          name=None)[source] forward(x,            indices)forward¶ extra_repr()extra_repr¶ 
class paddle.nn.Mish(name=None)[source] forward(x)forward¶ extra_repr()extra_repr¶ 
class paddle.nn.MSELoss(reduction='mean')[source] forward(input,            label)forward¶ 
class paddle.nn.MultiHeadAttention(embed_dim,          num_heads,          dropout=0.0,          kdim=None,          vdim=None,          need_weights=False,          weight_attr=None,          bias_attr=None)[source] class Cache(k,            v) k v class StaticCache(k,            v) k v compute_kv(key,            value)compute_kv¶ gen_cache(key,            value=None,            type=<class 'paddle.nn.layer.transformer.Cache'>)gen_cache¶ forward(query,            key=None,            value=None,            attn_mask=None,            cache=None)forward¶ 
class paddle.nn.MultiLabelSoftMarginLoss(weight=None,          reduction='mean',          name=None)[source] forward(input,            label)forward¶ 
class paddle.nn.NLLLoss(weight=None,          ignore_index=- 100,          reduction='mean',          name=None)[source] forward(input,            label)forward¶ 
class paddle.nn.Pad1D(padding,          mode='constant',          value=0.0,          data_format='NCL',          name=None)[source] forward(x)forward¶ extra_repr()extra_repr¶ 
class paddle.nn.Pad2D(padding,          mode='constant',          value=0.0,          data_format='NCHW',          name=None)[source] forward(x)forward¶ extra_repr()extra_repr¶ 
class paddle.nn.Pad3D(padding,          mode='constant',          value=0.0,          data_format='NCDHW',          name=None)[source] forward(x)forward¶ extra_repr()extra_repr¶ 
class paddle.nn.PairwiseDistance(p=2.0,          epsilon=1e-06,          keepdim=False,          name=None)[source] forward(x,            y)forward¶ extra_repr()extra_repr¶ 
class paddle.nn.ParameterList(parameters=None)[source] append(parameter)append¶ 
class paddle.nn.PixelShuffle(upscale_factor,          data_format='NCHW',          name=None)[source] forward(x)forward¶ extra_repr()extra_repr¶ 
class paddle.nn.PixelUnshuffle(downscale_factor,          data_format='NCHW',          name=None)[source] forward(x)forward¶ extra_repr()extra_repr¶ 
class paddle.nn.PReLU(num_parameters=1,          init=0.25,          weight_attr=None,          data_format='NCHW',          name=None)[source] forward(x)forward¶ extra_repr()extra_repr¶ 

class paddle.nn.ReLU(name=None)[source] forward(x)forward¶ extra_repr()extra_repr¶ 
class paddle.nn.ReLU6(name=None)[source] forward(x)forward¶ extra_repr()extra_repr¶ 
class paddle.nn.RNN(cell,          is_reverse=False,          time_major=False)[source] forward(inputs,            initial_states=None,            sequence_length=None,            **kwargs)forward¶ 
class paddle.nn.RNNCellBase(name_scope=None,          dtype='float32')[source] get_initial_states(batch_ref,            shape=None,            dtype=None,            init_value=0.0,            batch_dim_idx=0)get_initial_states¶ property state_shape property state_dtype 
class paddle.nn.RReLU(lower=0.125,          upper=0.3333333333333333,          name=None)[source] forward(x)forward¶ extra_repr()extra_repr¶ 
class paddle.nn.SELU(scale=1.0507009873554805,          alpha=1.6732632423543772,          name=None)[source] forward(x)forward¶ extra_repr()extra_repr¶ 
class paddle.nn.Sequential(*layers)[source] forward(input)forward¶ 
class paddle.nn.Sigmoid(name=None)[source] forward(x)forward¶ extra_repr()extra_repr¶ 
class paddle.nn.Silu(name=None)[source] forward(x)forward¶ extra_repr()extra_repr¶ 
class paddle.nn.SimpleRNN(input_size,          hidden_size,          num_layers=1,          direction='forward',          time_major=False,          dropout=0.0,          activation='tanh',          weight_ih_attr=None,          weight_hh_attr=None,          bias_ih_attr=None,          bias_hh_attr=None,          name=None)[source] 
class paddle.nn.SimpleRNNCell(input_size,          hidden_size,          activation='tanh',          weight_ih_attr=None,          weight_hh_attr=None,          bias_ih_attr=None,          bias_hh_attr=None,          name=None)[source] forward(inputs,            states=None)forward¶ property state_shape extra_repr()extra_repr¶ 
class paddle.nn.SmoothL1Loss(reduction='mean',          delta=1.0,          name=None)[source] forward(input,            label)forward¶ 
class paddle.nn.SoftMarginLoss(reduction='mean',          name=None)[source] forward(input,            label)forward¶ 
class paddle.nn.Softmax(axis=- 1,          name=None)[source] forward(x)forward¶ extra_repr()extra_repr¶ 
class paddle.nn.Softmax2D(name=None)[source] forward(x)forward¶ extra_repr()extra_repr¶ 
class paddle.nn.Softplus(beta=1,          threshold=20,          name=None)[source] forward(x)forward¶ extra_repr()extra_repr¶ 
class paddle.nn.Softshrink(threshold=0.5,          name=None)[source] forward(x)forward¶ extra_repr()extra_repr¶ 
class paddle.nn.Softsign(name=None)[source] forward(x)forward¶ extra_repr()extra_repr¶ 
class paddle.nn.SpectralNorm(weight_shape,          dim=0,          power_iters=1,          eps=1e-12,          dtype='float32')[source] forward(weight)forward¶ 
class paddle.nn.Swish(name=None)[source] forward(x)forward¶ extra_repr()extra_repr¶ 
class paddle.nn.SyncBatchNorm(num_features,          momentum=0.9,          epsilon=1e-05,          weight_attr=None,          bias_attr=None,          data_format='NCHW',          name=None)[source] forward(x)forward¶ classmethod convert_sync_batchnorm(layer)convert_sync_batchnorm¶ 
class paddle.nn.Tanh(name=None)[source] forward(x)forward¶ extra_repr()extra_repr¶ 
class paddle.nn.Tanhshrink(name=None)[source] forward(x)forward¶ extra_repr()extra_repr¶ 
class paddle.nn.ThresholdedReLU(threshold=1.0,          name=None)[source] forward(x)forward¶ extra_repr()extra_repr¶ 
class paddle.nn.Transformer(d_model=512,          nhead=8,          num_encoder_layers=6,          num_decoder_layers=6,          dim_feedforward=2048,          dropout=0.1,          activation='relu',          attn_dropout=None,          act_dropout=None,          normalize_before=False,          weight_attr=None,          bias_attr=None,          custom_encoder=None,          custom_decoder=None)[source] forward(src,            tgt,            src_mask=None,            tgt_mask=None,            memory_mask=None)forward¶ generate_square_subsequent_mask(length)generate_square_subsequent_mask¶ 
class paddle.nn.TransformerDecoder(decoder_layer,          num_layers,          norm=None)[source] forward(tgt,            memory,            tgt_mask=None,            memory_mask=None,            cache=None)forward¶ gen_cache(memory,            do_zip=False)gen_cache¶ 
class paddle.nn.TransformerDecoderLayer(d_model,          nhead,          dim_feedforward,          dropout=0.1,          activation='relu',          attn_dropout=None,          act_dropout=None,          normalize_before=False,          weight_attr=None,          bias_attr=None)[source] forward(tgt,            memory,            tgt_mask=None,            memory_mask=None,            cache=None)forward¶ gen_cache(memory)gen_cache¶ 
class paddle.nn.TransformerEncoder(encoder_layer,          num_layers,          norm=None)[source] forward(src,            src_mask=None,            cache=None)forward¶ gen_cache(src)gen_cache¶ 
class paddle.nn.TransformerEncoderLayer(d_model,          nhead,          dim_feedforward,          dropout=0.1,          activation='relu',          attn_dropout=None,          act_dropout=None,          normalize_before=False,          weight_attr=None,          bias_attr=None)[source] forward(src,            src_mask=None,            cache=None)forward¶ gen_cache(src)gen_cache¶ 
class paddle.nn.TripletMarginLoss(margin=1.0,          p=2.0,          epsilon=1e-06,          swap=False,          reduction='mean',          name=None)[source] forward(input,            positive,            negative)forward¶ 
class paddle.nn.TripletMarginWithDistanceLoss(distance_function=None,          margin=1.0,          swap=False,          reduction: str = 'mean',          name=None)[source] forward(input,            positive,            negative)forward¶ 
class paddle.nn.Unfold(kernel_sizes,          dilations=1,          paddings=0,          strides=1,          name=None)[source] forward(input)forward¶ extra_repr()extra_repr¶ 
class paddle.nn.Upsample(size=None,          scale_factor=None,          mode='nearest',          align_corners=False,          align_mode=0,          data_format='NCHW',          name=None)[source] forward(x)forward¶ extra_repr()extra_repr¶ 
class paddle.nn.UpsamplingBilinear2D(size=None,          scale_factor=None,          data_format='NCHW',          name=None)[source] forward(x)forward¶ extra_repr()extra_repr¶ 
class paddle.nn.UpsamplingNearest2D(size=None,          scale_factor=None,          data_format='NCHW',          name=None)[source] forward(x)forward¶ extra_repr()extra_repr¶ 

class paddle.nn.ZeroPad2D(padding,          data_format='NCHW',          name=None)[source] forward(x)forward¶ extra_repr()extra_repr¶ 
paddle.onnx.export(layer,          path,          input_spec=None,          opset_version=9,          **configs)[source] 
class paddle.optimizer.Adadelta(learning_rate=0.001,          epsilon=1e-06,          rho=0.95,          parameters=None,          weight_decay=None,          grad_clip=None,          name=None)[source] append_regularization_ops(parameters_and_grads,            regularization=None)append_regularization_ops¶ clear_grad(set_to_zero=True)clear_grad¶ get_lr()get_lr¶ minimize(loss,            startup_program=None,            parameters=None,            no_grad_set=None)minimize¶ set_lr(value)set_lr¶ set_state_dict(state_dict)set_state_dict¶ state_dict()state_dict¶ step()step¶ 
class paddle.optimizer.Adagrad(learning_rate,          epsilon=1e-06,          parameters=None,          weight_decay=None,          grad_clip=None,          name=None,          initial_accumulator_value=0.0)[source] append_regularization_ops(parameters_and_grads,            regularization=None)append_regularization_ops¶ clear_grad(set_to_zero=True)clear_grad¶ get_lr()get_lr¶ minimize(loss,            startup_program=None,            parameters=None,            no_grad_set=None)minimize¶ set_lr(value)set_lr¶ set_state_dict(state_dict)set_state_dict¶ state_dict()state_dict¶ step()step¶ 
class paddle.optimizer.Adam(learning_rate=0.001,          beta1=0.9,          beta2=0.999,          epsilon=1e-08,          parameters=None,          weight_decay=None,          grad_clip=None,          lazy_mode=False,          multi_precision=False,          use_multi_tensor=False,          name=None)[source] step()step¶ append_regularization_ops(parameters_and_grads,            regularization=None)append_regularization_ops¶ clear_grad(set_to_zero=True)clear_grad¶ get_lr()get_lr¶ minimize(loss,            startup_program=None,            parameters=None,            no_grad_set=None)minimize¶ set_lr(value)set_lr¶ set_state_dict(state_dict)set_state_dict¶ state_dict()state_dict¶ 
class paddle.optimizer.Adamax(learning_rate=0.001,          beta1=0.9,          beta2=0.999,          epsilon=1e-08,          parameters=None,          weight_decay=None,          grad_clip=None,          name=None)[source] append_regularization_ops(parameters_and_grads,            regularization=None)append_regularization_ops¶ clear_grad(set_to_zero=True)clear_grad¶ get_lr()get_lr¶ minimize(loss,            startup_program=None,            parameters=None,            no_grad_set=None)minimize¶ set_lr(value)set_lr¶ set_state_dict(state_dict)set_state_dict¶ state_dict()state_dict¶ step()step¶ 
class paddle.optimizer.AdamW(learning_rate=0.001,          beta1=0.9,          beta2=0.999,          epsilon=1e-08,          parameters=None,          weight_decay=0.01,          lr_ratio=None,          apply_decay_param_fun=None,          grad_clip=None,          lazy_mode=False,          multi_precision=False,          name=None)[source] append_regularization_ops(parameters_and_grads,            regularization=None)append_regularization_ops¶ clear_grad(set_to_zero=True)clear_grad¶ get_lr()get_lr¶ minimize(loss,            startup_program=None,            parameters=None,            no_grad_set=None)minimize¶ set_lr(value)set_lr¶ set_state_dict(state_dict)set_state_dict¶ state_dict()state_dict¶ step()step¶ 
class paddle.optimizer.Lamb(learning_rate=0.001,          lamb_weight_decay=0.01,          beta1=0.9,          beta2=0.999,          epsilon=1e-06,          parameters=None,          grad_clip=None,          exclude_from_weight_decay_fn=None,          multi_precision=False,          name=None)[source] append_regularization_ops(parameters_and_grads,            regularization=None)append_regularization_ops¶ clear_grad(set_to_zero=True)clear_grad¶ get_lr()get_lr¶ minimize(loss,            startup_program=None,            parameters=None,            no_grad_set=None)minimize¶ set_lr(value)set_lr¶ set_state_dict(state_dict)set_state_dict¶ state_dict()state_dict¶ step()step¶ 

class paddle.optimizer.Momentum(learning_rate=0.001,          momentum=0.9,          parameters=None,          use_nesterov=False,          weight_decay=None,          grad_clip=None,          multi_precision=False,          rescale_grad=1.0,          use_multi_tensor=False,          name=None)[source] append_regularization_ops(parameters_and_grads,            regularization=None)append_regularization_ops¶ clear_grad(set_to_zero=True)clear_grad¶ get_lr()get_lr¶ minimize(loss,            startup_program=None,            parameters=None,            no_grad_set=None)minimize¶ set_lr(value)set_lr¶ set_state_dict(state_dict)set_state_dict¶ state_dict()state_dict¶ step()step¶ 
class paddle.optimizer.Optimizer(learning_rate,          parameters=None,          weight_decay=None,          grad_clip=None,          name=None)[source] state_dict()state_dict¶ set_state_dict(state_dict)set_state_dict¶ set_lr(value)set_lr¶ get_lr()get_lr¶ append_regularization_ops(parameters_and_grads,            regularization=None)append_regularization_ops¶ clear_grad(set_to_zero=True)clear_grad¶ minimize(loss,            startup_program=None,            parameters=None,            no_grad_set=None)minimize¶ step()step¶ 
class paddle.optimizer.RMSProp(learning_rate,          rho=0.95,          epsilon=1e-06,          momentum=0.0,          centered=False,          parameters=None,          weight_decay=None,          grad_clip=None,          name=None)[source] append_regularization_ops(parameters_and_grads,            regularization=None)append_regularization_ops¶ clear_grad(set_to_zero=True)clear_grad¶ get_lr()get_lr¶ minimize(loss,            startup_program=None,            parameters=None,            no_grad_set=None)minimize¶ set_lr(value)set_lr¶ set_state_dict(state_dict)set_state_dict¶ state_dict()state_dict¶ step()step¶ 
class paddle.optimizer.SGD(learning_rate=0.001,          parameters=None,          weight_decay=None,          grad_clip=None,          multi_precision=False,          name=None)[source] append_regularization_ops(parameters_and_grads,            regularization=None)append_regularization_ops¶ clear_grad(set_to_zero=True)clear_grad¶ get_lr()get_lr¶ minimize(loss,            startup_program=None,            parameters=None,            no_grad_set=None)minimize¶ set_lr(value)set_lr¶ set_state_dict(state_dict)set_state_dict¶ state_dict()state_dict¶ step()step¶ 
paddle.profiler.export_chrome_tracing(dir_name: str,          worker_name: Optional[str] = None)→ Callable[source] 
paddle.profiler.export_protobuf(dir_name: str,          worker_name: Optional[str] = None)→ Callable[source] 
paddle.profiler.load_profiler_result(filename: str)[source] 
paddle.profiler.make_scheduler(*,          closed: int,          ready: int,          record: int,          repeat: int = 0,          skip_first: int = 0)→ Callable[source] 
class paddle.profiler.Profiler(*,          targets: Optional[Iterable[paddle.profiler.profiler.ProfilerTarget]] = None,          scheduler: Optional[Union[Callable[[int], paddle.profiler.profiler.ProfilerState], tuple]] = None,          on_trace_ready: Optional[Callable[[...], Any]] = None,          record_shapes: Optional[bool] = False,          profile_memory=False,          timer_only: Optional[bool] = False,          emit_nvtx: Optional[bool] = False,          custom_device_types: Optional[list] = [])[source] start()start¶ stop()stop¶ step(num_samples: Optional[int] = None)step¶ step_info(unit=None)step_info¶ export(path='',            format='json')export¶ summary(sorted_by=SortedKeys.CPUTotal,            op_detail=True,            thread_sep=False,            time_unit='ms',            views=None)summary¶ 
class paddle.profiler.ProfilerState(value)[source] 
class paddle.profiler.ProfilerTarget(value)[source] 
class paddle.profiler.RecordEvent(name: str,          event_type: paddle.fluid.libpaddle.TracerEventType = TracerEventType.PythonUserDefined)[source] begin()begin¶ end()end¶ 
class paddle.profiler.SortedKeys(value)[source] 
class paddle.profiler.SummaryView(value)[source] 
class paddle.regularizer.L1Decay(coeff=0.0)[source] 
class paddle.regularizer.L2Decay(coeff=0.0)[source] 
paddle.signal.istft(x,          n_fft,          hop_length=None,          win_length=None,          window=None,          center=True,          normalized=False,          onesided=True,          length=None,          return_complex=False,          name=None)[source] 
paddle.signal.stft(x,          n_fft,          hop_length=None,          win_length=None,          window=None,          center=True,          pad_mode='reflect',          normalized=False,          onesided=True,          name=None)[source] 
paddle.sparse.abs(x,          name=None)[source] 
paddle.sparse.add(x,          y,          name=None)[source] 
paddle.sparse.addmm(input,          x,          y,          beta=1.0,          alpha=1.0,          name=None)[source] 
paddle.sparse.asin(x,          name=None)[source] 
paddle.sparse.asinh(x,          name=None)[source] 
paddle.sparse.atan(x,          name=None)[source] 
paddle.sparse.atanh(x,          name=None)[source] 
paddle.sparse.cast(x,          index_dtype=None,          value_dtype=None,          name=None)[source] 
paddle.sparse.coalesce(x,          name=None)[source] 
paddle.sparse.deg2rad(x,          name=None)[source] 
paddle.sparse.divide(x,          y,          name=None)[source] 
paddle.sparse.expm1(x,          name=None)[source] 
paddle.sparse.is_same_shape(x,          y)[source] 
paddle.sparse.log1p(x,          name=None)[source] 
paddle.sparse.masked_matmul(x,          y,          mask,          name=None)[source] 
paddle.sparse.matmul(x,          y,          name=None)[source] 
paddle.sparse.multiply(x,          y,          name=None)[source] 
paddle.sparse.mv(x,          vec,          name=None)[source] 
paddle.sparse.neg(x,          name=None)[source] 

paddle.sparse.pow(x,          factor,          name=None)[source] 
paddle.sparse.rad2deg(x,          name=None)[source] 
paddle.sparse.reshape(x,          shape,          name=None)[source] 
paddle.sparse.sin(x,          name=None)[source] 
paddle.sparse.sinh(x,          name=None)[source] 
paddle.sparse.sparse_coo_tensor(indices,          values,          shape=None,          dtype=None,          place=None,          stop_gradient=True)[source] 
paddle.sparse.sparse_csr_tensor(crows,          cols,          values,          shape,          dtype=None,          place=None,          stop_gradient=True)[source] 
paddle.sparse.sqrt(x,          name=None)[source] 
paddle.sparse.square(x,          name=None)[source] 
paddle.sparse.subtract(x,          y,          name=None)[source] 
paddle.sparse.tan(x,          name=None)[source] 
paddle.sparse.tanh(x,          name=None)[source] 
paddle.sparse.transpose(x,          perm,          name=None)[source] 
paddle.static.accuracy(input,          label,          k=1,          correct=None,          total=None)[source] 
paddle.static.append_backward(loss,          parameter_list=None,          no_grad_set=None,          callbacks=None,          checkpoints=None,          distop_context=None)[source] 
paddle.static.auc(input,          label,          curve='ROC',          num_thresholds=4095,          topk=1,          slide_steps=1,          ins_tag_weight=None)[source] 
class paddle.static.BuildStrategy class GradientScaleStrategy property name class ReduceStrategy property name property debug_graphviz_path property enable_auto_fusion property enable_sequential_execution property fuse_bn_act_ops property fuse_bn_add_act_ops property fuse_broadcast_ops property fuse_elewise_add_act_ops property fuse_gemm_epilogue property fuse_relu_depthwise_conv property gradient_scale_strategy property memory_optimize property reduce_strategy property remove_unnecessary_lock property sync_batch_norm 
class paddle.static.CompiledProgram(program_or_graph,          build_strategy=None)[source] with_data_parallel(loss_name=None,            build_strategy=None,            exec_strategy=None,            share_vars_from=None,            places=None)with_data_parallel¶ 
paddle.static.cpu_places(device_count=None)[source] 
paddle.static.create_global_var(shape,          value,          dtype,          persistable=False,          force_cpu=False,          name=None)[source] 
paddle.static.ctr_metric_bundle(input,          label,          ins_tag_weight=None)[source] 
paddle.static.cuda_places(device_ids=None)[source] 
paddle.static.data(name,          shape,          dtype=None,          lod_level=0)[source] 
paddle.static.default_main_program()[source] 
paddle.static.default_startup_program()[source] 
paddle.static.deserialize_persistables(program,          data,          executor)[source] 
paddle.static.deserialize_program(data)[source] 
paddle.static.device_guard(device=None)[source] 
class paddle.static.ExecutionStrategy property allow_op_delay property num_iteration_per_drop_scope property num_iteration_per_run property num_threads property use_thread_barrier 
class paddle.static.Executor(place=None)[source] close()close¶ run(program=None,            feed=None,            fetch_list=None,            feed_var_name='feed',            fetch_var_name='fetch',            scope=None,            return_numpy=True,            use_program_cache=False,            return_merged=True,            use_prune=False)run¶ infer_from_dataset(program=None,            dataset=None,            scope=None,            thread=0,            debug=False,            fetch_list=None,            fetch_info=None,            print_period=100,            fetch_handler=None)infer_from_dataset¶ train_from_dataset(program=None,            dataset=None,            scope=None,            thread=0,            debug=False,            fetch_list=None,            fetch_info=None,            print_period=100,            fetch_handler=None)train_from_dataset¶ 
paddle.static.exponential_decay(learning_rate,          decay_steps,          decay_rate,          staircase=False)[source] 
class paddle.static.ExponentialMovingAverage(decay=0.999,          thres_steps=None,          name=None)[source] update()update¶ apply(executor,            need_restore=True)apply¶ restore(executor)restore¶ 
paddle.static.global_scope()[source] 
paddle.static.gradients(targets,          inputs,          target_gradients=None,          no_grad_set=None)[source] 
class paddle.static.InputSpec(shape,          dtype='float32',          name=None)[source] classmethod from_tensor(tensor,            name=None)from_tensor¶ classmethod from_numpy(ndarray,            name=None)from_numpy¶ batch(batch_size)[source]batch¶ unbatch()unbatch¶ 
paddle.static.ipu_shard_guard(index=- 1,          stage=- 1)[source] 
class paddle.static.IpuCompiledProgram(program=None,          scope=None,          ipu_strategy=None)[source] compile(feed_list,            fetch_list)compile¶ 
class paddle.static.IpuStrategy[source] register_patch()register_patch¶ release_patch()release_patch¶ set_optimizer(optimizer)set_optimizer¶ parse_optimizer(optimizer)parse_optimizer¶ set_graph_config(num_ipus=1,            is_training=True,            micro_batch_size=1,            enable_manual_shard=False)set_graph_config¶ set_pipelining_config(enable_pipelining=False,            batches_per_step=1,            enable_gradient_accumulation=False,            accumulation_factor=1)set_pipelining_config¶ set_precision_config(enable_fp16=False)set_precision_config¶ add_custom_op(paddle_op,            popart_op=None,            domain='custom.ops',            version=1)add_custom_op¶ set_options(options)set_options¶ get_option(option)get_option¶ enable_pattern(pattern)enable_pattern¶ disable_pattern(pattern)disable_pattern¶ property num_ipus property is_training property enable_pipelining property enable_fp16 
paddle.static.load(program,          model_path,          executor=None,          var_list=None)[source] 
paddle.static.load_from_file(path)[source] 
paddle.static.load_inference_model(path_prefix,          executor,          **kwargs)[source] 
paddle.static.load_program_state(model_path,          var_list=None)[source] 
paddle.static.mlu_places(device_ids=None)[source] 
paddle.static.name_scope(prefix=None)[source] 

paddle.static.normalize_program(program,          feed_vars,          fetch_vars)[source] 
paddle.static.npu_places(device_ids=None)[source] 
class paddle.static.ParallelExecutor(use_cuda,          loss_name=None,          main_program=None,          share_vars_from=None,          exec_strategy=None,          build_strategy=None,          num_trainers=1,          trainer_id=0,          scope=None)[source] run(fetch_list,            feed=None,            feed_dict=None,            return_numpy=True)run¶ drop_local_exe_scopes()drop_local_exe_scopes¶ 
paddle.static.Print(input,          first_n=- 1,          message=None,          summarize=20,          print_tensor_name=True,          print_tensor_type=True,          print_tensor_shape=True,          print_tensor_layout=True,          print_tensor_lod=True,          print_phase='both')[source] 
class paddle.static.Program[source] global_seed(seed=0)global_seed¶ to_string(throw_on_error,            with_details=False)to_string¶ clone(for_test=False)clone¶ static parse_from_string(binary_str)parse_from_string¶ property num_blocks property random_seed global_block()global_block¶ block(index)block¶ current_block()current_block¶ list_vars()list_vars¶ all_parameters()all_parameters¶ state_dict(mode='all',            scope=None)state_dict¶ set_state_dict(state_dict,            scope=None)set_state_dict¶ 
paddle.static.program_guard(main_program,          startup_program=None)[source] 
paddle.static.py_func(func,          x,          out,          backward_func=None,          skip_vars_in_backward_input=None)[source] 
paddle.static.save(program,          model_path,          protocol=4,          **configs)[source] 
paddle.static.save_inference_model(path_prefix,          feed_vars,          fetch_vars,          executor,          **kwargs)[source] 
paddle.static.save_to_file(path,          content)[source] 
paddle.static.scope_guard(scope)[source] 
paddle.static.serialize_persistables(feed_vars,          fetch_vars,          executor,          **kwargs)[source] 
paddle.static.serialize_program(feed_vars,          fetch_vars,          **kwargs)[source] 
paddle.static.set_ipu_shard(call_func,          index=- 1,          stage=- 1)[source] 
paddle.static.set_program_state(program,          state_dict)[source] 

class paddle.static.Variable(block,          type=VarType.LOD_TENSOR,          name=None,          shape=None,          dtype=None,          lod_level=None,          capacity=None,          persistable=None,          error_clip=None,          stop_gradient=False,          is_data=False,          need_check_feed=False,          belong_to_optimizer=False,          **kwargs)[source] detach()detach¶ numpy()numpy¶ backward(retain_graph=False)backward¶ gradient()gradient¶ clear_gradient()clear_gradient¶ to_string(throw_on_error,            with_details=False)to_string¶ element_size()element_size¶ property stop_gradient property persistable property is_parameter property grad_name property name property shape property dtype property lod_level property type property T clone()clone¶ get_value(scope=None)get_value¶ set_value(value,            scope=None)set_value¶ size()size¶ property attr_names property dist_attr abs(name=None)abs¶ acos(name=None)acos¶ acosh(name=None)acosh¶ add(y,            name=None)add¶ add_(y,            name=None)add_¶ add_n(name=None)add_n¶ addmm(x,            y,            beta=1.0,            alpha=1.0,            name=None)addmm¶ all(axis=None,            keepdim=False,            name=None)all¶ allclose(y,            rtol=1e-05,            atol=1e-08,            equal_nan=False,            name=None)allclose¶ amax(axis=None,            keepdim=False,            name=None)amax¶ amin(axis=None,            keepdim=False,            name=None)amin¶ angle(name=None)angle¶ any(axis=None,            keepdim=False,            name=None)any¶ append(var)append¶ argmax(axis=None,            keepdim=False,            dtype='int64',            name=None)argmax¶ argmin(axis=None,            keepdim=False,            dtype='int64',            name=None)argmin¶ argsort(axis=- 1,            descending=False,            name=None)argsort¶ as_complex(name=None)as_complex¶ as_real(name=None)as_real¶ asin(name=None)asin¶ asinh(name=None)asinh¶ astype(dtype)astype¶ atan(name=None)atan¶ atanh(name=None)atanh¶ bincount(weights=None,            minlength=0,            name=None)bincount¶ bitwise_and(y,            out=None,            name=None)bitwise_and¶ bitwise_not(out=None,            name=None)bitwise_not¶ bitwise_or(y,            out=None,            name=None)bitwise_or¶ bitwise_xor(y,            out=None,            name=None)bitwise_xor¶ bmm(y,            name=None)bmm¶ broadcast_shape(y_shape)broadcast_shape¶ broadcast_tensors(name=None)broadcast_tensors¶ broadcast_to(shape,            name=None)broadcast_to¶ bucketize(sorted_sequence,            out_int32=False,            right=False,            name=None)bucketize¶ cast(dtype)cast¶ ceil(name=None)ceil¶ ceil_(name=None)ceil_¶ cholesky(upper=False,            name=None)cholesky¶ cholesky_solve(y,            upper=False,            name=None)cholesky_solve¶ chunk(chunks,            axis=0,            name=None)chunk¶ clip(min=None,            max=None,            name=None)clip¶ clip_(min=None,            max=None,            name=None)clip_¶ concat(axis=0,            name=None)concat¶ cond(p=None,            name=None)cond¶ conj(name=None)conj¶ corrcoef(rowvar=True,            name=None)corrcoef¶ cos(name=None)cos¶ cosh(name=None)cosh¶ count_nonzero(axis=None,            keepdim=False,            name=None)count_nonzero¶ cov(rowvar=True,            ddof=True,            fweights=None,            aweights=None,            name=None)cov¶ cpu()cpu¶ cross(y,            axis=9,            name=None)cross¶ cuda()cuda¶ cumprod(dim=None,            dtype=None,            name=None)cumprod¶ cumsum(axis=None,            dtype=None,            name=None)cumsum¶ deg2rad(name=None)deg2rad¶ diagonal(offset=0,            axis1=0,            axis2=1,            name=None)diagonal¶ diff(n=1,            axis=- 1,            prepend=None,            append=None,            name=None)diff¶ digamma(name=None)digamma¶ dist(y,            p=2,            name=None)dist¶ divide(y,            name=None)divide¶ dot(y,            name=None)dot¶ eig(name=None)eig¶ eigvals(name=None)eigvals¶ eigvalsh(UPLO='L',            name=None)eigvalsh¶ equal(y,            name=None)equal¶ equal_all(y,            name=None)equal_all¶ erf(name=None)erf¶ erfinv(name=None)erfinv¶ erfinv_(name=None)erfinv_¶ exp(name=None)exp¶ exp_(name=None)exp_¶ expand(shape,            name=None)expand¶ expand_as(y,            name=None)expand_as¶ exponential_(lam=1.0,            name=None)exponential_¶ flatten(start_axis=0,            stop_axis=- 1,            name=None)flatten¶ flatten_(start_axis=0,            stop_axis=- 1,            name=None)flatten_¶ flip(axis,            name=None)flip¶ floor(name=None)floor¶ floor_(name=None)floor_¶ floor_divide(y,            name=None)floor_divide¶ floor_mod(y,            name=None)floor_mod¶ fmax(y,            name=None)fmax¶ fmin(y,            name=None)fmin¶ frac(name=None)frac¶ gather(index,            axis=None,            name=None)gather¶ gather_nd(index,            name=None)gather_nd¶ gcd(y,            name=None)gcd¶ greater_equal(y,            name=None)greater_equal¶ greater_than(y,            name=None)greater_than¶ heaviside(y,            name=None)heaviside¶ histogram(bins=100,            min=0,            max=0,            name=None)histogram¶ imag(name=None)imag¶ increment(value=1.0,            name=None)increment¶ index_add(index,            axis,            value,            name=None)index_add¶ index_add_(index,            axis,            value,            name=None)index_add_¶ index_sample(index)index_sample¶ index_select(index,            axis=0,            name=None)index_select¶ inner(y,            name=None)inner¶ inverse(name=None)inverse¶ is_complex()is_complex¶ is_empty(name=None)is_empty¶ is_floating_point()is_floating_point¶ is_integer()is_integer¶ is_tensor()is_tensor¶ isclose(y,            rtol=1e-05,            atol=1e-08,            equal_nan=False,            name=None)isclose¶ isfinite(name=None)isfinite¶ isinf(name=None)isinf¶ isnan(name=None)isnan¶ item()item¶ kron(y,            name=None)kron¶ kthvalue(k,            axis=None,            keepdim=False,            name=None)kthvalue¶ lcm(y,            name=None)lcm¶ lerp(y,            weight,            name=None)lerp¶ lerp_(y,            weight,            name=None)lerp_¶ less_equal(y,            name=None)less_equal¶ less_than(y,            name=None)less_than¶ lgamma(name=None)lgamma¶ log(name=None)log¶ log10(name=None)log10¶ log1p(name=None)log1p¶ log2(name=None)log2¶ logcumsumexp(axis=None,            dtype=None,            name=None)logcumsumexp¶ logical_and(y,            out=None,            name=None)logical_and¶ logical_not(out=None,            name=None)logical_not¶ logical_or(y,            out=None,            name=None)logical_or¶ logical_xor(y,            out=None,            name=None)logical_xor¶ logit(eps=None,            name=None)logit¶ logsumexp(axis=None,            keepdim=False,            name=None)logsumexp¶ lstsq(y,            rcond=None,            driver=None,            name=None)lstsq¶ lu(pivot=True,            get_infos=False,            name=None)lu¶ lu_unpack(y,            unpack_ludata=True,            unpack_pivots=True,            name=None)lu_unpack¶ masked_select(mask,            name=None)masked_select¶ matmul(y,            transpose_x=False,            transpose_y=False,            name=None)matmul¶ matrix_power(n,            name=None)matrix_power¶ max(axis=None,            keepdim=False,            name=None)max¶ maximum(y,            name=None)maximum¶ mean(axis=None,            keepdim=False,            name=None)mean¶ median(axis=None,            keepdim=False,            name=None)median¶ min(axis=None,            keepdim=False,            name=None)min¶ minimum(y,            name=None)minimum¶ mm(mat2,            name=None)mm¶ mod(y,            name=None)mod¶ mode(axis=- 1,            keepdim=False,            name=None)mode¶ moveaxis(source,            destination,            name=None)moveaxis¶ multi_dot(name=None)multi_dot¶ multiplex(index,            name=None)multiplex¶ multiply(y,            name=None)multiply¶ mv(vec,            name=None)mv¶ nanmean(axis=None,            keepdim=False,            name=None)nanmean¶ nanmedian(axis=None,            keepdim=True,            name=None)nanmedian¶ nanquantile(q,            axis=None,            keepdim=False)nanquantile¶ nansum(axis=None,            dtype=None,            keepdim=False,            name=None)nansum¶ property ndim neg(name=None)neg¶ nonzero(as_tuple=False)nonzero¶ norm(p='fro',            axis=None,            keepdim=False,            name=None)norm¶ not_equal(y,            name=None)not_equal¶ numel(name=None)numel¶ outer(y,            name=None)outer¶ place()place¶ pop(*args)pop¶ pow(y,            name=None)pow¶ prod(axis=None,            keepdim=False,            dtype=None,            name=None)prod¶ put_along_axis(indices,            values,            axis,            reduce='assign')put_along_axis¶ put_along_axis_(indices,            values,            axis,            reduce='assign')put_along_axis_¶ qr(mode='reduced',            name=None)qr¶ quantile(q,            axis=None,            keepdim=False)quantile¶ rad2deg(name=None)rad2deg¶ rank()rank¶ real(name=None)real¶ reciprocal(name=None)reciprocal¶ reciprocal_(name=None)reciprocal_¶ remainder(y,            name=None)remainder¶ remainder_(y,            name=None)remainder_¶ repeat_interleave(repeats,            axis=None,            name=None)repeat_interleave¶ reshape(shape,            name=None)reshape¶ reshape_(shape,            name=None)reshape_¶ reverse(axis,            name=None)reverse¶ roll(shifts,            axis=None,            name=None)roll¶ rot90(k=1,            axes=[0, 1],            name=None)rot90¶ round(name=None)round¶ round_(name=None)round_¶ rsqrt(name=None)rsqrt¶ rsqrt_(name=None)rsqrt_¶ scale(scale=1.0,            bias=0.0,            bias_after_scale=True,            act=None,            name=None)scale¶ scale_(scale=1.0,            bias=0.0,            bias_after_scale=True,            act=None,            name=None)scale_¶ scatter(index,            updates,            overwrite=True,            name=None)scatter¶ scatter_(index,            updates,            overwrite=True,            name=None)scatter_¶ scatter_nd(updates,            shape,            name=None)scatter_nd¶ scatter_nd_add(index,            updates,            name=None)scatter_nd_add¶ sgn(name=None)sgn¶ shard_index(index_num,            nshards,            shard_id,            ignore_value=- 1)shard_index¶ sign(name=None)sign¶ sin(name=None)sin¶ sinh(name=None)sinh¶ slice(axes,            starts,            ends)slice¶ solve(y,            name=None)solve¶ sort(axis=- 1,            descending=False,            name=None)sort¶ split(num_or_sections,            axis=0,            name=None)split¶ sqrt(name=None)sqrt¶ sqrt_(name=None)sqrt_¶ square(name=None)square¶ squeeze(axis=None,            name=None)squeeze¶ squeeze_(axis=None,            name=None)squeeze_¶ stack(axis=0,            name=None)stack¶ stanh(scale_a=0.67,            scale_b=1.7159,            name=None)stanh¶ std(axis=None,            unbiased=True,            keepdim=False,            name=None)std¶ strided_slice(axes,            starts,            ends,            strides,            name=None)strided_slice¶ subtract(y,            name=None)subtract¶ subtract_(y,            name=None)subtract_¶ sum(axis=None,            dtype=None,            keepdim=False,            name=None)sum¶ t(name=None)t¶ take(index,            mode='raise',            name=None)take¶ take_along_axis(indices,            axis)take_along_axis¶ tanh(name=None)tanh¶ tanh_(name=None)tanh_¶ tensordot(y,            axes=2,            name=None)tensordot¶ tile(repeat_times,            name=None)tile¶ topk(k,            axis=None,            largest=True,            sorted=True,            name=None)topk¶ trace(offset=0,            axis1=0,            axis2=1,            name=None)trace¶ transpose(perm,            name=None)transpose¶ trunc(name=None)trunc¶ unbind(axis=0)unbind¶ uniform_(min=- 1.0,            max=1.0,            seed=0,            name=None)uniform_¶ unique(return_index=False,            return_inverse=False,            return_counts=False,            axis=None,            dtype='int64',            name=None)unique¶ unique_consecutive(return_inverse=False,            return_counts=False,            axis=None,            dtype='int64',            name=None)unique_consecutive¶ unsqueeze(axis,            name=None)unsqueeze¶ unsqueeze_(axis,            name=None)unsqueeze_¶ unstack(axis=0,            num=None)unstack¶ var(axis=None,            unbiased=True,            keepdim=False,            name=None)var¶ where(x=None,            y=None,            name=None)where¶ 
class paddle.static.WeightNormParamAttr(dim=None,          name=None,          initializer=None,          learning_rate=1.0,          regularizer=None,          trainable=True,          do_model_average=False,          need_clip=True)[source] 
paddle.static.xpu_places(device_ids=None)[source] 
paddle.sysconfig.get_include()[source] 
paddle.sysconfig.get_lib()[source] 

paddle.Tensor.add_(x,          y,          name=None) 
paddle.Tensor.astype(self,          dtype) 
paddle.Tensor.backward(self,          grad_tensor=None,          retain_graph=False) 
paddle.Tensor.ceil_(x,          name=None) 
paddle.Tensor.clear_grad(self) 
paddle.Tensor.clip_(x,          min=None,          max=None,          name=None) 
paddle.Tensor.clone(self) 
paddle.Tensor.cpu(self) 
paddle.Tensor.cuda(self,          device_id=None,          blocking=True) 
paddle.Tensor.dim(x) 
paddle.Tensor.erfinv_(x,          name=None) 
paddle.Tensor.exp_(x,          name=None) 
paddle.Tensor.exponential_(x,          lam=1.0,          name=None) 
paddle.Tensor.fill_(x,          value) 
paddle.Tensor.fill_diagonal_(x,          value,          offset=0,          wrap=False,          name=None) 
paddle.Tensor.fill_diagonal_tensor(x,          y,          offset=0,          dim1=0,          dim2=1,          name=None) 
paddle.Tensor.fill_diagonal_tensor_(x,          y,          offset=0,          dim1=0,          dim2=1,          name=None) 
paddle.Tensor.flatten_(x,          start_axis=0,          stop_axis=- 1,          name=None) 
paddle.Tensor.floor_(x,          name=None) 
paddle.Tensor.gradient(self) 
paddle.Tensor.item(self,          *args) 
paddle.Tensor.lerp_(x,          y,          weight,          name=None) 
paddle.Tensor.ndimension(x) 
paddle.Tensor.pin_memory(self) 
paddle.Tensor.put_along_axis_(arr,          indices,          values,          axis,          reduce='assign') 
paddle.Tensor.reciprocal_(x,          name=None) 
paddle.Tensor.register_hook(self,          hook) 
paddle.Tensor.remainder_(x,          y,          name=None) 
paddle.Tensor.round_(x,          name=None) 
paddle.Tensor.rsqrt_(x,          name=None) 
paddle.Tensor.scale_(x,          scale=1.0,          bias=0.0,          bias_after_scale=True,          act=None,          name=None) 
paddle.Tensor.set_value(self,          value) 
paddle.Tensor.sqrt_(x,          name=None) 
paddle.Tensor.subtract_(x,          y,          name=None) 
paddle.Tensor.to_dense(self) 
paddle.Tensor.to_sparse_coo(self,          sparse_dim) 
paddle.Tensor.uniform_(x,          min=- 1.0,          max=1.0,          seed=0,          name=None) 
paddle.Tensor.value(self) 
paddle.Tensor.values(self) 
paddle.Tensor.zero_(x) 
class paddle.text.Conll05st(data_file=None,          word_dict_file=None,          verb_dict_file=None,          target_dict_file=None,          emb_file=None,          download=True)[source] get_dict()get_dict¶ get_embedding()get_embedding¶ 
class paddle.text.Imdb(data_file=None,          mode='train',          cutoff=150,          download=True)[source] 
class paddle.text.Imikolov(data_file=None,          data_type='NGRAM',          window_size=- 1,          mode='train',          min_word_freq=50,          download=True)[source] 
class paddle.text.Movielens(data_file=None,          mode='train',          test_ratio=0.1,          rand_seed=0,          download=True)[source] 
class paddle.text.UCIHousing(data_file=None,          mode='train',          download=True)[source] 
paddle.text.viterbi_decode(potentials,          transition_params,          lengths,          include_bos_eos_tag=True,          name=None)[source] 
class paddle.text.ViterbiDecoder(transitions,          include_bos_eos_tag=True,          name=None)[source] forward(potentials,            lengths)forward¶ add_parameter(name,            parameter)add_parameter¶ add_sublayer(name,            sublayer)add_sublayer¶ apply(fn)apply¶ buffers(include_sublayers=True)buffers¶ children()children¶ clear_gradients()clear_gradients¶ create_parameter(shape,            attr=None,            dtype=None,            is_bias=False,            default_initializer=None)create_parameter¶ create_tensor(name=None,            persistable=None,            dtype=None)create_tensor¶ create_variable(name=None,            persistable=None,            dtype=None)create_variable¶ eval()eval¶ extra_repr()extra_repr¶ full_name()full_name¶ load_dict(state_dict,            use_structured_name=True)load_dict¶ named_buffers(prefix='',            include_sublayers=True)named_buffers¶ named_children()named_children¶ named_parameters(prefix='',            include_sublayers=True)named_parameters¶ named_sublayers(prefix='',            include_self=False,            layers_set=None)named_sublayers¶ parameters(include_sublayers=True)parameters¶ register_buffer(name,            tensor,            persistable=True)register_buffer¶ register_forward_post_hook(hook)register_forward_post_hook¶ register_forward_pre_hook(hook)register_forward_pre_hook¶ set_dict(state_dict,            use_structured_name=True)set_dict¶ set_state_dict(state_dict,            use_structured_name=True)set_state_dict¶ state_dict(destination=None,            include_sublayers=True,            structured_name_prefix='',            use_hook=True)state_dict¶ sublayers(include_self=False)sublayers¶ to(device=None,            dtype=None,            blocking=None)to¶ to_static_state_dict(destination=None,            include_sublayers=True,            structured_name_prefix='',            use_hook=True)to_static_state_dict¶ train()train¶ 
class paddle.text.WMT14(data_file=None,          mode='train',          dict_size=- 1,          download=True)[source] get_dict(reverse=False)get_dict¶ 
class paddle.text.WMT16(data_file=None,          mode='train',          src_dict_size=- 1,          trg_dict_size=- 1,          lang='en',          download=True)[source] get_dict(lang,            reverse=False)get_dict¶ 

paddle.utils.deprecated(update_to='',          since='',          reason='',          level=0)[source] 



paddle.utils.require_version(min_version,          max_version=None)[source] 
paddle.utils.run_check()[source] 
paddle.utils.try_import(module_name)[source] 

paddle.version.cuda()[source] 
paddle.version.cudnn()[source] 
paddle.version.show()[source] 

paddle.vision.get_image_backend()[source] 
paddle.vision.image_load(path,          backend=None)[source] 


paddle.vision.set_image_backend(backend)[source] 

