import paddle

x = paddle.to_tensor([-0.4, -0.2, 0.1, 0.3])
out = paddle.abs(x)
print(out)
# [0.4 0.2 0.1 0.3]
import paddle

x = paddle.to_tensor([-0.4, -0.2, 0.1, 0.3])
out = paddle.acos(x)
print(out)
# [1.98231317 1.77215425 1.47062891 1.26610367]
import paddle
x = paddle.to_tensor([2, 3, 4], 'float64')
y = paddle.to_tensor([1, 5, 2], 'float64')
z = paddle.add(x, y)
print(z)  # [3., 8., 6. ]
import paddle

input0 = paddle.to_tensor([[1, 2, 3], [4, 5, 6]], dtype='float32')
input1 = paddle.to_tensor([[7, 8, 9], [10, 11, 12]], dtype='float32')
output = paddle.add_n([input0, input1])
# [[8., 10., 12.],
#  [14., 16., 18.]]
import paddle

x = paddle.ones([2,2])
y = paddle.ones([2,2])
input = paddle.ones([2,2])

out = paddle.addmm( input=input, x=x, y=y, beta=0.5, alpha=5.0 )

print(out)
# [[10.5 10.5]
# [10.5 10.5]]
import paddle
import numpy as np

# x is a bool Tensor with following elements:
#    [[True, False]
#     [True, True]]
x = paddle.assign(np.array([[1, 0], [1, 1]], dtype='int32'))
print(x)
x = paddle.cast(x, 'bool')

# out1 should be [False]
out1 = paddle.all(x)  # [False]
print(out1)

# out2 should be [True, False]
out2 = paddle.all(x, axis=0)  # [True, False]
print(out2)

# keep_dim=False, out3 should be [False, True], out.shape should be (2,)
out3 = paddle.all(x, axis=-1)  # [False, True]
print(out3)

# keep_dim=True, out4 should be [[False], [True]], out.shape should be (2,1)
out4 = paddle.all(x, axis=1, keepdim=True)
out4 = paddle.cast(out4, 'int32')  # [[False], [True]]
print(out4)
import paddle

x = paddle.to_tensor([10000., 1e-07])
y = paddle.to_tensor([10000.1, 1e-08])
result1 = paddle.allclose(x, y, rtol=1e-05, atol=1e-08,
                        equal_nan=False, name="ignore_nan")
np_result1 = result1.numpy()
# [False]
result2 = paddle.allclose(x, y, rtol=1e-05, atol=1e-08,
                            equal_nan=True, name="equal_nan")
np_result2 = result2.numpy()
# [False]

x = paddle.to_tensor([1.0, float('nan')])
y = paddle.to_tensor([1.0, float('nan')])
result1 = paddle.allclose(x, y, rtol=1e-05, atol=1e-08,
                        equal_nan=False, name="ignore_nan")
np_result1 = result1.numpy()
# [False]
result2 = paddle.allclose(x, y, rtol=1e-05, atol=1e-08,
                            equal_nan=True, name="equal_nan")
np_result2 = result2.numpy()
# [True]
import paddle
import numpy as np

# x is a bool Tensor with following elements:
#    [[True, False]
#     [False, False]]
x = paddle.assign(np.array([[1, 0], [1, 1]], dtype='int32'))
print(x)
x = paddle.cast(x, 'bool')

# out1 should be [True]
out1 = paddle.any(x)  # [True]
print(out1)

# out2 should be [True, True]
out2 = paddle.any(x, axis=0)  # [True, True]
print(out2)

# keep_dim=False, out3 should be [True, True], out.shape should be (2,)
out3 = paddle.any(x, axis=-1)  # [True, True]
print(out3)

# keep_dim=True, result should be [[True], [True]], out.shape should be (2,1)
out4 = paddle.any(x, axis=1, keepdim=True)
out4 = paddle.cast(out4, 'int32')  # [[True], [True]]
print(out4)
import paddle

out1 = paddle.arange(5)
# [0, 1, 2, 3, 4]

out2 = paddle.arange(3, 9, 2.0)
# [3, 5, 7]

# use 4.999 instead of 5.0 to avoid floating point rounding errors
out3 = paddle.arange(4.999, dtype='float32')
# [0., 1., 2., 3., 4.]

start_var = paddle.to_tensor([3])
out4 = paddle.arange(start_var, 7)
# [3, 4, 5, 6]
import paddle

x =  paddle.to_tensor([[5,8,9,5],
                         [0,0,1,7],
                         [6,9,2,4]])
out1 = paddle.argmax(x)
print(out1) # 2
out2 = paddle.argmax(x, axis=1)
print(out2)
# [2 3 1]
out3 = paddle.argmax(x, axis=-1)
print(out3)
# [2 3 1]
import paddle

x =  paddle.to_tensor([[5,8,9,5],
                         [0,0,1,7],
                         [6,9,2,4]])
out1 = paddle.argmin(x)
print(out1) # 4
out2 = paddle.argmin(x, axis=1)
print(out2)
# [0 0 2]
out3 = paddle.argmin(x, axis=-1)
print(out3)
# [0 0 2]
import paddle

x = paddle.to_tensor([[[5,8,9,5],
                       [0,0,1,7],
                       [6,9,2,4]],
                      [[5,2,4,2],
                       [4,7,7,9],
                       [1,7,0,6]]],
                    dtype='float32')
out1 = paddle.argsort(x=x, axis=-1)
out2 = paddle.argsort(x=x, axis=0)
out3 = paddle.argsort(x=x, axis=1)
print(out1)
#[[[0 3 1 2]
#  [0 1 2 3]
#  [2 3 0 1]]
# [[1 3 2 0]
#  [0 1 2 3]
#  [2 0 3 1]]]
print(out2)
#[[[0 1 1 1]
#  [0 0 0 0]
#  [1 1 1 0]]
# [[1 0 0 0]
#  [1 1 1 1]
#  [0 0 0 1]]]
print(out3)
#[[[1 1 1 2]
#  [0 0 2 0]
#  [2 2 0 1]]
# [[2 0 2 0]
#  [1 1 0 2]
#  [0 2 1 1]]]
import paddle

x = paddle.to_tensor([-0.4, -0.2, 0.1, 0.3])
out = paddle.asin(x)
print(out)
# [-0.41151685 -0.20135792  0.10016742  0.30469265]
import paddle
import numpy as np
data = paddle.full(shape=[3, 2], fill_value=2.5, dtype='float64') # [[2.5, 2.5], [2.5, 2.5], [2.5, 2.5]]
array = np.array([[1, 1],
                  [3, 4],
                  [1, 3]]).astype(np.int64)
result1 = paddle.zeros(shape=[3, 3], dtype='float32')
paddle.assign(array, result1) # result1 = [[1, 1], [3 4], [1, 3]]
result2 = paddle.assign(data)  # result2 = [[2.5, 2.5], [2.5, 2.5], [2.5, 2.5]]
result3 = paddle.assign(np.array([[2.5, 2.5], [2.5, 2.5], [2.5, 2.5]], dtype='float32')) # result3 = [[2.5, 2.5], [2.5, 2.5], [2.5, 2.5]]
import paddle

x = paddle.to_tensor([-0.4, -0.2, 0.1, 0.3])
out = paddle.atan(x)
print(out)
# [-0.38050638 -0.19739556  0.09966865  0.29145679]
import paddle

x = paddle.to_tensor([-1, +1, +1, -1]).astype('float32')
#Tensor(shape=[4], dtype=float32, place=CUDAPlace(0), stop_gradient=True,
#       [-1,  1,  1, -1])

y = paddle.to_tensor([-1, -1, +1, +1]).astype('float32')
#Tensor(shape=[4], dtype=float32, place=CUDAPlace(0), stop_gradient=True,
#       [-1,  -1,  1, 1])

out = paddle.atan2(x, y)
#Tensor(shape=[4], dtype=float32, place=CUDAPlace(0), stop_gradient=True,
#       [-2.35619450,  2.35619450,  0.78539819, -0.78539819])
import paddle
def reader():
    for i in range(10):
        yield i
batch_reader = paddle.batch(reader, batch_size=2)

for data in batch_reader():
    print(data)

# Output is
# [0, 1]
# [2, 3]
# [4, 5]
# [6, 7]
# [8, 9]
import paddle

paddle.set_device('cpu')  # on CPU device
paddle.seed(100)

x = paddle.rand([2,3])
print(x)
# [[0.55355281, 0.20714243, 0.01162981],
#  [0.51577556, 0.36369765, 0.26091650]]

out = paddle.bernoulli(x)
print(out)
# [[1., 0., 1.],
#  [0., 1., 0.]]
import paddle

x = paddle.to_tensor([1, 2, 1, 4, 5])
result1 = paddle.bincount(x)
print(result1) # [0, 2, 1, 0, 1, 1]

w = paddle.to_tensor([2.1, 0.4, 0.1, 0.5, 0.5])
result2 = paddle.bincount(x, weights=w)
print(result2) # [0., 2.19999981, 0.40000001, 0., 0.50000000, 0.50000000]
import paddle
x = paddle.to_tensor([-5, -1, 1])
y = paddle.to_tensor([4,  2, -3])
res = paddle.bitwise_and(x, y)
print(res)  # [0, 2, 1]
import paddle
x = paddle.to_tensor([-5, -1, 1])
res = paddle.bitwise_not(x)
print(res) # [4, 0, -2]
import paddle
x = paddle.to_tensor([-5, -1, 1])
y = paddle.to_tensor([4,  2, -3])
res = paddle.bitwise_or(x, y)
print(res)  # [-1, -1, -3]
import paddle
x = paddle.to_tensor([-5, -1, 1])
y = paddle.to_tensor([4,  2, -3])
res = paddle.bitwise_xor(x, y)
print(res) # [-1, -3, -4]
import paddle

# In imperative mode:
# size x: (2, 2, 3) and y: (2, 3, 2)
x = paddle.to_tensor([[[1.0, 1.0, 1.0],
                    [2.0, 2.0, 2.0]],
                    [[3.0, 3.0, 3.0],
                    [4.0, 4.0, 4.0]]])
y = paddle.to_tensor([[[1.0, 1.0],[2.0, 2.0],[3.0, 3.0]],
                    [[4.0, 4.0],[5.0, 5.0],[6.0, 6.0]]])
out = paddle.bmm(x, y)
#output size: (2, 2, 2)
#output value:
#[[[6.0, 6.0],[12.0, 12.0]],[[45.0, 45.0],[60.0, 60.0]]]
out_np = out.numpy()
import paddle

shape = paddle.broadcast_shape([2, 1, 3], [1, 3, 1])
# [2, 3, 3]

# shape = paddle.broadcast_shape([2, 1, 3], [3, 3, 1])
# ValueError (terminated with error message).
import paddle
x1 = paddle.rand([1, 2, 3, 4]).astype('float32')
x2 = paddle.rand([1, 2, 1, 4]).astype('float32')
x3 = paddle.rand([1, 1, 3, 1]).astype('float32')
out1, out2, out3 = paddle.broadcast_tensors(input=[x1, x2, x3])
# out1, out2, out3: tensors broadcasted from x1, x2, x3 with shape [1,2,3,4]
import paddle

data = paddle.to_tensor([1, 2, 3], dtype='int32')
out = paddle.broadcast_to(data, shape=[2, 3])
print(out)
# [[1, 2, 3], [1, 2, 3]]
import paddle

x = paddle.to_tensor([2, 3, 4], 'float64')
y = paddle.cast(x, 'uint8')
import paddle

x = paddle.to_tensor([-0.4, -0.2, 0.1, 0.3])
out = paddle.ceil(x)
print(out)
# [-0. -0.  1.  1.]
import paddle
import numpy as np

a = np.random.rand(3, 3)
a_t = np.transpose(a, [1, 0])
x_data = np.matmul(a, a_t) + 1e-03
x = paddle.to_tensor(x_data)
out = paddle.cholesky(x, upper=False)
print(out)
# [[1.190523   0.         0.        ]
#  [0.9906703  0.27676893 0.        ]
#  [1.25450498 0.05600871 0.06400121]]
import numpy as np
import paddle

# x is a Tensor which shape is [3, 9, 5]
x_np = np.random.random([3, 9, 5]).astype("int32")
x = paddle.to_tensor(x_np)

out0, out1, out2 = paddle.chunk(x, chunks=3, axis=1)
# out0.shape [3, 3, 5]
# out1.shape [3, 3, 5]
# out2.shape [3, 3, 5]


# axis is negative, the real axis is (rank(x) + axis) which real
# value is 1.
out0, out1, out2 = paddle.chunk(x, chunks=3, axis=-2)
# out0.shape [3, 3, 5]
# out1.shape [3, 3, 5]
# out2.shape [3, 3, 5]
import paddle

x1 = paddle.to_tensor([[1.2, 3.5], [4.5, 6.4]], 'float32')
out1 = paddle.clip(x1, min=3.5, max=5.0)
out2 = paddle.clip(x1, min=2.5)
print(out1)
# [[3.5, 3.5]
# [4.5, 5.0]]
print(out2)
# [[2.5, 3.5]
# [[4.5, 6.4]
import paddle

x1 = paddle.to_tensor([[1, 2, 3],
                       [4, 5, 6]])
x2 = paddle.to_tensor([[11, 12, 13],
                       [14, 15, 16]])
x3 = paddle.to_tensor([[21, 22],
                       [23, 24]])
zero = paddle.full(shape=[1], dtype='int32', fill_value=0)
# When the axis is negative, the real axis is (axis + Rank(x))
# As follow, axis is -1, Rank(x) is 2, the real axis is 1
out1 = paddle.concat(x=[x1, x2, x3], axis=-1)
out2 = paddle.concat(x=[x1, x2], axis=0)
out3 = paddle.concat(x=[x1, x2], axis=zero)
# out1
# [[ 1  2  3 11 12 13 21 22]
#  [ 4  5  6 14 15 16 23 24]]
# out2 out3
# [[ 1  2  3]
#  [ 4  5  6]
#  [11 12 13]
#  [14 15 16]]
import paddle
data=paddle.to_tensor([[1+1j, 2+2j, 3+3j], [4+4j, 5+5j, 6+6j]])
#Tensor(shape=[2, 3], dtype=complex64, place=CUDAPlace(0), stop_gradient=True,
#       [[(1+1j), (2+2j), (3+3j)],
#        [(4+4j), (5+5j), (6+6j)]])

conj_data=paddle.conj(data)
#Tensor(shape=[2, 3], dtype=complex64, place=CUDAPlace(0), stop_gradient=True,
#       [[(1-1j), (2-2j), (3-3j)],
#        [(4-4j), (5-5j), (6-6j)]])
import paddle

x = paddle.to_tensor([-0.4, -0.2, 0.1, 0.3])
out = paddle.cos(x)
print(out)
# [0.92106099 0.98006658 0.99500417 0.95533649]
import paddle

x = paddle.to_tensor([-0.4, -0.2, 0.1, 0.3])
out = paddle.cosh(x)
print(out)
# [1.08107237 1.02006676 1.00500417 1.04533851]
import paddle
cpu_place = paddle.CPUPlace()
import paddle
paddle.enable_static()
W = paddle.static.create_parameter(shape=[784, 200], dtype='float32')
import paddle
  x = paddle.to_tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9]])
  # x.shape = [3, 3]
  # x = [[1, 2, 3], [4, 5, 6], [7, 8, 9]]

  # shape can be a 1-D Tensor or list or tuple.
  shape = paddle.to_tensor([2, 2], dtype='int32')
  # shape = [2, 2]
  # shape = (2, 2)
  out = paddle.crop(x, shape)
  # out.shape = [2, 2]
  # out = [[1,2], [4,5]]

  # offsets can be a 1-D Tensor or list or tuple.
  offsets = paddle.to_tensor([0, 1], dtype='int32')
  # offsets = [1, 0]
  # offsets = (1, 1)
  out = paddle.crop(x, shape, offsets)
  # out.shape = [2, 2]
  # if offsets = [0, 0], out = [[1,2], [4,5]]
  # if offsets = [0, 1], out = [[2,3], [5,6]]
  # if offsets = [1, 0], out = [[4,5], [7,8]]
  # if offsets = [1, 1], out = [[5,6], [8,9]]
import paddle

x = paddle.to_tensor([[1.0, 1.0, 1.0],
                      [2.0, 2.0, 2.0],
                      [3.0, 3.0, 3.0]])
y = paddle.to_tensor([[1.0, 1.0, 1.0],
                      [1.0, 1.0, 1.0],
                      [1.0, 1.0, 1.0]])

z1 = paddle.cross(x, y)
# [[-1. -1. -1.]
#  [ 2.  2.  2.]
#  [-1. -1. -1.]]

z2 = paddle.cross(x, y, axis=1)
# [[0. 0. 0.]
#  [0. 0. 0.]
#  [0. 0. 0.]]
import paddle
place = paddle.CUDAPinnedPlace()
import paddle

place = paddle.CUDAPlace(0)
import paddle

data = paddle.arange(12)
data = paddle.reshape(data, (3, 4))
# [[ 0  1  2  3 ]
#  [ 4  5  6  7 ]
#  [ 8  9  10 11]]

y = paddle.cumprod(data, dim=0)
# [[ 0  1   2   3]
#  [ 0  5  12  21]
#  [ 0 45 120 231]]

y = paddle.cumprod(data, dim=-1)
# [[ 0   0   0    0]
#  [ 4  20 120  840]
#  [ 8  72 720 7920]]

y = paddle.cumprod(data, dim=1, dtype='float64')
# [[ 0.   0.   0.    0.]
#  [ 4.  20. 120.  840.]
#  [ 8.  72. 720. 7920.]]

print(y.dtype)
# paddle.float64
import paddle

data = paddle.arange(12)
data = paddle.reshape(data, (3, 4))

y = paddle.cumsum(data)
# [ 0  1  3  6 10 15 21 28 36 45 55 66]

y = paddle.cumsum(data, axis=0)
# [[ 0  1  2  3]
#  [ 4  6  8 10]
#  [12 15 18 21]]

y = paddle.cumsum(data, axis=-1)
# [[ 0  1  3  6]
#  [ 4  9 15 22]
#  [ 8 17 27 38]]

y = paddle.cumsum(data, dtype='float64')
print(y.dtype)
# VarType.FP64
# required: distributed
import paddle
import paddle.nn as nn
import paddle.optimizer as opt
import paddle.distributed as dist

class LinearNet(nn.Layer):
    def __init__(self):
        super(LinearNet, self).__init__()
        self._linear1 = nn.Linear(10, 10)
        self._linear2 = nn.Linear(10, 1)

    def forward(self, x):
        return self._linear2(self._linear1(x))

def train():
    # 1. initialize parallel environment
    dist.init_parallel_env()

    # 2. create data parallel layer & optimizer
    layer = LinearNet()
    dp_layer = paddle.DataParallel(layer)

    loss_fn = nn.MSELoss()
    adam = opt.Adam(
        learning_rate=0.001, parameters=dp_layer.parameters())

    # 3. run layer
    inputs = paddle.randn([10, 10], 'float32')
    outputs = dp_layer(inputs)
    labels = paddle.randn([10, 1], 'float32')
    loss = loss_fn(outputs, labels)

    loss.backward()

    adam.step()
    adam.clear_grad()

if __name__ == '__main__':
    # 1. start by ``paddle.distributed.spawn`` (default)
    dist.spawn(train, nprocs=2)
    # 2. start by ``paddle.distributed.launch``
    # train()
# required: distributed
import numpy
import paddle
import paddle.distributed as dist
from paddle.autograd import PyLayer
from paddle.distributed.fleet.utils.hybrid_parallel_util import fused_allreduce_gradients

class cus_tanh(PyLayer):
    @staticmethod
    def forward(ctx, x):
        y = paddle.tanh(x)
        ctx.save_for_backward(y)
        return y

    @staticmethod
    def backward(ctx, dy):
        y, = ctx.saved_tensor()
        grad = dy * (1 - paddle.square(y))
        return grad

class SimpleNet(paddle.nn.Layer):
    def __init__(self):
        super(SimpleNet, self).__init__()
        self.linear = paddle.nn.Linear(2, 2)

    def forward(self, inputs):
        inputs = cus_tanh.apply(inputs)
        return self.linear(inputs)

if __name__ == '__main__':
    dist.init_parallel_env()

    model = SimpleNet()
    model = paddle.DataParallel(model)
    opt = paddle.optimizer.SGD(learning_rate=0.01, parameters=model.parameters())

    for step in range(10):
        x_data = numpy.random.randn(2, 2).astype(numpy.float32)
        x = paddle.to_tensor(x_data)
        x.stop_gradient = False

        # step 1 : skip gradient synchronization by 'no_sync'
        with model.no_sync():
            y_pred = model(x)
            loss = y_pred.mean()
            loss.backward()

        # step 2 : fuse + allreduce manually before optimization
        fused_allreduce_gradients(list(model.parameters()), None)

        opt.step()
        opt.clear_grad()
# required: distributed
import paddle
import paddle.nn as nn
import paddle.distributed as dist

class SimpleNet(nn.Layer):
    def __init__(self):
        super(SimpleNet, self).__init__()
        self._linear = nn.Linear(10, 1)

    def forward(self, x):
        return self._linear(x)

dist.init_parallel_env()
model = SimpleNet()
dp_model = paddle.DataParallel(model)

inputs_1 = paddle.randn([10, 10], 'float32')
inputs_2 = paddle.ones([10, 10], 'float32')

with dp_model.no_sync():
    # gradients will not be synchronized
    dp_model(inputs_1).backward()

# synchronization happens here
dp_model(inputs_2).backward()
import paddle
import paddle.distributed as dist

dist.init_parallel_env()

emb = fluid.dygraph.Embedding([10, 10])
emb = fluid.dygraph.DataParallel(emb)

state_dict = emb.state_dict()
paddle.save(state_dict, "paddle_dy.pdparams")
import paddle

class MyLayer(paddle.nn.Layer):
    def __init__(self):
        super(MyLayer, self).__init__()
        self._linear = paddle.nn.Linear(1, 1)
        w_tmp = self.create_parameter([1,1])
        self.add_parameter("w_tmp", w_tmp)

    def forward(self, input):
        return self._linear(input)

mylayer = MyLayer()
for name, param in mylayer.named_parameters():
    print(name, param)      # will print w_tmp,_linear.weight,_linear.bias
import paddle

class MySequential(paddle.nn.Layer):
    def __init__(self, *layers):
        super(MySequential, self).__init__()
        if len(layers) > 0 and isinstance(layers[0], tuple):
            for name, layer in layers:
                self.add_sublayer(name, layer)
        else:
            for idx, layer in enumerate(layers):
                self.add_sublayer(str(idx), layer)

    def forward(self, input):
        for layer in self._sub_layers.values():
            input = layer(input)
        return input

fc1 = paddle.nn.Linear(10, 3)
fc2 = paddle.nn.Linear(3, 10, bias_attr=False)
model = MySequential(fc1, fc2)
for prefix, layer in model.named_sublayers():
    print(prefix, layer)
import paddle
import paddle.nn as nn

net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2))

def init_weights(layer):
    if type(layer) == nn.Linear:
        print('before init weight:', layer.weight.numpy())
        new_weight = paddle.full(shape=layer.weight.shape, dtype=layer.weight.dtype, fill_value=0.9)
        layer.weight.set_value(new_weight)
        print('after init weight:', layer.weight.numpy())

net.apply(init_weights)

print(net.state_dict())
import numpy as np
import paddle

linear = paddle.nn.Linear(10, 3)
value = np.array([0]).astype("float32")
buffer = paddle.to_tensor(value)
linear.register_buffer("buf_name", buffer, persistable=True)

print(linear.buffers())     # == print([linear.buf_name])
import paddle

linear1 = paddle.nn.Linear(10, 3)
linear2 = paddle.nn.Linear(3, 10, bias_attr=False)
model = paddle.nn.Sequential(linear1, linear2)

layer_list = list(model.children())

print(layer_list)   # [<paddle.nn.layer.common.Linear object at 0x7f7b8113f830>, <paddle.nn.layer.common.Linear object at 0x7f7b8113f950>]
import paddle
import numpy as np

value = np.arange(26).reshape(2, 13).astype("float32")
a = paddle.to_tensor(value)
linear = paddle.nn.Linear(13, 5)
adam = paddle.optimizer.Adam(learning_rate=0.01,
                            parameters=linear.parameters())
out = linear(a)
out.backward()
adam.step()
linear.clear_gradients()
import paddle

class MyLayer(paddle.nn.Layer):
    def __init__(self):
        super(MyLayer, self).__init__()
        self._linear = paddle.nn.Linear(1, 1)
        w_tmp = self.create_parameter([1,1])
        self.add_parameter("w_tmp", w_tmp)

    def forward(self, input):
        return self._linear(input)

mylayer = MyLayer()
for name, param in mylayer.named_parameters():
    print(name, param)      # will print w_tmp,_linear.weight,_linear.bias
import paddle

class MyLinear(paddle.nn.Layer):
    def __init__(self,
                in_features,
                out_features):
        super(MyLinear, self).__init__()
        self.linear = paddle.nn.Linear( 10, 10)

        self.back_var = self.create_tensor(name = "linear_tmp_0", dtype=self._dtype)

    def forward(self, input):
        out = self.linear(input)
        paddle.assign( out, self.back_var)

        return out
import paddle

class MyLinear(paddle.nn.Layer):
    def __init__(self,
                in_features,
                out_features):
        super(MyLinear, self).__init__()
        self.linear = paddle.nn.Linear( 10, 10)

        self.back_var = self.create_variable(name = "linear_tmp_0", dtype=self._dtype)

    def forward(self, input):
        out = self.linear(input)
        paddle.assign( out, self.back_var)

        return out
import paddle

class MyLayer(paddle.nn.Layer):
    def __init__(self):
        super(MyLayer, self).__init__()
        self._linear = paddle.nn.Linear(1, 1)
        self._dropout = paddle.nn.Dropout(p=0.5)

    def forward(self, input):
        temp = self._linear(input)
        temp = self._dropout(temp)
        return temp

x = paddle.randn([10, 1], 'float32')
mylayer = MyLayer()
mylayer.eval()  # set mylayer._dropout to eval mode
out = mylayer(x)
print(out)
import paddle

class LinearNet(paddle.nn.Layer):
    def __init__(self):
        super(LinearNet, self).__init__(name_scope = "demo_linear_net")
        self._linear = paddle.nn.Linear(1, 1)

    def forward(self, x):
        return self._linear(x)

linear_net = LinearNet()
print(linear_net.full_name())   # demo_linear_net_0
import numpy as np
import paddle

fc1 = paddle.nn.Linear(10, 3)
buffer1 = paddle.to_tensor(np.array([0]).astype("float32"))
# register a tensor as buffer by specific `persistable`
fc1.register_buffer("buf_name_1", buffer1, persistable=True)

fc2 = paddle.nn.Linear(3, 10)
buffer2 = paddle.to_tensor(np.array([1]).astype("float32"))
# register a buffer by assigning an attribute with Tensor.
# The `persistable` can only be False by this way.
fc2.buf_name_2 = buffer2

model = paddle.nn.Sequential(fc1, fc2)

# get all named buffers
for name, buffer in model.named_buffers():
    print(name, buffer)
import paddle

linear1 = paddle.nn.Linear(10, 3)
linear2 = paddle.nn.Linear(3, 10, bias_attr=False)
model = paddle.nn.Sequential(linear1, linear2)
for prefix, layer in model.named_children():
    print(prefix, layer)
    # ('0', <paddle.nn.layer.common.Linear object at 0x7fb61ed85830>)
    # ('1', <paddle.nn.layer.common.Linear object at 0x7fb61ed85950>)
import paddle

fc1 = paddle.nn.Linear(10, 3)
fc2 = paddle.nn.Linear(3, 10, bias_attr=False)
model = paddle.nn.Sequential(fc1, fc2)
for name, param in model.named_parameters():
    print(name, param)
import paddle

fc1 = paddle.nn.Linear(10, 3)
fc2 = paddle.nn.Linear(3, 10, bias_attr=False)
model = paddle.nn.Sequential(fc1, fc2)
for prefix, layer in model.named_sublayers():
    print(prefix, layer)

import numpy as np
import paddle

linear = paddle.nn.Linear(10, 3)
value = np.array([0]).astype("float32")
buffer = paddle.to_tensor(value)
linear.register_buffer("buf_name", buffer, persistable=True)

# get the buffer by attribute.
print(linear.buf_name)
import paddle
import numpy as np

# the forward_post_hook change the output of the layer: output = output * 2
def forward_post_hook(layer, input, output):
    # user can use layer, input and output for information statistis tasks

    # change the output
    return output * 2

linear = paddle.nn.Linear(13, 5)

# register the hook
forward_post_hook_handle = linear.register_forward_post_hook(forward_post_hook)

value1 = np.arange(26).reshape(2, 13).astype("float32")
in1 = paddle.to_tensor(value1)

out0 = linear(in1)

# remove the hook
forward_post_hook_handle.remove()

out1 = linear(in1)

# hook change the linear's output to output * 2, so out0 is equal to out1 * 2.
assert (out0.numpy() == (out1.numpy()) * 2).any()
import paddle
import numpy as np

# the forward_post_hook change the input of the layer: input = input * 2
def forward_pre_hook(layer, input):
    # user can use layer and input for information statistis tasks

    # change the input
    input_return = (input[0] * 2)
    return input_return

linear = paddle.nn.Linear(13, 5)

# register the hook
forward_pre_hook_handle = linear.register_forward_pre_hook(forward_pre_hook)

value0 = np.arange(26).reshape(2, 13).astype("float32")
in0 = paddle.to_tensor(value0)
out0 = linear(in0)

# remove the hook
forward_pre_hook_handle.remove()

value1 = value0 * 2
in1 = paddle.to_tensor(value1)
out1 = linear(in1)

# hook change the linear's input to input * 2, so out0 is equal to out1.
assert (out0.numpy() == out1.numpy()).any()
import paddle
import paddle.distributed as dist

dist.init_parallel_env()

emb = paddle.nn.Embedding(10, 10)
emb = fluid.dygraph.DataParallel(emb)

state_dict = emb.state_dict()
paddle.save(state_dict, "paddle_dy.pdparams")

para_state_dict = paddle.load("paddle_dy.pdparams")
emb.set_state_dict(para_state_dict)
import paddle

class MyLayer(paddle.nn.Layer):
    def __init__(self):
        super(MyLayer, self).__init__()
        self._linear = paddle.nn.Linear(1, 1)
        self._dropout = paddle.nn.Dropout(p=0.5)

    def forward(self, input):
        temp = self._linear(input)
        temp = self._dropout(temp)
        return temp

mylayer = MyLayer()
print(mylayer.sublayers())  # [<paddle.nn.layer.common.Linear object at 0x7f44b58977d0>, <paddle.nn.layer.common.Dropout object at 0x7f44b58978f0>]
# required: skip
import paddle

linear=paddle.nn.Linear(2, 2)
linear.weight
#Parameter containing:
#Tensor(shape=[2, 2], dtype=float32, place=CUDAPlace(0), stop_gradient=False,
#       [[-0.32770029,  0.38653070],
#        [ 0.46030545,  0.08158520]])

linear.to(dtype='float64')
linear.weight
#Tenor(shape=[2, 2], dtype=float64, place=CUDAPlace(0), stop_gradient=False,
#       [[-0.32770029,  0.38653070],
#        [ 0.46030545,  0.08158520]])

linear.to(device='cpu')
linear.weight
#Tensor(shape=[2, 2], dtype=float64, place=CPUPlace, stop_gradient=False,
#       [[-0.32770029,  0.38653070],
#        [ 0.46030545,  0.08158520]])
linear.to(device=paddle.CUDAPinnedPlace(), blocking=False)
linear.weight
#Tensor(shape=[2, 2], dtype=float64, place=CUDAPinnedPlace, stop_gradient=False,
#       [[-0.04989364, -0.56889004],
#        [ 0.33960250,  0.96878713]])
import paddle

emb = paddle.nn.Embedding(10, 10)

state_dict = emb.to_static_state_dict()
paddle.save( state_dict, "paddle_dy.pdparams")
import paddle

class MyLayer(paddle.nn.Layer):
    def __init__(self):
        super(MyLayer, self).__init__()
        self._linear = paddle.nn.Linear(1, 1)
        self._dropout = paddle.nn.Dropout(p=0.5)

    def forward(self, input):
        temp = self._linear(input)
        temp = self._dropout(temp)
        return temp

x = paddle.randn([10, 1], 'float32')
mylayer = MyLayer()
mylayer.eval()  # set mylayer._dropout to eval mode
out = mylayer(x)
mylayer.train()  # set mylayer._dropout to train mode
out = mylayer(x)
import paddle
import paddle.distributed as dist

dist.init_parallel_env()

emb = paddle.nn.Embedding(10, 10)
emb = fluid.dygraph.DataParallel(emb)

state_dict = emb.state_dict()
paddle.save(state_dict, "paddle_dy.pdparams")

para_state_dict = paddle.load("paddle_dy.pdparams")
emb.set_state_dict(para_state_dict)
import paddle
import paddle.distributed as dist

dist.init_parallel_env()

emb = paddle.nn.Embedding(10, 10)
emb = fluid.dygraph.DataParallel(emb)

state_dict = emb.state_dict()
paddle.save(state_dict, "paddle_dy.pdparams")

para_state_dict = paddle.load("paddle_dy.pdparams")
emb.set_state_dict(para_state_dict)
import paddle

paddle.disable_static()
x = paddle.to_tensor([1, 2, 3])
y = paddle.diag(x)
print(y.numpy())
# [[1 0 0]
#  [0 2 0]
#  [0 0 3]]

y = paddle.diag(x, offset=1)
print(y.numpy())
# [[0 1 0 0]
#  [0 0 2 0]
#  [0 0 0 3]
#  [0 0 0 0]]

y = paddle.diag(x, padding_value=6)
print(y.numpy())
# [[1 6 6]
#  [6 2 6]
#  [6 6 3]]
import paddle

paddle.disable_static()
x = paddle.to_tensor([[1, 2, 3], [4, 5, 6]])
y = paddle.diag(x)
print(y.numpy())
# [1 5]

y = paddle.diag(x, offset=1)
print(y.numpy())
# [2 6]

y = paddle.diag(x, offset=-1)
print(y.numpy())
# [4]
import paddle

x = paddle.to_tensor([1, 2, 3])
y = paddle.diagflat(x)
print(y.numpy())
# [[1 0 0]
#  [0 2 0]
#  [0 0 3]]

y = paddle.diagflat(x, offset=1)
print(y.numpy())
# [[0 1 0 0]
#  [0 0 2 0]
#  [0 0 0 3]
#  [0 0 0 0]]

y = paddle.diagflat(x, offset=-1)
print(y.numpy())
# [[0 0 0 0]
#  [1 0 0 0]
#  [0 2 0 0]
#  [0 0 3 0]]
import paddle

x = paddle.to_tensor([[1, 2], [3, 4]])
y = paddle.diagflat(x)
print(y.numpy())
# [[1 0 0 0]
#  [0 2 0 0]
#  [0 0 3 0]
#  [0 0 0 4]]

y = paddle.diagflat(x, offset=1)
print(y.numpy())
# [[0 1 0 0 0]
#  [0 0 2 0 0]
#  [0 0 0 3 0]
#  [0 0 0 0 4]
#  [0 0 0 0 0]]

y = paddle.diagflat(x, offset=-1)
print(y.numpy())
# [[0 0 0 0 0]
#  [1 0 0 0 0]
#  [0 2 0 0 0]
#  [0 0 3 0 0]
#  [0 0 0 4 0]]
import paddle

x = paddle.rand([2,2,3],'float32')
print(x)
# Tensor(shape=[2, 2, 3], dtype=float32, place=CUDAPlace(0), stop_gradient=True,
#        [[[0.45661032, 0.03751532, 0.90191704],
#          [0.43760979, 0.86177313, 0.65221709]],

#         [[0.17020577, 0.00259554, 0.28954273],
#          [0.51795638, 0.27325270, 0.18117726]]])

out1 = paddle.diagonal(x)
print(out1)
#Tensor(shape=[3, 2], dtype=float32, place=CUDAPlace(0), stop_gradient=True,
#       [[0.45661032, 0.51795638],
#        [0.03751532, 0.27325270],
#        [0.90191704, 0.18117726]])

out2 = paddle.diagonal(x, offset=0, axis1=2, axis2=1)
print(out2)
#Tensor(shape=[2, 2], dtype=float32, place=CUDAPlace(0), stop_gradient=True,
#       [[0.45661032, 0.86177313],
#        [0.17020577, 0.27325270]])

out3 = paddle.diagonal(x, offset=1, axis1=0, axis2=1)
print(out3)
#Tensor(shape=[3, 1], dtype=float32, place=CUDAPlace(0), stop_gradient=True,
#       [[0.43760979],
#        [0.86177313],
#        [0.65221709]])

out4 = paddle.diagonal(x, offset=0, axis1=1, axis2=2)
print(out4)
#Tensor(shape=[2, 2], dtype=float32, place=CUDAPlace(0), stop_gradient=True,
#       [[0.45661032, 0.86177313],
#        [0.17020577, 0.27325270]])
import paddle

data = paddle.to_tensor([[1, 1.5], [0, -2.2]], dtype='float32')
res = paddle.digamma(data)
print(res)
# Tensor(shape=[2, 2], dtype=float32, place=CUDAPlace(0), stop_gradient=True,
#       [[-0.57721591,  0.03648996],
#        [ nan       ,  5.32286835]])
import paddle
paddle.disable_signal_handler()
import paddle
print(paddle.in_dynamic_mode())  # True, dynamic mode is turn ON by default since paddle 2.0.0

paddle.enable_static()
print(paddle.in_dynamic_mode())  # False, Now we are in static mode

paddle.disable_static()
print(paddle.in_dynamic_mode())  # True, Now we are in dynamic mode
import paddle
import numpy as np

x = paddle.to_tensor(np.array([[3, 3],[3, 3]]), "float32")
y = paddle.to_tensor(np.array([[3, 3],[3, 1]]), "float32")
out = paddle.dist(x, y, 0)
print(out) # out = [1.]

out = paddle.dist(x, y, 2)
print(out) # out = [2.]

out = paddle.dist(x, y, float("inf"))
print(out) # out = [2.]

out = paddle.dist(x, y, float("-inf"))
print(out) # out = [0.]
import paddle

x = paddle.to_tensor([2, 3, 4], dtype='float64')
y = paddle.to_tensor([1, 5, 2], dtype='float64')
z = paddle.divide(x, y)
print(z)  # [2., 0.6, 2.]
import paddle
import numpy as np

x_data = np.random.uniform(0.1, 1, [10]).astype(np.float32)
y_data = np.random.uniform(1, 3, [10]).astype(np.float32)
x = paddle.to_tensor(x_data)
y = paddle.to_tensor(y_data)
z = paddle.dot(x, y)
print(z)

import paddle
import numpy as np

paddle.set_device("cpu")  # and use cpu device

# example 1: argument ``shape`` is a list which doesn't contain Tensor.
data1 = paddle.empty(shape=[2,3], dtype='float32')
#[[4.3612203e+27 1.8176809e+31 1.3555911e-19]     # uninitialized
# [1.1699684e-19 1.3563156e-19 3.6408321e-11]]    # uninitialized

# example 2: argument ``shape`` is a Tensor, the data type must be int64 or int32.
shape_data = np.array([2, 3]).astype('int32')
shape = paddle.to_tensor(shape_data)
data2 = paddle.empty(shape=shape, dtype='float32')
#[[1.7192326e-37 4.8125365e-38 1.9866003e-36]     # uninitialized
# [1.3284029e-40 7.1117408e-37 2.5353012e+30]]    # uninitialized

# example 3: argument ``shape`` is a list which contains Tensor.
dim2_data = np.array([3]).astype('int32')
dim2 = paddle.to_tensor(dim2_data)
data3 = paddle.empty(shape=[2, dim2], dtype='float32')
#[[1.1024214e+24 7.0379409e+22 6.5737699e-34]     # uninitialized
# [7.5563101e+31 7.7130405e+31 2.8020654e+20]]    # uninitialized
import paddle
import numpy as np

paddle.set_device("cpu")  # and use cpu device

x = paddle.randn([2, 3], 'float32')
output = paddle.empty_like(x)
#[[1.8491974e+20 1.8037303e+28 1.7443726e+28]     # uninitialized
# [4.9640171e+28 3.0186127e+32 5.6715899e-11]]    # uninitialized
import paddle
print(paddle.in_dynamic_mode())  # True, dynamic mode is turn ON by default since paddle 2.0.0

paddle.enable_static()
print(paddle.in_dynamic_mode())  # False, Now we are in static mode

paddle.disable_static()
print(paddle.in_dynamic_mode())  # True, Now we are in dynamic mode
import paddle

x = paddle.to_tensor([1, 2, 3])
y = paddle.to_tensor([1, 3, 2])
result1 = paddle.equal(x, y)
print(result1)  # result1 = [True False False]
import paddle

x = paddle.to_tensor([1, 2, 3])
y = paddle.to_tensor([1, 2, 3])
z = paddle.to_tensor([1, 4, 3])
result1 = paddle.equal_all(x, y)
print(result1) # result1 = [True ]
result2 = paddle.equal_all(x, z)
print(result2) # result2 = [False ]
import paddle
x = paddle.to_tensor([-0.4, -0.2, 0.1, 0.3])
out = paddle.erf(x)
print(out)
# [-0.42839236 -0.22270259  0.11246292  0.32862676]
import paddle

x = paddle.to_tensor([-0.4, -0.2, 0.1, 0.3])
out = paddle.exp(x)
print(out)
# [0.67032005 0.81873075 1.10517092 1.34985881]
import paddle

data = paddle.to_tensor([1, 2, 3], dtype='int32')
out = paddle.expand(data, shape=[2, 3])
print(out)
# [[1, 2, 3], [1, 2, 3]]
import paddle

data_x = paddle.to_tensor([1, 2, 3], 'int32')
data_y = paddle.to_tensor([[1, 2, 3], [4, 5, 6]], 'int32')
out = paddle.expand_as(data_x, data_y)
np_out = out.numpy()
# [[1, 2, 3], [1, 2, 3]]
import paddle

x = paddle.to_tensor([-0.4, -0.2, 0.1, 0.3])
out = paddle.expm1(x)
print(out)
# [-0.32967997, -0.18126924,  0.10517092,  0.34985882]
import paddle

data = paddle.eye(3, dtype='int32')
# [[1 0 0]
#  [0 1 0]
#  [0 0 1]]
data = paddle.eye(2, 3, dtype='int32')
# [[1 0 0]
#  [0 1 0]]
import paddle

image_shape=(2, 3, 4, 4)

x = paddle.arange(end=image_shape[0] * image_shape[1] * image_shape[2] * image_shape[3])
img = paddle.reshape(x, image_shape)

out = paddle.flatten(img, start_axis=1, stop_axis=2)
# out shape is [2, 12, 4]

# out shares data with img in dygraph mode
img[0, 0, 0, 0] = -1
print(out[0, 0, 0]) # [-1]
import paddle
import numpy as np

image_shape=(3, 2, 2)
x = np.arange(image_shape[0] * image_shape[1] * image_shape[2]).reshape(image_shape)
x = x.astype('float32')
img = paddle.to_tensor(x)
tmp = paddle.flip(img, [0,1])
print(tmp) # [[[10,11],[8, 9]], [[6, 7],[4, 5]], [[2, 3],[0, 1]]]

out = paddle.flip(tmp,-1)
print(out) # [[[11,10],[9, 8]], [[7, 6],[5, 4]], [[3, 2],[1, 0]]]
import paddle

x = paddle.to_tensor([-0.4, -0.2, 0.1, 0.3])
out = paddle.floor(x)
print(out)
# [-1. -1.  0.  0.]
import paddle

x = paddle.to_tensor([2, 3, 8, 7])
y = paddle.to_tensor([1, 5, 3, 3])
z = paddle.floor_divide(x, y)
print(z)  # [2, 0, 2, 2]
import paddle

x = paddle.to_tensor([2, 3, 8, 7])
y = paddle.to_tensor([1, 5, 3, 3])
z = paddle.remainder(x, y)
print(z)  # [0, 3, 2, 1]
import paddle
import paddle.nn as nn

class LeNet(nn.Layer):
    def __init__(self, num_classes=10):
        super(LeNet, self).__init__()
        self.num_classes = num_classes
        self.features = nn.Sequential(
            nn.Conv2D(
                1, 6, 3, stride=1, padding=1),
            nn.ReLU(),
            nn.MaxPool2D(2, 2),
            nn.Conv2D(
                6, 16, 5, stride=1, padding=0),
            nn.ReLU(),
            nn.MaxPool2D(2, 2))

        if num_classes > 0:
            self.fc = nn.Sequential(
                nn.Linear(400, 120),
                nn.Linear(120, 84),
                nn.Linear(
                    84, 10))

    def forward(self, inputs):
        x = self.features(inputs)

        if self.num_classes > 0:
            x = paddle.flatten(x, 1)
            x = self.fc(x)
        return x

lenet = LeNet()
# m is the instance of nn.Layer, x is the intput of layer, y is the output of layer.
def count_leaky_relu(m, x, y):
    x = x[0]
    nelements = x.numel()
    m.total_ops += int(nelements)

FLOPs = paddle.flops(lenet, [1, 1, 28, 28], custom_ops= {nn.LeakyReLU: count_leaky_relu},
                    print_detail=True)
print(FLOPs)

#+--------------+-----------------+-----------------+--------+--------+
#|  Layer Name  |   Input Shape   |   Output Shape  | Params | Flops  |
#+--------------+-----------------+-----------------+--------+--------+
#|   conv2d_2   |  [1, 1, 28, 28] |  [1, 6, 28, 28] |   60   | 47040  |
#|   re_lu_2    |  [1, 6, 28, 28] |  [1, 6, 28, 28] |   0    |   0    |
#| max_pool2d_2 |  [1, 6, 28, 28] |  [1, 6, 14, 14] |   0    |   0    |
#|   conv2d_3   |  [1, 6, 14, 14] | [1, 16, 10, 10] |  2416  | 241600 |
#|   re_lu_3    | [1, 16, 10, 10] | [1, 16, 10, 10] |   0    |   0    |
#| max_pool2d_3 | [1, 16, 10, 10] |  [1, 16, 5, 5]  |   0    |   0    |
#|   linear_0   |     [1, 400]    |     [1, 120]    | 48120  | 48000  |
#|   linear_1   |     [1, 120]    |     [1, 84]     | 10164  | 10080  |
#|   linear_2   |     [1, 84]     |     [1, 10]     |  850   |  840   |
#+--------------+-----------------+-----------------+--------+--------+
#Total Flops: 347560     Total Params: 61610
import paddle

data1 = paddle.full(shape=[2,1], fill_value=0, dtype='int64')
#[[0]
# [0]]

# attr shape is a list which contains Tensor.
positive_2 = paddle.full([1], 2, "int32")
data3 = paddle.full(shape=[1, positive_2], dtype='float32', fill_value=1.5)
# [[1.5 1.5]]

# attr shape is a Tensor.
shape = paddle.full([2], 2, "int32")
data4 = paddle.full(shape=shape, dtype='bool', fill_value=True)
# [[True True]
#  [True True]]

# attr fill_value is a Tensor.
val = paddle.full([1], 2.0, "float32")
data5 = paddle.full(shape=[2,1], fill_value=val, dtype='float32')
# [[2.0]
#  [2.0]]
import paddle
import numpy as np

input = paddle.full(shape=[2, 3], fill_value=0.0, dtype='float32', name='input')
output = paddle.full_like(input, 2.0)
# [[2. 2. 2.]
#  [2. 2. 2.]]
import paddle

input = paddle.to_tensor([[1,2],[3,4],[5,6]])
index = paddle.to_tensor([0,1])
output = paddle.gather(input, index, axis=0)
# expected output: [[1,2],[3,4]]
import paddle

x = paddle.to_tensor([[[1, 2], [3, 4], [5, 6]],
                      [[7, 8], [9, 10], [11, 12]]])
index = paddle.to_tensor([[0, 1]])

output = paddle.gather_nd(x, index) #[[3, 4]]
import paddle
sts = paddle.get_cuda_rng_state()
import paddle
paddle.get_default_dtype()
import paddle

flags = ['FLAGS_eager_delete_tensor_gb', 'FLAGS_check_nan_inf']
res = paddle.get_flags(flags)
print(res)
# {'FLAGS_eager_delete_tensor_gb': 0.0, 'FLAGS_check_nan_inf': False}
import paddle

def test_dygraph_grad(create_graph):
    x = paddle.ones(shape=[1], dtype='float32')
    x.stop_gradient = False
    y = x * x

    # Since y = x * x, dx = 2 * x
    dx = paddle.grad(
            outputs=[y],
            inputs=[x],
            create_graph=create_graph,
            retain_graph=True)[0]

    z = y + dx

    # If create_graph = False, the gradient of dx
    # would not be backpropagated. Therefore,
    # z = x * x + dx, and x.gradient() = 2 * x = 2.0

    # If create_graph = True, the gradient of dx
    # would be backpropagated. Therefore,
    # z = x * x + dx = x * x + 2 * x, and
    # x.gradient() = 2 * x + 2 = 4.0

    z.backward()
    return x.gradient()

print(test_dygraph_grad(create_graph=False)) # [2.]
print(test_dygraph_grad(create_graph=True)) # [4.]
import paddle

def test_dygraph_grad(grad_outputs=None):
    x = paddle.to_tensor(2.0)
    x.stop_gradient = False

    y1 = x * x
    y2 = x * 3

    # If grad_outputs=None, dy1 = [1], dy2 = [1].
    # If grad_outputs=[g1, g2], then:
    #    - dy1 = [1] if g1 is None else g1
    #    - dy2 = [1] if g2 is None else g2

    # Since y1 = x * x, dx = 2 * x * dy1.
    # Since y2 = x * 3, dx = 3 * dy2.
    # Therefore, the final result would be:
    # dx = 2 * x * dy1 + 3 * dy2 = 4 * dy1 + 3 * dy2.

    dx = paddle.grad(
        outputs=[y1, y2],
        inputs=[x],
        grad_outputs=grad_outputs)[0]

    return dx.numpy()

grad_value = paddle.to_tensor(4.0)
# dy1 = [1], dy2 = [1]
print(test_dygraph_grad(None)) # [7.]

# dy1 = [1], dy2 = [4]
print(test_dygraph_grad([None, grad_value])) # [16.]

# dy1 = [4], dy2 = [1]
print(test_dygraph_grad([grad_value, None])) # [19.]

# dy1 = [3], dy2 = [4]
grad_y1 = paddle.to_tensor(3.0)
print(test_dygraph_grad([grad_y1, grad_value])) # [24.]
import paddle

x = paddle.to_tensor([1, 2, 3])
y = paddle.to_tensor([1, 3, 2])
result1 = paddle.greater_equal(x, y)
print(result1)  # result1 = [True False True]
import paddle

x = paddle.to_tensor([1, 2, 3])
y = paddle.to_tensor([1, 3, 2])
result1 = paddle.greater_than(x, y)
print(result1)  # result1 = [False False True]
import paddle

inputs = paddle.to_tensor([1, 2, 1])
result = paddle.histogram(inputs, bins=4, min=0, max=3)
print(result) # [0, 2, 1, 0]
import paddle

x = paddle.to_tensor(
    [[1 + 6j, 2 + 5j, 3 + 4j], [4 + 3j, 5 + 2j, 6 + 1j]])
# Tensor(shape=[2, 3], dtype=complex64, place=CUDAPlace(0), stop_gradient=True,
#        [[(1+6j), (2+5j), (3+4j)],
#         [(4+3j), (5+2j), (6+1j)]])

imag_res = paddle.imag(x)
# Tensor(shape=[2, 3], dtype=float32, place=CUDAPlace(0), stop_gradient=True,
#        [[6., 5., 4.],
#         [3., 2., 1.]])

imag_t = x.imag()
# Tensor(shape=[2, 3], dtype=float32, place=CUDAPlace(0), stop_gradient=True,
#        [[6., 5., 4.],
#         [3., 2., 1.]])
import paddle
print(paddle.in_dynamic_mode())  # True, dynamic mode is turn ON by default since paddle 2.0.0

paddle.enable_static()
print(paddle.in_dynamic_mode())  # False, Now we are in static mode

paddle.disable_static()
print(paddle.in_dynamic_mode())  # True, Now we are in dynamic mode
import paddle

data = paddle.zeros(shape=[1], dtype='float32')
counter = paddle.increment(data)
# [1.]
import paddle

x = paddle.to_tensor([[1.0, 2.0, 3.0, 4.0],
                      [5.0, 6.0, 7.0, 8.0],
                      [9.0, 10.0, 11.0, 12.0]], dtype='float32')
index = paddle.to_tensor([[0, 1, 2],
                          [1, 2, 3],
                          [0, 0, 0]], dtype='int32')
target = paddle.to_tensor([[100, 200, 300, 400],
                           [500, 600, 700, 800],
                           [900, 1000, 1100, 1200]], dtype='int32')
out_z1 = paddle.index_sample(x, index)
print(out_z1)
#[[1. 2. 3.]
# [6. 7. 8.]
# [9. 9. 9.]]

# Use the index of the maximum value by topk op
# get the value of the element of the corresponding index in other tensors
top_value, top_index = paddle.topk(x, k=2)
out_z2 = paddle.index_sample(target, top_index)
print(top_value)
#[[ 4.  3.]
# [ 8.  7.]
# [12. 11.]]

print(top_index)
#[[3 2]
# [3 2]
# [3 2]]

print(out_z2)
#[[ 400  300]
# [ 800  700]
# [1200 1100]]
import paddle

x = paddle.to_tensor([[1.0, 2.0, 3.0, 4.0],
                      [5.0, 6.0, 7.0, 8.0],
                      [9.0, 10.0, 11.0, 12.0]])
index = paddle.to_tensor([0, 1, 1], dtype='int32')
out_z1 = paddle.index_select(x=x, index=index)
#[[1. 2. 3. 4.]
# [5. 6. 7. 8.]
# [5. 6. 7. 8.]]
out_z2 = paddle.index_select(x=x, index=index, axis=1)
#[[ 1.  2.  2.]
# [ 5.  6.  6.]
# [ 9. 10. 10.]]
import paddle

mat = paddle.to_tensor([[2, 0], [0, 2]], dtype='float32')
inv = paddle.inverse(mat)
print(inv) # [[0.5, 0], [0, 0.5]]
import paddle

input = paddle.rand(shape=[4, 32, 32], dtype='float32')
res = paddle.is_empty(x=input)
print("res:", res)
# ('res:', Tensor: eager_tmp_1
#    - place: CPUPlace
#    - shape: [1]
#    - layout: NCHW
#    - dtype: bool
#    - data: [0])
import paddle

input1 = paddle.rand(shape=[2, 3, 5], dtype='float32')
check = paddle.is_tensor(input1)
print(check)  #True

input3 = [1, 4]
check = paddle.is_tensor(input3)
print(check)  #False
import paddle

x = paddle.to_tensor([float('-inf'), -2, 3.6, float('inf'), 0, float('-nan'), float('nan')])
out = paddle.tensor.isfinite(x)
print(out)  # [False  True  True False  True False False]
import paddle
x = paddle.to_tensor([float('-inf'), -2, 3.6, float('inf'), 0, float('-nan'), float('nan')])
out = paddle.tensor.isinf(x)
print(out)  # [ True False False  True False False False]
import paddle
x = paddle.to_tensor([float('-inf'), -2, 3.6, float('inf'), 0, float('-nan'), float('nan')])
out = paddle.tensor.isnan(x)
print(out)  # [False False False False False  True  True]
import paddle
x = paddle.to_tensor([[1, 2], [3, 4]], dtype='int64')
y = paddle.to_tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9]], dtype='int64')
out = paddle.kron(x, y)
print(out)
#        [[1, 2, 3, 2, 4, 6],
#         [ 4,  5,  6,  8, 10, 12],
#         [ 7,  8,  9, 14, 16, 18],
#         [ 3,  6,  9,  4,  8, 12],
#         [12, 15, 18, 16, 20, 24],
#         [21, 24, 27, 28, 32, 36]])
import paddle

x = paddle.to_tensor([1, 2, 3])
y = paddle.to_tensor([1, 3, 2])
result1 = paddle.less_equal(x, y)
print(result1)  # result1 = [True True False]
import paddle

x = paddle.to_tensor([1, 2, 3])
y = paddle.to_tensor([1, 3, 2])
result1 = paddle.less_than(x, y)
print(result1)  # result1 = [False True False]
import paddle

x = paddle.to_tensor([-0.4, -0.2, 0.1, 0.3])
out = paddle.lgamma(x)
print(out)
# [1.31452441, 1.76149750, 2.25271273, 1.09579802]
import paddle
data = paddle.linspace(0, 10, 5, 'float32') # [0.0,  2.5,  5.0,  7.5, 10.0]
data = paddle.linspace(0, 10, 1, 'float32') # [0.0]
# example 1: dynamic graph
import paddle
emb = paddle.nn.Embedding(10, 10)
layer_state_dict = emb.state_dict()

# save state_dict of emb
paddle.save(layer_state_dict, "emb.pdparams")

scheduler = paddle.optimizer.lr.NoamDecay(
    d_model=0.01, warmup_steps=100, verbose=True)
adam = paddle.optimizer.Adam(
    learning_rate=scheduler,
    parameters=emb.parameters())
opt_state_dict = adam.state_dict()

# save state_dict of optimizer
paddle.save(opt_state_dict, "adam.pdopt")
# save weight of emb
paddle.save(emb.weight, "emb.weight.pdtensor")

# load state_dict of emb
load_layer_state_dict = paddle.load("emb.pdparams")
# load state_dict of optimizer
load_opt_state_dict = paddle.load("adam.pdopt")
# load weight of emb
load_weight = paddle.load("emb.weight.pdtensor")


# example 2: Load multiple state_dict at the same time
from paddle import nn
from paddle.optimizer import Adam

layer = paddle.nn.Linear(3, 4)
adam = Adam(learning_rate=0.001, parameters=layer.parameters())
obj = {'model': layer.state_dict(), 'opt': adam.state_dict(), 'epoch': 100}
path = 'example/model.pdparams'
paddle.save(obj, path)
obj_load = paddle.load(path)


# example 3: static graph
import paddle
import paddle.static as static

paddle.enable_static()

# create network
x = paddle.static.data(name="x", shape=[None, 224], dtype='float32')
z = paddle.static.nn.fc(x, 10)

place = paddle.CPUPlace()
exe = paddle.static.Executor(place)
exe.run(paddle.static.default_startup_program())
prog = paddle.static.default_main_program()
for var in prog.list_vars():
    if list(var.shape) == [224, 10]:
        tensor = var.get_value()
        break

# save/load tensor
path_tensor = 'temp/tensor.pdtensor'
paddle.save(tensor, path_tensor)
load_tensor = paddle.load(path_tensor)

# save/load state_dict
path_state_dict = 'temp/model.pdparams'
paddle.save(prog.state_dict("param"), path_tensor)
load_state_dict = paddle.load(path_tensor)


# example 4: load program
import paddle

paddle.enable_static()

data = paddle.static.data(
    name='x_static_save', shape=(None, 224), dtype='float32')
y_static = z = paddle.static.nn.fc(data, 10)
main_program = paddle.static.default_main_program()
path = "example/main_program.pdmodel"
paddle.save(main_program, path)
load_main = paddle.load(path)
print(load_main)


# example 5: save object to memory
from io import BytesIO
import paddle
from paddle.nn import Linear
paddle.disable_static()

linear = Linear(5, 10)
state_dict = linear.state_dict()
byio = BytesIO()
paddle.save(state_dict, byio)
tensor = paddle.randn([2, 3], dtype='float32')
paddle.save(tensor, byio)
byio.seek(0)
# load state_dict
dict_load = paddle.load(byio)
import paddle

x = [[2,3,4], [7,8,9]]
x = paddle.to_tensor(x, dtype='float32')
res = paddle.log(x)
# [[0.693147, 1.09861, 1.38629], [1.94591, 2.07944, 2.19722]]
import paddle

# example 1: x is a float
x_i = paddle.to_tensor([[1.0], [10.0]])
res = paddle.log10(x_i) # [[0.], [1.0]]

# example 2: x is float32
x_i = paddle.full(shape=[1], fill_value=10, dtype='float32')
paddle.to_tensor(x_i)
res = paddle.log10(x_i)
print(res) # [1.0]

# example 3: x is float64
x_i = paddle.full(shape=[1], fill_value=10, dtype='float64')
paddle.to_tensor(x_i)
res = paddle.log10(x_i)
print(res) # [1.0]
import paddle

data = paddle.to_tensor([[0], [1]], dtype='float32')
res = paddle.log1p(data)
# [[0.], [0.6931472]]
import paddle

# example 1: x is a float
x_i = paddle.to_tensor([[1.0], [2.0]])
res = paddle.log2(x_i) # [[0.], [1.0]]

# example 2: x is float32
x_i = paddle.full(shape=[1], fill_value=2, dtype='float32')
paddle.to_tensor(x_i)
res = paddle.log2(x_i)
print(res) # [1.0]

# example 3: x is float64
x_i = paddle.full(shape=[1], fill_value=2, dtype='float64')
paddle.to_tensor(x_i)
res = paddle.log2(x_i)
print(res) # [1.0]
import paddle

x = paddle.to_tensor([True])
y = paddle.to_tensor([True, False, True, False])
res = paddle.logical_and(x, y)
print(res) # [True False True False]
import paddle

x = paddle.to_tensor([True, False, True, False])
res = paddle.logical_not(x)
print(res) # [False  True False  True]
import paddle
import numpy as np

x_data = np.array([True, False], dtype=np.bool).reshape(2, 1)
y_data = np.array([True, False, True, False], dtype=np.bool).reshape(2, 2)
x = paddle.to_tensor(x_data)
y = paddle.to_tensor(y_data)
res = paddle.logical_or(x, y)
print(res) # [[ True  True] [ True False]]
import paddle
import numpy as np

x_data = np.array([True, False], dtype=np.bool).reshape([2, 1])
y_data = np.array([True, False, True, False], dtype=np.bool).reshape([2, 2])
x = paddle.to_tensor(x_data)
y = paddle.to_tensor(y_data)
res = paddle.logical_xor(x, y)
print(res) # [[False,  True], [ True, False]]
import paddle

x = paddle.to_tensor([[-1.5, 0., 2.], [3., 1.2, -2.4]])
out1 = paddle.logsumexp(x) # [3.4691226]
out2 = paddle.logsumexp(x, 1) # [2.15317821, 3.15684602]
import paddle

x = paddle.to_tensor([[1.0, 2.0, 3.0, 4.0],
                      [5.0, 6.0, 7.0, 8.0],
                      [9.0, 10.0, 11.0, 12.0]])
mask = paddle.to_tensor([[True, False, False, False],
                         [True, True, False, False],
                         [True, False, False, False]])
out = paddle.masked_select(x, mask)
#[1.0 5.0 6.0 9.0]
import paddle
import numpy as np

# vector * vector
x_data = np.random.random([10]).astype(np.float32)
y_data = np.random.random([10]).astype(np.float32)
x = paddle.to_tensor(x_data)
y = paddle.to_tensor(y_data)
z = paddle.matmul(x, y)
print(z.numpy().shape)
# [1]

# matrix * vector
x_data = np.random.random([10, 5]).astype(np.float32)
y_data = np.random.random([5]).astype(np.float32)
x = paddle.to_tensor(x_data)
y = paddle.to_tensor(y_data)
z = paddle.matmul(x, y)
print(z.numpy().shape)
# [10]

# batched matrix * broadcasted vector
x_data = np.random.random([10, 5, 2]).astype(np.float32)
y_data = np.random.random([2]).astype(np.float32)
x = paddle.to_tensor(x_data)
y = paddle.to_tensor(y_data)
z = paddle.matmul(x, y)
print(z.numpy().shape)
# [10, 5]

# batched matrix * batched matrix
x_data = np.random.random([10, 5, 2]).astype(np.float32)
y_data = np.random.random([10, 2, 5]).astype(np.float32)
x = paddle.to_tensor(x_data)
y = paddle.to_tensor(y_data)
z = paddle.matmul(x, y)
print(z.numpy().shape)
# [10, 5, 5]

# batched matrix * broadcasted matrix
x_data = np.random.random([10, 1, 5, 2]).astype(np.float32)
y_data = np.random.random([1, 3, 2, 5]).astype(np.float32)
x = paddle.to_tensor(x_data)
y = paddle.to_tensor(y_data)
z = paddle.matmul(x, y)
print(z.numpy().shape)
# [10, 3, 5, 5]
import paddle

# data_x is a Tensor with shape [2, 4]
# the axis is a int element

x = paddle.to_tensor([[0.2, 0.3, 0.5, 0.9],
                      [0.1, 0.2, 0.6, 0.7]])
result1 = paddle.max(x)
print(result1)
#[0.9]
result2 = paddle.max(x, axis=0)
print(result2)
#[0.2 0.3 0.6 0.9]
result3 = paddle.max(x, axis=-1)
print(result3)
#[0.9 0.7]
result4 = paddle.max(x, axis=1, keepdim=True)
print(result4)
#[[0.9]
# [0.7]]

# data_y is a Tensor with shape [2, 2, 2]
# the axis is list

y = paddle.to_tensor([[[1.0, 2.0], [3.0, 4.0]],
                      [[5.0, 6.0], [7.0, 8.0]]])
result5 = paddle.max(y, axis=[1, 2])
print(result5)
#[4. 8.]
result6 = paddle.max(y, axis=[0, 1])
print(result6)
#[7. 8.]
import numpy as np
import paddle

x = paddle.to_tensor([[1, 2], [7, 8]])
y = paddle.to_tensor([[3, 4], [5, 6]])
res = paddle.maximum(x, y)
print(res)
#    [[3, 4],
#     [7, 8]]

x = paddle.to_tensor([[1, 2, 3], [1, 2, 3]])
y = paddle.to_tensor([3, 0, 4])
res = paddle.maximum(x, y)
print(res)
#    [[3, 2, 4],
#     [3, 2, 4]]

x = paddle.to_tensor([2, 3, 5], dtype='float32')
y = paddle.to_tensor([1, np.nan, np.nan], dtype='float32')
res = paddle.maximum(x, y)
print(res)
#    [ 2., nan, nan]

x = paddle.to_tensor([5, 3, np.inf], dtype='float32')
y = paddle.to_tensor([1, -np.inf, 5], dtype='float32')
res = paddle.maximum(x, y)
print(res)
#    [  5.,   3., inf.]
import paddle

x = paddle.to_tensor([[[1., 2., 3., 4.],
                       [5., 6., 7., 8.],
                       [9., 10., 11., 12.]],
                      [[13., 14., 15., 16.],
                       [17., 18., 19., 20.],
                       [21., 22., 23., 24.]]])
out1 = paddle.mean(x)
# [12.5]
out2 = paddle.mean(x, axis=-1)
# [[ 2.5  6.5 10.5]
#  [14.5 18.5 22.5]]
out3 = paddle.mean(x, axis=-1, keepdim=True)
# [[[ 2.5]
#   [ 6.5]
#   [10.5]]
#  [[14.5]
#   [18.5]
#   [22.5]]]
out4 = paddle.mean(x, axis=[0, 2])
# [ 8.5 12.5 16.5]
import paddle

x = paddle.arange(12).reshape([3, 4])
# x is [[0 , 1 , 2 , 3 ],
#       [4 , 5 , 6 , 7 ],
#       [8 , 9 , 10, 11]]

y1 = paddle.median(x)
# y1 is [5.5]

y2 = paddle.median(x, axis=0)
# y2 is [4., 5., 6., 7.]

y3 = paddle.median(x, axis=1)
# y3 is [1.5, 5.5, 9.5]

y4 = paddle.median(x, axis=0, keepdim=True)
# y4 is [[4., 5., 6., 7.]]
import paddle

x = paddle.randint(low=0, high=100, shape=[100])
y = paddle.randint(low=0, high=100, shape=[200])

grid_x, grid_y = paddle.meshgrid(x, y)

print(grid_x.shape)
print(grid_y.shape)

#the shape of res_1 is (100, 200)
#the shape of res_2 is (100, 200)
import paddle

# x is a tensor with shape [2, 4]
# the axis is a int element
x = paddle.to_tensor([[0.2, 0.3, 0.5, 0.9],
                      [0.1, 0.2, 0.6, 0.7]])
result1 = paddle.min(x)
print(result1)
#[0.1]
result2 = paddle.min(x, axis=0)
print(result2)
#[0.1 0.2 0.5 0.7]
result3 = paddle.min(x, axis=-1)
print(result3)
#[0.2 0.1]
result4 = paddle.min(x, axis=1, keepdim=True)
print(result4)
#[[0.2]
# [0.1]]

# y is a Tensor with shape [2, 2, 2]
# the axis is list
y = paddle.to_tensor([[[1.0, 2.0], [3.0, 4.0]],
                      [[5.0, 6.0], [7.0, 8.0]]])
result5 = paddle.min(y, axis=[1, 2])
print(result5)
#[1. 5.]
result6 = paddle.min(y, axis=[0, 1])
print(result6)
#[1. 2.]
import numpy as np
import paddle

x = paddle.to_tensor([[1, 2], [7, 8]])
y = paddle.to_tensor([[3, 4], [5, 6]])
res = paddle.minimum(x, y)
print(res)
#       [[1, 2],
#        [5, 6]]

x = paddle.to_tensor([[[1, 2, 3], [1, 2, 3]]])
y = paddle.to_tensor([3, 0, 4])
res = paddle.minimum(x, y)
print(res)
#       [[[1, 0, 3],
#         [1, 0, 3]]]

x = paddle.to_tensor([2, 3, 5], dtype='float32')
y = paddle.to_tensor([1, np.nan, np.nan], dtype='float32')
res = paddle.minimum(x, y)
print(res)
#       [ 1., nan, nan]

x = paddle.to_tensor([5, 3, np.inf], dtype='float64')
y = paddle.to_tensor([1, -np.inf, 5], dtype='float64')
res = paddle.minimum(x, y)
print(res)
#       [   1., -inf.,    5.]
import paddle
input = paddle.arange(1, 7).reshape((3, 2)).astype('float32')
mat2 = paddle.arange(1, 9).reshape((2, 4)).astype('float32')
out = paddle.mm(input, mat2)
print(out)
#        [[11., 14., 17., 20.],
#         [23., 30., 37., 44.],
#         [35., 46., 57., 68.]])
import paddle
import paddle.nn as nn
import paddle.vision.transforms as T
from paddle.static import InputSpec

device = paddle.set_device('cpu') # or 'gpu'

net = nn.Sequential(
    nn.Flatten(1),
    nn.Linear(784, 200),
    nn.Tanh(),
    nn.Linear(200, 10))

# inputs and labels are not required for dynamic graph.
input = InputSpec([None, 784], 'float32', 'x')
label = InputSpec([None, 1], 'int64', 'label')

model = paddle.Model(net, input, label)
optim = paddle.optimizer.SGD(learning_rate=1e-3,
    parameters=model.parameters())

model.prepare(optim,
              paddle.nn.CrossEntropyLoss(),
              paddle.metric.Accuracy())

transform = T.Compose([
    T.Transpose(),
    T.Normalize([127.5], [127.5])
])
data = paddle.vision.datasets.MNIST(mode='train', transform=transform)
model.fit(data, epochs=2, batch_size=32, verbose=1)
import paddle
import paddle.nn as nn
import paddle.vision.transforms as T

def run_example_code():
  device = paddle.set_device('gpu')

  net = nn.Sequential(nn.Flatten(1), nn.Linear(784, 200), nn.Tanh(),
                      nn.Linear(200, 10))

  model = paddle.Model(net)
  optim = paddle.optimizer.SGD(learning_rate=1e-3, parameters=model.parameters())

  amp_configs = {
      "level": "O1",
      "custom_white_list": {'conv2d'},
      "use_dynamic_loss_scaling": True
  }
  model.prepare(optim,
      paddle.nn.CrossEntropyLoss(),
      paddle.metric.Accuracy(),
      amp_configs=amp_configs)

  transform = T.Compose([T.Transpose(), T.Normalize([127.5], [127.5])])
  data = paddle.vision.datasets.MNIST(mode='train', transform=transform)
  model.fit(data, epochs=2, batch_size=32, verbose=1)

# mixed precision training is only supported on GPU now.
if paddle.is_compiled_with_cuda():
  run_example_code()
import numpy as np
import paddle
import paddle.nn as nn
from paddle.static import InputSpec

device = paddle.set_device('cpu') # or 'gpu'

net = nn.Sequential(
    nn.Linear(784, 200),
    nn.Tanh(),
    nn.Linear(200, 10))

input = InputSpec([None, 784], 'float32', 'x')
label = InputSpec([None, 1], 'int64', 'label')
model = paddle.Model(net, input, label)
optim = paddle.optimizer.SGD(learning_rate=1e-3,
    parameters=model.parameters())
model.prepare(optim, paddle.nn.CrossEntropyLoss())
data = np.random.random(size=(4,784)).astype(np.float32)
label = np.random.randint(0, 10, size=(4, 1)).astype(np.int64)
loss = model.train_batch([data], [label])
print(loss)
import numpy as np
import paddle
import paddle.nn as nn
from paddle.static import InputSpec

device = paddle.set_device('cpu') # or 'gpu'

net = nn.Sequential(
    nn.Linear(784, 200),
    nn.Tanh(),
    nn.Linear(200, 10))

input = InputSpec([None, 784], 'float32', 'x')
label = InputSpec([None, 1], 'int64', 'label')
model = paddle.Model(net, input, label)
optim = paddle.optimizer.SGD(learning_rate=1e-3,
    parameters=model.parameters())
model.prepare(optim,
              paddle.nn.CrossEntropyLoss())
data = np.random.random(size=(4,784)).astype(np.float32)
label = np.random.randint(0, 10, size=(4, 1)).astype(np.int64)
loss = model.eval_batch([data], [label])
print(loss)
import numpy as np
import paddle
import paddle.nn as nn
from paddle.static import InputSpec

device = paddle.set_device('cpu') # or 'gpu'

input = InputSpec([None, 784], 'float32', 'x')
label = InputSpec([None, 1], 'int64', 'label')

net = nn.Sequential(
    nn.Linear(784, 200),
    nn.Tanh(),
    nn.Linear(200, 10),
    nn.Softmax())

model = paddle.Model(net, input, label)
model.prepare()
data = np.random.random(size=(4,784)).astype(np.float32)
out = model.predict_batch([data])
print(out)
import paddle
import paddle.nn as nn
import paddle.vision.transforms as T
from paddle.static import InputSpec

class Mnist(nn.Layer):
    def __init__(self):
        super(Mnist, self).__init__()
        self.net = nn.Sequential(
            nn.Flatten(1),
            nn.Linear(784, 200),
            nn.Tanh(),
            nn.Linear(200, 10),
            nn.Softmax())

    def forward(self, x):
        return self.net(x)

dynamic = True  # False
# if use static graph, do not set
if not dynamic:
    paddle.enable_static()

input = InputSpec([None, 784], 'float32', 'x')
label = InputSpec([None, 1], 'int64', 'label')
model = paddle.Model(Mnist(), input, label)
optim = paddle.optimizer.SGD(learning_rate=1e-3,
    parameters=model.parameters())
model.prepare(optim, paddle.nn.CrossEntropyLoss())

transform = T.Compose([
    T.Transpose(),
    T.Normalize([127.5], [127.5])
])
data = paddle.vision.datasets.MNIST(mode='train', transform=transform)

model.fit(data, epochs=1, batch_size=32, verbose=0)
model.save('checkpoint/test')  # save for training
model.save('inference_model', False)  # save for inference
import paddle
import paddle.nn as nn
from paddle.static import InputSpec

device = paddle.set_device('cpu')

input = InputSpec([None, 784], 'float32', 'x')

model = paddle.Model(nn.Sequential(
    nn.Linear(784, 200),
    nn.Tanh(),
    nn.Linear(200, 10),
    nn.Softmax()), input)

model.save('checkpoint/test')
model.load('checkpoint/test')
import paddle
import paddle.nn as nn
from paddle.static import InputSpec

input = InputSpec([None, 784], 'float32', 'x')

model = paddle.Model(nn.Sequential(
    nn.Linear(784, 200),
    nn.Tanh(),
    nn.Linear(200, 10)), input)

params = model.parameters()
import paddle
import paddle.vision.transforms as T
from paddle.vision.datasets import MNIST
from paddle.static import InputSpec

dynamic = True
if not dynamic:
    paddle.enable_static()

transform = T.Compose([
    T.Transpose(),
    T.Normalize([127.5], [127.5])
])
train_dataset = MNIST(mode='train', transform=transform)
val_dataset = MNIST(mode='test', transform=transform)

input = InputSpec([None, 1, 28, 28], 'float32', 'image')
label = InputSpec([None, 1], 'int64', 'label')

model = paddle.Model(
    paddle.vision.models.LeNet(),
    input, label)
optim = paddle.optimizer.Adam(
    learning_rate=0.001, parameters=model.parameters())
model.prepare(
    optim,
    paddle.nn.CrossEntropyLoss(),
    paddle.metric.Accuracy(topk=(1, 2)))
model.fit(train_dataset,
          val_dataset,
          epochs=2,
          batch_size=64,
          save_dir='mnist_checkpoint')
import paddle
import paddle.vision.transforms as T
from paddle.vision.datasets import MNIST
from paddle.static import InputSpec

dynamic = True
if not dynamic:
    paddle.enable_static()

transform = T.Compose([
      T.Transpose(),
      T.Normalize([127.5], [127.5])
  ])
train_dataset = MNIST(mode='train', transform=transform)
train_loader = paddle.io.DataLoader(train_dataset,
    batch_size=64)
val_dataset = MNIST(mode='test', transform=transform)
val_loader = paddle.io.DataLoader(val_dataset,
    batch_size=64)

input = InputSpec([None, 1, 28, 28], 'float32', 'image')
label = InputSpec([None, 1], 'int64', 'label')

model = paddle.Model(
    paddle.vision.models.LeNet(), input, label)
optim = paddle.optimizer.Adam(
    learning_rate=0.001, parameters=model.parameters())
model.prepare(
    optim,
    paddle.nn.CrossEntropyLoss(),
    paddle.metric.Accuracy(topk=(1, 2)))
model.fit(train_loader,
          val_loader,
          epochs=2,
          save_dir='mnist_checkpoint')
import paddle
import paddle.vision.transforms as T
from paddle.static import InputSpec

# declarative mode
transform = T.Compose([
        T.Transpose(),
        T.Normalize([127.5], [127.5])
    ])
val_dataset = paddle.vision.datasets.MNIST(mode='test', transform=transform)

input = InputSpec([-1, 1, 28, 28], 'float32', 'image')
label = InputSpec([None, 1], 'int64', 'label')
model = paddle.Model(paddle.vision.models.LeNet(), input, label)
model.prepare(metrics=paddle.metric.Accuracy())
result = model.evaluate(val_dataset, batch_size=64)
print(result)
import numpy as np
import paddle
from paddle.static import InputSpec

class MnistDataset(paddle.vision.datasets.MNIST):
    def __init__(self, mode, return_label=True):
        super(MnistDataset, self).__init__(mode=mode)
        self.return_label = return_label

    def __getitem__(self, idx):
        img = np.reshape(self.images[idx], [1, 28, 28])
        if self.return_label:
            return img, np.array(self.labels[idx]).astype('int64')
        return img,

    def __len__(self):
        return len(self.images)

test_dataset = MnistDataset(mode='test', return_label=False)

# imperative mode
input = InputSpec([-1, 1, 28, 28], 'float32', 'image')
model = paddle.Model(paddle.vision.models.LeNet(), input)
model.prepare()
result = model.predict(test_dataset, batch_size=64)
print(len(result[0]), result[0][0].shape)

# declarative mode
device = paddle.set_device('cpu')
paddle.enable_static()
input = InputSpec([-1, 1, 28, 28], 'float32', 'image')
model = paddle.Model(paddle.vision.models.LeNet(), input)
model.prepare()

result = model.predict(test_dataset, batch_size=64)
print(len(result[0]), result[0][0].shape)
import paddle
from paddle.static import InputSpec

input = InputSpec([None, 1, 28, 28], 'float32', 'image')
label = InputSpec([None, 1], 'int64', 'label')

model = paddle.Model(paddle.vision.models.LeNet(),
    input, label)
optim = paddle.optimizer.Adam(
    learning_rate=0.001, parameters=model.parameters())
model.prepare(
    optim,
    paddle.nn.CrossEntropyLoss())

params_info = model.summary()
print(params_info)
import paddle

paddle.seed(100) # on CPU device
x = paddle.rand([2,4])
print(x)
# [[0.5535528  0.20714243 0.01162981 0.51577556]
# [0.36369765 0.2609165  0.18905126 0.5621971 ]]

paddle.seed(200) # on CPU device
out1 = paddle.multinomial(x, num_samples=5, replacement=True)
print(out1)
# [[3 3 0 0 0]
# [3 3 3 1 0]]

# out2 = paddle.multinomial(x, num_samples=5)
# InvalidArgumentError: When replacement is False, number of samples
#  should be less than non-zero categories

paddle.seed(300) # on CPU device
out3 = paddle.multinomial(x, num_samples=3)
print(out3)
# [[3 0 1]
# [3 1 0]]
import paddle
import numpy as np
img1 = np.array([[1, 2], [3, 4]]).astype(np.float32)
img2 = np.array([[5, 6], [7, 8]]).astype(np.float32)
inputs = [paddle.to_tensor(img1), paddle.to_tensor(img2)]
index = paddle.to_tensor(np.array([[1], [0]]).astype(np.int32))
res = paddle.multiplex(inputs, index)
print(res) # [array([[5., 6.], [3., 4.]], dtype=float32)]
import paddle

x = paddle.to_tensor([[1, 2], [3, 4]])
y = paddle.to_tensor([[5, 6], [7, 8]])
res = paddle.multiply(x, y)
print(res) # [[5, 12], [21, 32]]

x = paddle.to_tensor([[[1, 2, 3], [1, 2, 3]]])
y = paddle.to_tensor([2])
res = paddle.multiply(x, y)
print(res) # [[[2, 4, 6], [2, 4, 6]]]
# x: [M, N], vec: [N]
# paddle.mv(x, vec)  # out: [M]

import numpy as np
import paddle

x_data = np.array([[2, 1, 3], [3, 0, 1]]).astype("float64")
x = paddle.to_tensor(x_data)
vec_data = np.array([3, 5, 1])
vec = paddle.to_tensor(vec_data).astype("float64")
out = paddle.mv(x, vec)
import paddle

x = paddle.to_tensor([-0.4, -0.2, 0.1, 0.3])
out = paddle.neg(x)
print(out)
# [0.4 0.2 -0.1 -0.3]
import paddle

x1 = paddle.to_tensor([[1.0, 0.0, 0.0],
                       [0.0, 2.0, 0.0],
                       [0.0, 0.0, 3.0]])
x2 = paddle.to_tensor([0.0, 1.0, 0.0, 3.0])
out_z1 = paddle.nonzero(x1)
print(out_z1)
#[[0 0]
# [1 1]
# [2 2]]
out_z1_tuple = paddle.nonzero(x1, as_tuple=True)
for out in out_z1_tuple:
    print(out)
#[[0]
# [1]
# [2]]
#[[0]
# [1]
# [2]]
out_z2 = paddle.nonzero(x2)
print(out_z2)
#[[1]
# [3]]
out_z2_tuple = paddle.nonzero(x2, as_tuple=True)
for out in out_z2_tuple:
    print(out)
#[[1]
# [3]]
import paddle
import numpy as np
shape=[2, 3, 4]
np_input = np.arange(24).astype('float32') - 12
np_input = np_input.reshape(shape)
x = paddle.to_tensor(np_input)
#[[[-12. -11. -10.  -9.] [ -8.  -7.  -6.  -5.] [ -4.  -3.  -2.  -1.]]
# [[  0.   1.   2.   3.] [  4.   5.   6.   7.] [  8.   9.  10.  11.]]]

# compute frobenius norm along last two dimensions.
out_fro = paddle.norm(x, p='fro', axis=[0,1])
# out_fro.numpy() [17.435596 16.911535 16.7332   16.911535]

# compute 2-order vector norm along last dimension.
out_pnorm = paddle.norm(x, p=2, axis=-1)
#out_pnorm.numpy(): [[21.118711  13.190906   5.477226]
#                    [ 3.7416575 11.224972  19.131126]]

# compute 2-order  norm along [0,1] dimension.
out_pnorm = paddle.norm(x, p=2, axis=[0,1])
#out_pnorm.numpy(): [17.435596 16.911535 16.7332   16.911535]

# compute inf-order  norm
out_pnorm = paddle.norm(x, p=np.inf)
#out_pnorm.numpy()  = [12.]
out_pnorm = paddle.norm(x, p=np.inf, axis=0)
#out_pnorm.numpy(): [[12. 11. 10. 9.] [8. 7. 6. 7.] [8. 9. 10. 11.]]

# compute -inf-order  norm
out_pnorm = paddle.norm(x, p=-np.inf)
#out_pnorm.numpy(): [0.]
out_pnorm = paddle.norm(x, p=-np.inf, axis=0)
#out_pnorm.numpy(): [[0. 1. 2. 3.] [4. 5. 6. 5.] [4. 3. 2. 1.]]
import paddle

out1 = paddle.normal(shape=[2, 3])
# [[ 0.17501129  0.32364586  1.561118  ]  # random
#  [-1.7232178   1.1545963  -0.76156676]]  # random

mean_tensor = paddle.to_tensor([1.0, 2.0, 3.0])
out2 = paddle.normal(mean=mean_tensor)
# [ 0.18644847 -1.19434458  3.93694787]  # random

std_tensor = paddle.to_tensor([1.0, 2.0, 3.0])
out3 = paddle.normal(mean=mean_tensor, std=std_tensor)
# [1.00780561 3.78457445 5.81058198]  # random
import paddle

x = paddle.to_tensor([1, 2, 3])
y = paddle.to_tensor([1, 3, 2])
result1 = paddle.not_equal(x, y)
print(result1)  # result1 = [False True True]
.. code-block:: python
  import paddle
  npu_place = paddle.NPUPlace(0)
import paddle

x = paddle.full(shape=[4, 5, 7], fill_value=0, dtype='int32')
numel = paddle.numel(x) # 140
import paddle

# default dtype for ones OP
data1 = paddle.ones(shape=[3, 2])
# [[1. 1.]
#  [1. 1.]
#  [1. 1.]]

data2 = paddle.ones(shape=[2, 2], dtype='int32')
# [[1 1]
#  [1 1]]

# shape is a Tensor
shape = paddle.full(shape=[2], dtype='int32', fill_value=2)
data3 = paddle.ones(shape=shape, dtype='int32')
# [[1 1]
#  [1 1]]
import paddle

x = paddle.to_tensor([1,2,3])
out1 = paddle.ones_like(x) # [1., 1., 1.]
out2 = paddle.ones_like(x, dtype='int32') # [1, 1, 1]
import paddle

weight_attr = paddle.ParamAttr(name="weight",
                               learning_rate=0.5,
                               regularizer=paddle.regularizer.L2Decay(1.0),
                               trainable=True)
print(weight_attr.name) # "weight"
paddle.nn.Linear(3, 4, weight_attr=weight_attr)
import paddle

x = paddle.to_tensor([1, 2, 3], dtype='float32')

# example 1: y is a float or int
res = paddle.pow(x, 2)
print(res)
# Tensor(shape=[3], dtype=float32, place=CUDAPlace(0), stop_gradient=True,
#        [1., 4., 9.])
res = paddle.pow(x, 2.5)
print(res)
# Tensor(shape=[3], dtype=float32, place=CUDAPlace(0), stop_gradient=True,
#        [1.         , 5.65685415 , 15.58845711])

# example 2: y is a Tensor
y = paddle.to_tensor([2], dtype='float32')
res = paddle.pow(x, y)
print(res)
# Tensor(shape=[3], dtype=float32, place=CUDAPlace(0), stop_gradient=True,
#        [1., 4., 9.])
import paddle

# the axis is a int element
x = paddle.to_tensor([[0.2, 0.3, 0.5, 0.9],
                      [0.1, 0.2, 0.6, 0.7]])
out1 = paddle.prod(x)
# [0.0002268]

out2 = paddle.prod(x, -1)
# [0.027  0.0084]

out3 = paddle.prod(x, 0)
# [0.02 0.06 0.3  0.63]

out4 = paddle.prod(x, 0, keepdim=True)
# [[0.02 0.06 0.3  0.63]]

out5 = paddle.prod(x, 0, dtype='int64')
# [0 0 0 0]

# the axis is list
y = paddle.to_tensor([[[1.0, 2.0], [3.0, 4.0]],
                      [[5.0, 6.0], [7.0, 8.0]]])
out6 = paddle.prod(y, [0, 1])
# [105. 384.]

out7 = paddle.prod(y, (1, 2))
# [  24. 1680.]
import paddle

# example 1: attr shape is a list which doesn't contain Tensor.
out1 = paddle.rand(shape=[2, 3])
# [[0.451152  , 0.55825245, 0.403311  ],  # random
#  [0.22550228, 0.22106001, 0.7877319 ]]  # random

# example 2: attr shape is a list which contains Tensor.
dim1 = paddle.to_tensor([2], 'int64')
dim2 = paddle.to_tensor([3], 'int32')
out2 = paddle.rand(shape=[dim1, dim2, 2])
# [[[0.8879919 , 0.25788337],  # random
#   [0.28826773, 0.9712097 ],  # random
#   [0.26438272, 0.01796806]],  # random
#  [[0.33633623, 0.28654453],  # random
#   [0.79109055, 0.7305809 ],  # random
#   [0.870881  , 0.2984597 ]]]  # random

# example 3: attr shape is a Tensor, the data type must be int64 or int32.
shape_tensor = paddle.to_tensor([2, 3])
out3 = paddle.rand(shape_tensor)
# [[0.22920267, 0.841956  , 0.05981819],  # random
#  [0.4836288 , 0.24573246, 0.7516129 ]]  # random
import paddle

# example 1:
# attr shape is a list which doesn't contain Tensor.
out1 = paddle.randint(low=-5, high=5, shape=[3])
# [0, -3, 2]  # random

# example 2:
# attr shape is a list which contains Tensor.
dim1 = paddle.to_tensor([2], 'int64')
dim2 = paddle.to_tensor([3], 'int32')
out2 = paddle.randint(low=-5, high=5, shape=[dim1, dim2])
# [[0, -1, -3],  # random
#  [4, -2,  0]]  # random

# example 3:
# attr shape is a Tensor
shape_tensor = paddle.to_tensor(3)
out3 = paddle.randint(low=-5, high=5, shape=shape_tensor)
# [-2, 2, 3]  # random

# example 4:
# data type is int32
out4 = paddle.randint(low=-5, high=5, shape=[3], dtype='int32')
# [-5, 4, -4]  # random

# example 5:
# Input only one parameter
# low=0, high=10, shape=[1], dtype='int64'
out5 = paddle.randint(10)
# [7]  # random
import paddle

# example 1: attr shape is a list which doesn't contain Tensor.
out1 = paddle.randn(shape=[2, 3])
# [[-2.923464  ,  0.11934398, -0.51249987],  # random
#  [ 0.39632758,  0.08177969,  0.2692008 ]]  # random

# example 2: attr shape is a list which contains Tensor.
dim1 = paddle.to_tensor([2], 'int64')
dim2 = paddle.to_tensor([3], 'int32')
out2 = paddle.randn(shape=[dim1, dim2, 2])
# [[[-2.8852394 , -0.25898588],  # random
#   [-0.47420555,  0.17683524],  # random
#   [-0.7989969 ,  0.00754541]],  # random
#  [[ 0.85201347,  0.32320443],  # random
#   [ 1.1399018 ,  0.48336947],  # random
#   [ 0.8086993 ,  0.6868893 ]]]  # random

# example 3: attr shape is a Tensor, the data type must be int64 or int32.
shape_tensor = paddle.to_tensor([2, 3])
out3 = paddle.randn(shape_tensor)
# [[-2.878077 ,  0.17099959,  0.05111201]  # random
#  [-0.3761474, -1.044801  ,  1.1870178 ]]  # random
import paddle

out1 = paddle.randperm(5)
# [4, 1, 2, 3, 0]  # random

out2 = paddle.randperm(7, 'int32')
# [1, 6, 2, 0, 4, 3, 5]  # random
import paddle

input = paddle.rand((3, 100, 100))
rank = paddle.rank(input)
print(rank)
# 3
import paddle

x = paddle.to_tensor(
    [[1 + 6j, 2 + 5j, 3 + 4j], [4 + 3j, 5 + 2j, 6 + 1j]])
# Tensor(shape=[2, 3], dtype=complex64, place=CUDAPlace(0), stop_gradient=True,
#        [[(1+6j), (2+5j), (3+4j)],
#         [(4+3j), (5+2j), (6+1j)]])

real_res = paddle.real(x)
# Tensor(shape=[2, 3], dtype=float32, place=CUDAPlace(0), stop_gradient=True,
#        [[1., 2., 3.],
#         [4., 5., 6.]])

real_t = x.real()
# Tensor(shape=[2, 3], dtype=float32, place=CUDAPlace(0), stop_gradient=True,
#        [[1., 2., 3.],
#         [4., 5., 6.]])
import paddle

x = paddle.to_tensor([-0.4, -0.2, 0.1, 0.3])
out = paddle.reciprocal(x)
print(out)
# [-2.5        -5.         10.          3.33333333]
import numpy as np
import paddle

x = paddle.rand([2, 4, 6], dtype="float32")
positive_four = paddle.full([1], 4, "int32")

out = paddle.reshape(x, [-1, 0, 3, 2])
print(out)
# the shape is [2,4,3,2].

out = paddle.reshape(x, shape=[positive_four, 12])
print(out)
# the shape of out_2 is [4, 12].

shape_tensor = paddle.to_tensor(np.array([8, 6]).astype("int32"))
out = paddle.reshape(x, shape=shape_tensor)
print(out)
# the shape is [8, 6].
# out shares data with x in dygraph mode
x[0, 0, 0] = 10.
print(out[0, 0])
# the value is [10.]
import paddle

x = paddle.to_tensor([[1.0, 2.0, 3.0],
                      [4.0, 5.0, 6.0],
                      [7.0, 8.0, 9.0]])
out_z1 = paddle.roll(x, shifts=1)
print(out_z1)
#[[9. 1. 2.]
# [3. 4. 5.]
# [6. 7. 8.]]
out_z2 = paddle.roll(x, shifts=1, axis=0)
print(out_z2)
#[[7. 8. 9.]
# [1. 2. 3.]
# [4. 5. 6.]]
import paddle

x = paddle.to_tensor([-0.5, -0.2, 0.6, 1.5])
out = paddle.round(x)
print(out)
# [-1. -0.  1.  2.]
import paddle

x = paddle.to_tensor([0.1, 0.2, 0.3, 0.4])
out = paddle.rsqrt(x)
print(out)
# [3.16227766 2.23606798 1.82574186 1.58113883]
# example 1: dynamic graph
import paddle
emb = paddle.nn.Embedding(10, 10)
layer_state_dict = emb.state_dict()

# save state_dict of emb
paddle.save(layer_state_dict, "emb.pdparams")

scheduler = paddle.optimizer.lr.NoamDecay(
    d_model=0.01, warmup_steps=100, verbose=True)
adam = paddle.optimizer.Adam(
    learning_rate=scheduler,
    parameters=emb.parameters())
opt_state_dict = adam.state_dict()

# save state_dict of optimizer
paddle.save(opt_state_dict, "adam.pdopt")
# save weight of emb
paddle.save(emb.weight, "emb.weight.pdtensor")

# example 2: Save multiple state_dict at the same time
from paddle import nn
from paddle.optimizer import Adam

layer = paddle.nn.Linear(3, 4)
adam = Adam(learning_rate=0.001, parameters=layer.parameters())
obj = {'model': layer.state_dict(), 'opt': adam.state_dict(), 'epoch': 100}
path = 'example/model.pdparams'
paddle.save(obj, path)


# example 3: static graph
import paddle
import paddle.static as static

paddle.enable_static()

# create network
x = paddle.static.data(name="x", shape=[None, 224], dtype='float32')
z = paddle.static.nn.fc(x, 10)

place = paddle.CPUPlace()
exe = paddle.static.Executor(place)
exe.run(paddle.static.default_startup_program())
prog = paddle.static.default_main_program()
for var in prog.list_vars():
    if list(var.shape) == [224, 10]:
        tensor = var.get_value()
        break

# save/load tensor
path_tensor = 'temp/tensor.pdtensor'
paddle.save(tensor, path_tensor)

# save/load state_dict
path_state_dict = 'temp/model.pdparams'
paddle.save(prog.state_dict("param"), path_tensor)

# example 4: save program
import paddle

paddle.enable_static()

data = paddle.static.data(
    name='x_static_save', shape=(None, 224), dtype='float32')
y_static = z = paddle.static.nn.fc(data, 10)
main_program = paddle.static.default_main_program()
path = "example/main_program.pdmodel"
paddle.save(main_program, path)


# example 5: save object to memory
from io import BytesIO
import paddle
from paddle.nn import Linear
paddle.disable_static()

linear = Linear(5, 10)
state_dict = linear.state_dict()
byio = BytesIO()
paddle.save(state_dict, byio)
tensor = paddle.randn([2, 3], dtype='float32')
paddle.save(tensor, byio)
# scale as a float32 number
import paddle

data = paddle.randn(shape=[2,3], dtype='float32')
res = paddle.scale(data, scale=2.0, bias=1.0)
# scale with parameter scale as a Tensor
import paddle

data = paddle.randn(shape=[2, 3], dtype='float32')
factor = paddle.to_tensor([2], dtype='float32')
res = paddle.scale(data, scale=factor, bias=1.0)
import numpy as np
#input:
x = np.array([[1, 1], [2, 2], [3, 3]])
index = np.array([2, 1, 0, 1])
# shape of updates should be the same as x
# shape of updates with dim > 1 should be the same as input
updates = np.array([[1, 1], [2, 2], [3, 3], [4, 4]])
overwrite = False
# calculation:
if not overwrite:
    for i in range(len(index)):
        x[index[i]] = np.zeros((2))
for i in range(len(index)):
    if (overwrite):
        x[index[i]] = updates[i]
    else:
        x[index[i]] += updates[i]
# output:
out = np.array([[3, 3], [6, 6], [1, 1]])
out.shape # [3, 2]
import paddle

x = paddle.to_tensor([[1, 1], [2, 2], [3, 3]], dtype='float32')
index = paddle.to_tensor([2, 1, 0, 1], dtype='int64')
updates = paddle.to_tensor([[1, 1], [2, 2], [3, 3], [4, 4]], dtype='float32')

output1 = paddle.scatter(x, index, updates, overwrite=False)
# [[3., 3.],
#  [6., 6.],
#  [1., 1.]]

output2 = paddle.scatter(x, index, updates, overwrite=True)
# CPU device:
# [[3., 3.],
#  [4., 4.],
#  [1., 1.]]
# GPU device maybe have two results because of the repeated numbers in index
# result 1:
# [[3., 3.],
#  [4., 4.],
#  [1., 1.]]
# result 2:
# [[3., 3.],
#  [2., 2.],
#  [1., 1.]]
import paddle
import numpy as np

index_data = np.array([[1, 1],
                       [0, 1],
                       [1, 3]]).astype(np.int64)
index = paddle.to_tensor(index_data)
updates = paddle.rand(shape=[3, 9, 10], dtype='float32')
shape = [3, 5, 9, 10]

output = paddle.scatter_nd(index, updates, shape)
import paddle
import numpy as np

x = paddle.rand(shape=[3, 5, 9, 10], dtype='float32')
updates = paddle.rand(shape=[3, 9, 10], dtype='float32')
index_data = np.array([[1, 1],
                       [0, 1],
                       [1, 3]]).astype(np.int64)
index = paddle.to_tensor(index_data)
output = paddle.scatter_nd_add(x, index, updates)
import paddle

sorted_sequence = paddle.to_tensor([[1, 3, 5, 7, 9, 11],
                                    [2, 4, 6, 8, 10, 12]], dtype='int32')
values = paddle.to_tensor([[3, 6, 9, 10], [3, 6, 9, 10]], dtype='int32')
out1 = paddle.searchsorted(sorted_sequence, values)
print(out1)
# Tensor(shape=[2, 4], dtype=int64, place=CUDAPlace(0), stop_gradient=True,
#        [[1, 3, 4, 5],
#         [1, 2, 4, 4]])
out2 = paddle.searchsorted(sorted_sequence, values, right=True)
print(out2)
# Tensor(shape=[2, 4], dtype=int64, place=CUDAPlace(0), stop_gradient=True,
#        [[2, 3, 5, 5],
#         [1, 3, 4, 5]])
sorted_sequence_1d = paddle.to_tensor([1, 3, 5, 7, 9, 11, 13])
out3 = paddle.searchsorted(sorted_sequence_1d, values)
print(out3)
# Tensor(shape=[2, 4], dtype=int64, place=CUDAPlace(0), stop_gradient=True,
#        [[1, 3, 4, 5],
#         [1, 3, 4, 5]])
import paddle
gen = paddle.seed(102)
import paddle
sts = paddle.get_cuda_rng_state()
paddle.set_cuda_rng_state(sts)
import paddle
paddle.set_default_dtype("float32")
import paddle
paddle.set_flags({'FLAGS_eager_delete_tensor_gb': 1.0})
import paddle
x = paddle.ones([3, 2])
x.stop_gradient = False
with paddle.set_grad_enabled(False):
    y = x * 2
    with paddle.set_grad_enabled(True):
        z = x * 2
print(y.stop_gradient)   # True
print(z.stop_gradient)   # False
import paddle

paddle.seed(10)
a = paddle.rand([10, 20])
paddle.set_printoptions(4, 100, 3)
print(a)

'''
Tensor(shape=[10, 20], dtype=float32, place=CUDAPlace(0), stop_gradient=True,
       [[0.0002, 0.8503, 0.0135, ..., 0.9508, 0.2621, 0.6661],
        [0.9710, 0.2605, 0.9950, ..., 0.4427, 0.9241, 0.9363],
        [0.0948, 0.3226, 0.9955, ..., 0.1198, 0.0889, 0.9231],
        ...,
        [0.7206, 0.0941, 0.5292, ..., 0.4856, 0.1379, 0.0351],
        [0.1745, 0.5621, 0.3602, ..., 0.2998, 0.4011, 0.1764],
        [0.0728, 0.7786, 0.0314, ..., 0.2583, 0.1654, 0.0637]])
'''
import paddle.fluid as fluid
import numpy as np
import paddle
paddle.enable_static()

inputs = fluid.data(name="x", shape=[3, 100, 100], dtype="float32")
output = fluid.layers.shape(inputs)

exe = fluid.Executor(fluid.CPUPlace())
exe.run(fluid.default_startup_program())

img = np.ones((3, 100, 100)).astype(np.float32)

res = exe.run(fluid.default_main_program(), feed={'x':img}, fetch_list=[output])
print(res) # [array([  3, 100, 100], dtype=int32)]
shard_size = (index_num + nshards - 1) // nshards
v = v - shard_id * shard_size if shard_id * shard_size <= v < (shard_id+1) * shard_size else ignore_value
import paddle
label = paddle.to_tensor([[16], [1]], "int64")
shard_label = paddle.shard_index(input=label,
                                 index_num=20,
                                 nshards=2,
                                 shard_id=0)
print(shard_label)
# [[-1], [1]]
import paddle

x = paddle.to_tensor([3.0, 0.0, -2.0, 1.7], dtype='float32')
out = paddle.sign(x=x)
print(out)  # [1.0, 0.0, -1.0, 1.0]
import paddle

x = paddle.to_tensor([-0.4, -0.2, 0.1, 0.3])
out = paddle.sin(x)
print(out)
# [-0.38941834 -0.19866933  0.09983342  0.29552021]
import paddle

x = paddle.to_tensor([-0.4, -0.2, 0.1, 0.3])
out = paddle.sinh(x)
print(out)
# [-0.41075233 -0.201336    0.10016675  0.30452029]
import paddle

input = paddle.rand(shape=[4, 5, 6], dtype='float32')
# example 1:
# attr starts is a list which doesn't contain tensor.
axes = [0, 1, 2]
starts = [-3, 0, 2]
ends = [3, 2, 4]
sliced_1 = paddle.slice(input, axes=axes, starts=starts, ends=ends)
# sliced_1 is input[0:3, 0:2, 2:4].

# example 2:
# attr starts is a list which contain tensor.
minus_3 = paddle.full([1], -3, "int32")
sliced_2 = paddle.slice(input, axes=axes, starts=[minus_3, 0, 2], ends=ends)
# sliced_2 is input[0:3, 0:2, 2:4].
import paddle

x = paddle.to_tensor([[[5,8,9,5],
                       [0,0,1,7],
                       [6,9,2,4]],
                      [[5,2,4,2],
                       [4,7,7,9],
                       [1,7,0,6]]],
                     dtype='float32')
out1 = paddle.sort(x=x, axis=-1)
out2 = paddle.sort(x=x, axis=0)
out3 = paddle.sort(x=x, axis=1)
print(out1)
#[[[5. 5. 8. 9.]
#  [0. 0. 1. 7.]
#  [2. 4. 6. 9.]]
# [[2. 2. 4. 5.]
#  [4. 7. 7. 9.]
#  [0. 1. 6. 7.]]]
print(out2)
#[[[5. 2. 4. 2.]
#  [0. 0. 1. 7.]
#  [1. 7. 0. 4.]]
# [[5. 8. 9. 5.]
#  [4. 7. 7. 9.]
#  [6. 9. 2. 6.]]]
print(out3)
#[[[0. 0. 1. 4.]
#  [5. 8. 2. 5.]
#  [6. 9. 9. 7.]]
# [[1. 2. 0. 2.]
#  [4. 7. 4. 6.]
#  [5. 7. 7. 9.]]]
import paddle

# x is a Tensor of shape [3, 9, 5]
x = paddle.rand([3, 9, 5])

out0, out1, out2 = paddle.split(x, num_or_sections=3, axis=1)
print(out0.shape)  # [3, 3, 5]
print(out1.shape)  # [3, 3, 5]
print(out2.shape)  # [3, 3, 5]

out0, out1, out2 = paddle.split(x, num_or_sections=[2, 3, 4], axis=1)
print(out0.shape)  # [3, 2, 5]
print(out1.shape)  # [3, 3, 5]
print(out2.shape)  # [3, 4, 5]

out0, out1, out2 = paddle.split(x, num_or_sections=[2, 3, -1], axis=1)
print(out0.shape)  # [3, 2, 5]
print(out1.shape)  # [3, 3, 5]
print(out2.shape)  # [3, 4, 5]

# axis is negative, the real axis is (rank(x) + axis)=1
out0, out1, out2 = paddle.split(x, num_or_sections=3, axis=-2)
print(out0.shape)  # [3, 3, 5]
print(out1.shape)  # [3, 3, 5]
print(out2.shape)  # [3, 3, 5]
import paddle

x = paddle.to_tensor([0.1, 0.2, 0.3, 0.4])
out = paddle.sqrt(x)
print(out)
# [0.31622777 0.4472136  0.54772256 0.63245553]
import paddle

x = paddle.to_tensor([-0.4, -0.2, 0.1, 0.3])
out = paddle.square(x)
print(out)
# [0.16 0.04 0.01 0.09]
import paddle

x = paddle.rand([5, 1, 10])
output = paddle.squeeze(x, axis=1)

print(x.shape)  # [5, 1, 10]
print(output.shape)  # [5, 10]

# output shares data with x in dygraph mode
x[0, 0, 0] = 10.
print(output[0, 0]) # [10.]
import paddle

x1 = paddle.to_tensor([[1.0, 2.0]])
x2 = paddle.to_tensor([[3.0, 4.0]])
x3 = paddle.to_tensor([[5.0, 6.0]])
out = paddle.stack([x1, x2, x3], axis=0)
print(out.shape)  # [3, 1, 2]
print(out)
# [[[1., 2.]],
#  [[3., 4.]],
#  [[5., 6.]]]
import paddle

# example 1: attr shape is a list which doesn't contain Tensor.
out1 = paddle.standard_normal(shape=[2, 3])
# [[-2.923464  ,  0.11934398, -0.51249987],  # random
#  [ 0.39632758,  0.08177969,  0.2692008 ]]  # random

# example 2: attr shape is a list which contains Tensor.
dim1 = paddle.to_tensor([2], 'int64')
dim2 = paddle.to_tensor([3], 'int32')
out2 = paddle.standard_normal(shape=[dim1, dim2, 2])
# [[[-2.8852394 , -0.25898588],  # random
#   [-0.47420555,  0.17683524],  # random
#   [-0.7989969 ,  0.00754541]],  # random
#  [[ 0.85201347,  0.32320443],  # random
#   [ 1.1399018 ,  0.48336947],  # random
#   [ 0.8086993 ,  0.6868893 ]]]  # random

# example 3: attr shape is a Tensor, the data type must be int64 or int32.
shape_tensor = paddle.to_tensor([2, 3])
out3 = paddle.standard_normal(shape_tensor)
# [[-2.878077 ,  0.17099959,  0.05111201]  # random
#  [-0.3761474, -1.044801  ,  1.1870178 ]]  # random
import paddle

x = paddle.to_tensor([1.0, 2.0, 3.0, 4.0])
out = paddle.stanh(x, scale_a=0.67, scale_b=1.72) # [1.00616539, 1.49927628, 1.65933108, 1.70390463]
import paddle

x = paddle.to_tensor([[1.0, 2.0, 3.0], [1.0, 4.0, 5.0]])
out1 = paddle.std(x)
# [1.63299316]
out2 = paddle.std(x, axis=1)
# [1.       2.081666]
import paddle
x = paddle.zeros(shape=[3,4,5,6], dtype="float32")
# example 1:
# attr starts is a list which doesn't contain Tensor.
axes = [1, 2, 3]
starts = [-3, 0, 2]
ends = [3, 2, 4]
strides_1 = [1, 1, 1]
strides_2 = [1, 1, 2]
sliced_1 = paddle.strided_slice(x, axes=axes, starts=starts, ends=ends, strides=strides_1)
# sliced_1 is x[:, 1:3:1, 0:2:1, 2:4:1].
# example 2:
# attr starts is a list which contain tensor Tensor.
minus_3 = paddle.full(shape=[1], fill_value=-3, dtype='int32')
sliced_2 = paddle.strided_slice(x, axes=axes, starts=[minus_3, 0, 2], ends=ends, strides=strides_2)
# sliced_2 is x[:, 1:3:1, 0:2:1, 2:4:2].
import numpy as np
import paddle

x = paddle.to_tensor([[1, 2], [7, 8]])
y = paddle.to_tensor([[5, 6], [3, 4]])
res = paddle.subtract(x, y)
print(res)
#       [[-4, -4],
#        [4, 4]]

x = paddle.to_tensor([[[1, 2, 3], [1, 2, 3]]])
y = paddle.to_tensor([1, 0, 4])
res = paddle.subtract(x, y)
print(res)
#       [[[ 0,  2, -1],
#         [ 0,  2, -1]]]

x = paddle.to_tensor([2, np.nan, 5], dtype='float32')
y = paddle.to_tensor([1, 4, np.nan], dtype='float32')
res = paddle.subtract(x, y)
print(res)
#       [ 1., nan, nan]

x = paddle.to_tensor([5, np.inf, -np.inf], dtype='float64')
y = paddle.to_tensor([1, 4, 5], dtype='float64')
res = paddle.subtract(x, y)
print(res)
#       [   4.,  inf., -inf.]
import paddle

# x is a Tensor with following elements:
#    [[0.2, 0.3, 0.5, 0.9]
#     [0.1, 0.2, 0.6, 0.7]]
# Each example is followed by the corresponding output tensor.
x = paddle.to_tensor([[0.2, 0.3, 0.5, 0.9],
                      [0.1, 0.2, 0.6, 0.7]])
out1 = paddle.sum(x)  # [3.5]
out2 = paddle.sum(x, axis=0)  # [0.3, 0.5, 1.1, 1.6]
out3 = paddle.sum(x, axis=-1)  # [1.9, 1.6]
out4 = paddle.sum(x, axis=1, keepdim=True)  # [[1.9], [1.6]]

# y is a Tensor with shape [2, 2, 2] and elements as below:
#      [[[1, 2], [3, 4]],
#      [[5, 6], [7, 8]]]
# Each example is followed by the corresponding output tensor.
y = paddle.to_tensor([[[1, 2], [3, 4]],
                      [[5, 6], [7, 8]]])
out5 = paddle.sum(y, axis=[1, 2]) # [10, 26]
out6 = paddle.sum(y, axis=[0, 1]) # [16, 20]

# x is a Tensor with following elements:
#    [[True, True, True, True]
#     [False, False, False, False]]
# Each example is followed by the corresponding output tensor.
x = paddle.to_tensor([[True, True, True, True],
                      [False, False, False, False]])
out7 = paddle.sum(x)  # [4]
out8 = paddle.sum(x, axis=0)  # [1, 1, 1, 1]
out9 = paddle.sum(x, axis=1)  # [4, 0]
import paddle
import paddle.nn as nn

class LeNet(nn.Layer):
    def __init__(self, num_classes=10):
        super(LeNet, self).__init__()
        self.num_classes = num_classes
        self.features = nn.Sequential(
            nn.Conv2D(
                1, 6, 3, stride=1, padding=1),
            nn.ReLU(),
            nn.MaxPool2D(2, 2),
            nn.Conv2D(
                6, 16, 5, stride=1, padding=0),
            nn.ReLU(),
            nn.MaxPool2D(2, 2))

        if num_classes > 0:
            self.fc = nn.Sequential(
                nn.Linear(400, 120),
                nn.Linear(120, 84),
                nn.Linear(
                    84, 10))

    def forward(self, inputs):
        x = self.features(inputs)

        if self.num_classes > 0:
            x = paddle.flatten(x, 1)
            x = self.fc(x)
        return x

lenet = LeNet()

params_info = paddle.summary(lenet, (1, 1, 28, 28))
print(params_info)

# multi input demo
class LeNetMultiInput(LeNet):

    def forward(self, inputs, y):
        x = self.features(inputs)

        if self.num_classes > 0:
            x = paddle.flatten(x, 1)
            x = self.fc(x + y)
        return x

lenet_multi_input = LeNetMultiInput()

params_info = paddle.summary(lenet_multi_input, [(1, 1, 28, 28), (1, 400)],
                            dtypes=['float32', 'float32'])
print(params_info)

# list input demo
class LeNetListInput(LeNet):

    def forward(self, inputs):
        x = self.features(inputs[0])

        if self.num_classes > 0:
            x = paddle.flatten(x, 1)
            x = self.fc(x + inputs[1])
        return x

lenet_list_input = LeNetListInput()
input_data = [paddle.rand([1, 1, 28, 28]), paddle.rand([1, 400])]
params_info = paddle.summary(lenet_list_input, input=input_data)
print(params_info)

# dict input demo
class LeNetDictInput(LeNet):

    def forward(self, inputs):
        x = self.features(inputs['x1'])

        if self.num_classes > 0:
            x = paddle.flatten(x, 1)
            x = self.fc(x + inputs['x2'])
        return x

lenet_dict_input = LeNetDictInput()
input_data = {'x1': paddle.rand([1, 1, 28, 28]),
              'x2': paddle.rand([1, 400])}
params_info = paddle.summary(lenet_dict_input, input=input_data)
print(params_info)
import paddle
x = paddle.ones(shape=[2, 3], dtype='int32')
x_transposed = paddle.t(x)
print(x_transposed.shape)
# [3, 2]
import paddle

x = paddle.to_tensor([-0.4, -0.2, 0.1, 0.3])
out = paddle.tan(x)
print(out)
# [-0.42279324, -0.20271005, 0.10033467, 0.30933627]
import paddle

x = paddle.to_tensor([-0.4, -0.2, 0.1, 0.3])
out = paddle.tanh(x)
print(out)
# [-0.37994896 -0.19737532  0.09966799  0.29131261]
import paddle

x = paddle.to_tensor([-0.4, -0.2, 0.1, 0.3])
out = paddle.abs(x)
print(out)
# [0.4 0.2 0.1 0.3]
import paddle

x = paddle.to_tensor([-0.4, -0.2, 0.1, 0.3])
out = paddle.acos(x)
print(out)
# [1.98231317 1.77215425 1.47062891 1.26610367]
import paddle
x = paddle.to_tensor([2, 3, 4], 'float64')
y = paddle.to_tensor([1, 5, 2], 'float64')
z = paddle.add(x, y)
print(z)  # [3., 8., 6. ]
import paddle

input0 = paddle.to_tensor([[1, 2, 3], [4, 5, 6]], dtype='float32')
input1 = paddle.to_tensor([[7, 8, 9], [10, 11, 12]], dtype='float32')
output = paddle.add_n([input0, input1])
# [[8., 10., 12.],
#  [14., 16., 18.]]
import paddle

x = paddle.ones([2,2])
y = paddle.ones([2,2])
input = paddle.ones([2,2])

out = paddle.addmm( input=input, x=x, y=y, beta=0.5, alpha=5.0 )

print(out)
# [[10.5 10.5]
# [10.5 10.5]]
import paddle
import numpy as np

# x is a bool Tensor with following elements:
#    [[True, False]
#     [True, True]]
x = paddle.assign(np.array([[1, 0], [1, 1]], dtype='int32'))
print(x)
x = paddle.cast(x, 'bool')

# out1 should be [False]
out1 = paddle.all(x)  # [False]
print(out1)

# out2 should be [True, False]
out2 = paddle.all(x, axis=0)  # [True, False]
print(out2)

# keep_dim=False, out3 should be [False, True], out.shape should be (2,)
out3 = paddle.all(x, axis=-1)  # [False, True]
print(out3)

# keep_dim=True, out4 should be [[False], [True]], out.shape should be (2,1)
out4 = paddle.all(x, axis=1, keepdim=True)
out4 = paddle.cast(out4, 'int32')  # [[False], [True]]
print(out4)
import paddle

x = paddle.to_tensor([10000., 1e-07])
y = paddle.to_tensor([10000.1, 1e-08])
result1 = paddle.allclose(x, y, rtol=1e-05, atol=1e-08,
                        equal_nan=False, name="ignore_nan")
np_result1 = result1.numpy()
# [False]
result2 = paddle.allclose(x, y, rtol=1e-05, atol=1e-08,
                            equal_nan=True, name="equal_nan")
np_result2 = result2.numpy()
# [False]

x = paddle.to_tensor([1.0, float('nan')])
y = paddle.to_tensor([1.0, float('nan')])
result1 = paddle.allclose(x, y, rtol=1e-05, atol=1e-08,
                        equal_nan=False, name="ignore_nan")
np_result1 = result1.numpy()
# [False]
result2 = paddle.allclose(x, y, rtol=1e-05, atol=1e-08,
                            equal_nan=True, name="equal_nan")
np_result2 = result2.numpy()
# [True]
import paddle
import numpy as np

# x is a bool Tensor with following elements:
#    [[True, False]
#     [False, False]]
x = paddle.assign(np.array([[1, 0], [1, 1]], dtype='int32'))
print(x)
x = paddle.cast(x, 'bool')

# out1 should be [True]
out1 = paddle.any(x)  # [True]
print(out1)

# out2 should be [True, True]
out2 = paddle.any(x, axis=0)  # [True, True]
print(out2)

# keep_dim=False, out3 should be [True, True], out.shape should be (2,)
out3 = paddle.any(x, axis=-1)  # [True, True]
print(out3)

# keep_dim=True, result should be [[True], [True]], out.shape should be (2,1)
out4 = paddle.any(x, axis=1, keepdim=True)
out4 = paddle.cast(out4, 'int32')  # [[True], [True]]
print(out4)
import paddle

x =  paddle.to_tensor([[5,8,9,5],
                         [0,0,1,7],
                         [6,9,2,4]])
out1 = paddle.argmax(x)
print(out1) # 2
out2 = paddle.argmax(x, axis=1)
print(out2)
# [2 3 1]
out3 = paddle.argmax(x, axis=-1)
print(out3)
# [2 3 1]
import paddle

x =  paddle.to_tensor([[5,8,9,5],
                         [0,0,1,7],
                         [6,9,2,4]])
out1 = paddle.argmin(x)
print(out1) # 4
out2 = paddle.argmin(x, axis=1)
print(out2)
# [0 0 2]
out3 = paddle.argmin(x, axis=-1)
print(out3)
# [0 0 2]
import paddle

x = paddle.to_tensor([[[5,8,9,5],
                       [0,0,1,7],
                       [6,9,2,4]],
                      [[5,2,4,2],
                       [4,7,7,9],
                       [1,7,0,6]]],
                    dtype='float32')
out1 = paddle.argsort(x=x, axis=-1)
out2 = paddle.argsort(x=x, axis=0)
out3 = paddle.argsort(x=x, axis=1)
print(out1)
#[[[0 3 1 2]
#  [0 1 2 3]
#  [2 3 0 1]]
# [[1 3 2 0]
#  [0 1 2 3]
#  [2 0 3 1]]]
print(out2)
#[[[0 1 1 1]
#  [0 0 0 0]
#  [1 1 1 0]]
# [[1 0 0 0]
#  [1 1 1 1]
#  [0 0 0 1]]]
print(out3)
#[[[1 1 1 2]
#  [0 0 2 0]
#  [2 2 0 1]]
# [[2 0 2 0]
#  [1 1 0 2]
#  [0 2 1 1]]]
import paddle

x = paddle.to_tensor([-0.4, -0.2, 0.1, 0.3])
out = paddle.asin(x)
print(out)
# [-0.41151685 -0.20135792  0.10016742  0.30469265]
import paddle
import numpy as np

original_tensor = paddle.ones([2, 2])
print("original tensor's dtype is: {}".format(original_tensor.dtype))
new_tensor = original_tensor.astype('float32')
print("new tensor's dtype is: {}".format(new_tensor.dtype))
import paddle

x = paddle.to_tensor([-0.4, -0.2, 0.1, 0.3])
out = paddle.atan(x)
print(out)
# [-0.38050638 -0.19739556  0.09966865  0.29145679]
import paddle
x = paddle.to_tensor(5., stop_gradient=False)
for i in range(5):
    y = paddle.pow(x, 4.0)
    y.backward()
    print("{}: {}".format(i, x.grad))
# 0: [500.]
# 1: [1000.]
# 2: [1500.]
# 3: [2000.]
# 4: [2500.]

x.clear_grad()
print("{}".format(x.grad))
# 0.

grad_tensor=paddle.to_tensor(2.)
for i in range(5):
    y = paddle.pow(x, 4.0)
    y.backward(grad_tensor)
    print("{}: {}".format(i, x.grad))
# 0: [1000.]
# 1: [2000.]
# 2: [3000.]
# 3: [4000.]
# 4: [5000.]
import paddle

x = paddle.to_tensor([1, 2, 1, 4, 5])
result1 = paddle.bincount(x)
print(result1) # [0, 2, 1, 0, 1, 1]

w = paddle.to_tensor([2.1, 0.4, 0.1, 0.5, 0.5])
result2 = paddle.bincount(x, weights=w)
print(result2) # [0., 2.19999981, 0.40000001, 0., 0.50000000, 0.50000000]
import paddle
x = paddle.to_tensor([-5, -1, 1])
y = paddle.to_tensor([4,  2, -3])
res = paddle.bitwise_and(x, y)
print(res)  # [0, 2, 1]
import paddle
x = paddle.to_tensor([-5, -1, 1])
res = paddle.bitwise_not(x)
print(res) # [4, 0, -2]
import paddle
x = paddle.to_tensor([-5, -1, 1])
y = paddle.to_tensor([4,  2, -3])
res = paddle.bitwise_or(x, y)
print(res)  # [-1, -1, -3]
import paddle
x = paddle.to_tensor([-5, -1, 1])
y = paddle.to_tensor([4,  2, -3])
res = paddle.bitwise_xor(x, y)
print(res) # [-1, -3, -4]
import paddle

# In imperative mode:
# size x: (2, 2, 3) and y: (2, 3, 2)
x = paddle.to_tensor([[[1.0, 1.0, 1.0],
                    [2.0, 2.0, 2.0]],
                    [[3.0, 3.0, 3.0],
                    [4.0, 4.0, 4.0]]])
y = paddle.to_tensor([[[1.0, 1.0],[2.0, 2.0],[3.0, 3.0]],
                    [[4.0, 4.0],[5.0, 5.0],[6.0, 6.0]]])
out = paddle.bmm(x, y)
#output size: (2, 2, 2)
#output value:
#[[[6.0, 6.0],[12.0, 12.0]],[[45.0, 45.0],[60.0, 60.0]]]
out_np = out.numpy()
import paddle

shape = paddle.broadcast_shape([2, 1, 3], [1, 3, 1])
# [2, 3, 3]

# shape = paddle.broadcast_shape([2, 1, 3], [3, 3, 1])
# ValueError (terminated with error message).
import paddle
x1 = paddle.rand([1, 2, 3, 4]).astype('float32')
x2 = paddle.rand([1, 2, 1, 4]).astype('float32')
x3 = paddle.rand([1, 1, 3, 1]).astype('float32')
out1, out2, out3 = paddle.broadcast_tensors(input=[x1, x2, x3])
# out1, out2, out3: tensors broadcasted from x1, x2, x3 with shape [1,2,3,4]
import paddle

data = paddle.to_tensor([1, 2, 3], dtype='int32')
out = paddle.broadcast_to(data, shape=[2, 3])
print(out)
# [[1, 2, 3], [1, 2, 3]]
import paddle

x = paddle.to_tensor([2, 3, 4], 'float64')
y = paddle.cast(x, 'uint8')
import paddle

x = paddle.to_tensor([-0.4, -0.2, 0.1, 0.3])
out = paddle.ceil(x)
print(out)
# [-0. -0.  1.  1.]
import paddle
import numpy as np

a = np.random.rand(3, 3)
a_t = np.transpose(a, [1, 0])
x_data = np.matmul(a, a_t) + 1e-03
x = paddle.to_tensor(x_data)
out = paddle.cholesky(x, upper=False)
print(out)
# [[1.190523   0.         0.        ]
#  [0.9906703  0.27676893 0.        ]
#  [1.25450498 0.05600871 0.06400121]]
import numpy as np
import paddle

# x is a Tensor which shape is [3, 9, 5]
x_np = np.random.random([3, 9, 5]).astype("int32")
x = paddle.to_tensor(x_np)

out0, out1, out2 = paddle.chunk(x, chunks=3, axis=1)
# out0.shape [3, 3, 5]
# out1.shape [3, 3, 5]
# out2.shape [3, 3, 5]


# axis is negative, the real axis is (rank(x) + axis) which real
# value is 1.
out0, out1, out2 = paddle.chunk(x, chunks=3, axis=-2)
# out0.shape [3, 3, 5]
# out1.shape [3, 3, 5]
# out2.shape [3, 3, 5]
import paddle
input = paddle.uniform([10, 2])
linear = paddle.nn.Linear(2, 3)
out = linear(input)
out.backward()
print("Before clear_gradient, linear.weight.grad: {}".format(linear.weight.grad))
linear.weight.clear_gradient()
print("After clear_gradient, linear.weight.grad: {}".format(linear.weight.grad))
import paddle

x1 = paddle.to_tensor([[1.2, 3.5], [4.5, 6.4]], 'float32')
out1 = paddle.clip(x1, min=3.5, max=5.0)
out2 = paddle.clip(x1, min=2.5)
print(out1)
# [[3.5, 3.5]
# [4.5, 5.0]]
print(out2)
# [[2.5, 3.5]
# [[4.5, 6.4]
import paddle

x = paddle.to_tensor(1.0, stop_gradient=False)
clone_x = x.clone()
y = clone_x**2
y.backward()
print(clone_x.stop_gradient) # False
print(clone_x.grad)          # [2.0], support gradient propagation
print(x.stop_gradient)       # False
print(x.grad)                # [2.0], clone_x support gradient propagation for x

x = paddle.to_tensor(1.0)
clone_x = x.clone()
clone_x.stop_gradient = False
z = clone_x**3
z.backward()
print(clone_x.stop_gradient) # False
print(clone_x.grad)          # [3.0], support gradient propagation
print(x.stop_gradient) # True
print(x.grad)          # None
import paddle

x1 = paddle.to_tensor([[1, 2, 3],
                       [4, 5, 6]])
x2 = paddle.to_tensor([[11, 12, 13],
                       [14, 15, 16]])
x3 = paddle.to_tensor([[21, 22],
                       [23, 24]])
zero = paddle.full(shape=[1], dtype='int32', fill_value=0)
# When the axis is negative, the real axis is (axis + Rank(x))
# As follow, axis is -1, Rank(x) is 2, the real axis is 1
out1 = paddle.concat(x=[x1, x2, x3], axis=-1)
out2 = paddle.concat(x=[x1, x2], axis=0)
out3 = paddle.concat(x=[x1, x2], axis=zero)
# out1
# [[ 1  2  3 11 12 13 21 22]
#  [ 4  5  6 14 15 16 23 24]]
# out2 out3
# [[ 1  2  3]
#  [ 4  5  6]
#  [11 12 13]
#  [14 15 16]]
import paddle
import numpy as np

x = paddle.to_tensor([[1., 0, -1], [0, 1, 0], [1, 0, 1]])

# compute conditional number when p is None
out = paddle.linalg.cond(x)
# out.numpy() [1.4142135]

# compute conditional number when order of the norm is 'fro'
out_fro = paddle.linalg.cond(x, p='fro')
# out_fro.numpy() [3.1622777]

# compute conditional number when order of the norm is 'nuc'
out_nuc = paddle.linalg.cond(x, p='nuc')
# out_nuc.numpy() [9.2426405]

# compute conditional number when order of the norm is 1
out_1 = paddle.linalg.cond(x, p=1)
# out_1.numpy() [2.]

# compute conditional number when order of the norm is -1
out_minus_1 = paddle.linalg.cond(x, p=-1)
# out_minus_1.numpy() [1.]

# compute conditional number when order of the norm is 2
out_2 = paddle.linalg.cond(x, p=2)
# out_2.numpy() [1.4142135]

# compute conditional number when order of the norm is -1
out_minus_2 = paddle.linalg.cond(x, p=-2)
# out_minus_2.numpy() [0.70710677]

# compute conditional number when order of the norm is inf
out_inf = paddle.linalg.cond(x, p=np.inf)
# out_inf.numpy() [2.]

# compute conditional number when order of the norm is -inf
out_minus_inf = paddle.linalg.cond(x, p=-np.inf)
# out_minus_inf.numpy() [1.]

a = paddle.to_tensor(np.random.randn(2, 4, 4).astype('float32'))
# a.numpy()
# [[[ 0.14063153 -0.996288    0.7996131  -0.02571543]
#   [-0.16303636  1.5534962  -0.49919784 -0.04402903]
#   [-1.1341571  -0.6022629   0.5445269   0.29154757]
#   [-0.16816919 -0.30972657  1.7521842  -0.5402487 ]]
#  [[-0.58081484  0.12402827  0.7229862  -0.55046535]
#   [-0.15178485 -1.1604939   0.75810957  0.30971205]
#   [-0.9669573   1.0940945  -0.27363303 -0.35416734]
#   [-1.216529    2.0018666  -0.7773689  -0.17556527]]]
a_cond_fro = paddle.linalg.cond(a, p='fro')
# a_cond_fro.numpy()  [31.572273 28.120834]

b = paddle.to_tensor(np.random.randn(2, 3, 4).astype('float64'))
# b.numpy()
# [[[ 1.61707487  0.46829144  0.38130416  0.82546736]
#   [-1.72710298  0.08866375 -0.62518804  0.16128892]
#   [-0.02822879 -1.67764516  0.11141444  0.3220113 ]]
#  [[ 0.22524372  0.62474921 -0.85503233 -1.03960523]
#   [-0.76620689  0.56673047  0.85064753 -0.45158196]
#   [ 1.47595418  2.23646462  1.5701758   0.10497519]]]
b_cond_2 = paddle.linalg.cond(b, p=2)
# b_cond_2.numpy()  [3.30064451 2.51976252]
import paddle
data=paddle.to_tensor([[1+1j, 2+2j, 3+3j], [4+4j, 5+5j, 6+6j]])
#Tensor(shape=[2, 3], dtype=complex64, place=CUDAPlace(0), stop_gradient=True,
#       [[(1+1j), (2+2j), (3+3j)],
#        [(4+4j), (5+5j), (6+6j)]])

conj_data=paddle.conj(data)
#Tensor(shape=[2, 3], dtype=complex64, place=CUDAPlace(0), stop_gradient=True,
#       [[(1-1j), (2-2j), (3-3j)],
#        [(4-4j), (5-5j), (6-6j)]])
import paddle

x = paddle.to_tensor([-0.4, -0.2, 0.1, 0.3])
out = paddle.cos(x)
print(out)
# [0.92106099 0.98006658 0.99500417 0.95533649]
import paddle

x = paddle.to_tensor([-0.4, -0.2, 0.1, 0.3])
out = paddle.cosh(x)
print(out)
# [1.08107237 1.02006676 1.00500417 1.04533851]
import paddle
x = paddle.to_tensor(1.0, place=paddle.CUDAPlace(0))
print(x.place)    # CUDAPlace(0)

y = x.cpu()
print(y.place)    # CPUPlace
import paddle

x = paddle.to_tensor([[1.0, 1.0, 1.0],
                      [2.0, 2.0, 2.0],
                      [3.0, 3.0, 3.0]])
y = paddle.to_tensor([[1.0, 1.0, 1.0],
                      [1.0, 1.0, 1.0],
                      [1.0, 1.0, 1.0]])

z1 = paddle.cross(x, y)
# [[-1. -1. -1.]
#  [ 2.  2.  2.]
#  [-1. -1. -1.]]

z2 = paddle.cross(x, y, axis=1)
# [[0. 0. 0.]
#  [0. 0. 0.]
#  [0. 0. 0.]]
# required: gpu
import paddle
x = paddle.to_tensor(1.0, place=paddle.CPUPlace())
print(x.place)        # CPUPlace

y = x.cuda()
print(y.place)        # CUDAPlace(0)

y = x.cuda(None)
print(y.place)        # CUDAPlace(0)

y = x.cuda(1)
print(y.place)        # CUDAPlace(1)
import paddle

data = paddle.arange(12)
data = paddle.reshape(data, (3, 4))
# [[ 0  1  2  3 ]
#  [ 4  5  6  7 ]
#  [ 8  9  10 11]]

y = paddle.cumprod(data, dim=0)
# [[ 0  1   2   3]
#  [ 0  5  12  21]
#  [ 0 45 120 231]]

y = paddle.cumprod(data, dim=-1)
# [[ 0   0   0    0]
#  [ 4  20 120  840]
#  [ 8  72 720 7920]]

y = paddle.cumprod(data, dim=1, dtype='float64')
# [[ 0.   0.   0.    0.]
#  [ 4.  20. 120.  840.]
#  [ 8.  72. 720. 7920.]]

print(y.dtype)
# paddle.float64
import paddle

data = paddle.arange(12)
data = paddle.reshape(data, (3, 4))

y = paddle.cumsum(data)
# [ 0  1  3  6 10 15 21 28 36 45 55 66]

y = paddle.cumsum(data, axis=0)
# [[ 0  1  2  3]
#  [ 4  6  8 10]
#  [12 15 18 21]]

y = paddle.cumsum(data, axis=-1)
# [[ 0  1  3  6]
#  [ 4  9 15 22]
#  [ 8 17 27 38]]

y = paddle.cumsum(data, dtype='float64')
print(y.dtype)
# VarType.FP64
import paddle

x = paddle.to_tensor(1.0, stop_gradient=False)
detach_x = x.detach()
detach_x[:] = 10.0
print(x)  # Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=False,
          #        [10.])
y = x**2
y.backward()
print(x.grad)         # [20.0]
print(detach_x.grad)  # None, 'stop_gradient=True' by default

detach_x.stop_gradient = False # Set stop_gradient to be False, supported auto-grad
z = detach_x**3
z.backward()

print(x.grad)         # [20.0], detach_x is detached from x's graph, not affect each other
print(detach_x.grad)  # [300.0], detach_x has its own graph

# Due to sharing of data with origin Tensor, There are some unsafe operations:
y = 2 * x
detach_x[:] = 5.0
y.backward()
# It will raise Error:
#   one of the variables needed for gradient computation has been modified by an inplace operation.
import paddle

x = paddle.rand([2,2,3],'float32')
print(x)
# Tensor(shape=[2, 2, 3], dtype=float32, place=CUDAPlace(0), stop_gradient=True,
#        [[[0.45661032, 0.03751532, 0.90191704],
#          [0.43760979, 0.86177313, 0.65221709]],

#         [[0.17020577, 0.00259554, 0.28954273],
#          [0.51795638, 0.27325270, 0.18117726]]])

out1 = paddle.diagonal(x)
print(out1)
#Tensor(shape=[3, 2], dtype=float32, place=CUDAPlace(0), stop_gradient=True,
#       [[0.45661032, 0.51795638],
#        [0.03751532, 0.27325270],
#        [0.90191704, 0.18117726]])

out2 = paddle.diagonal(x, offset=0, axis1=2, axis2=1)
print(out2)
#Tensor(shape=[2, 2], dtype=float32, place=CUDAPlace(0), stop_gradient=True,
#       [[0.45661032, 0.86177313],
#        [0.17020577, 0.27325270]])

out3 = paddle.diagonal(x, offset=1, axis1=0, axis2=1)
print(out3)
#Tensor(shape=[3, 1], dtype=float32, place=CUDAPlace(0), stop_gradient=True,
#       [[0.43760979],
#        [0.86177313],
#        [0.65221709]])

out4 = paddle.diagonal(x, offset=0, axis1=1, axis2=2)
print(out4)
#Tensor(shape=[2, 2], dtype=float32, place=CUDAPlace(0), stop_gradient=True,
#       [[0.45661032, 0.86177313],
#        [0.17020577, 0.27325270]])
import paddle

data = paddle.to_tensor([[1, 1.5], [0, -2.2]], dtype='float32')
res = paddle.digamma(data)
print(res)
# Tensor(shape=[2, 2], dtype=float32, place=CUDAPlace(0), stop_gradient=True,
#       [[-0.57721591,  0.03648996],
#        [ nan       ,  5.32286835]])
import paddle
import numpy as np

x = paddle.to_tensor(np.array([[3, 3],[3, 3]]), "float32")
y = paddle.to_tensor(np.array([[3, 3],[3, 1]]), "float32")
out = paddle.dist(x, y, 0)
print(out) # out = [1.]

out = paddle.dist(x, y, 2)
print(out) # out = [2.]

out = paddle.dist(x, y, float("inf"))
print(out) # out = [2.]

out = paddle.dist(x, y, float("-inf"))
print(out) # out = [0.]
import paddle

x = paddle.to_tensor([2, 3, 4], dtype='float64')
y = paddle.to_tensor([1, 5, 2], dtype='float64')
z = paddle.divide(x, y)
print(z)  # [2., 0.6, 2.]
import paddle
import numpy as np

x_data = np.random.uniform(0.1, 1, [10]).astype(np.float32)
y_data = np.random.uniform(1, 3, [10]).astype(np.float32)
x = paddle.to_tensor(x_data)
y = paddle.to_tensor(y_data)
z = paddle.dot(x, y)
print(z)
import paddle
import numpy as np

paddle.device.set_device("cpu")

x_data = np.array([[1.6707249, 7.2249975, 6.5045543],
                   [9.956216,  8.749598,  6.066444 ],
                   [4.4251957, 1.7983172, 0.370647 ]]).astype("float32")
x = paddle.to_tensor(x_data)
w, v = paddle.linalg.eig(x)
print(w)
# Tensor(shape=[3, 3], dtype=complex128, place=CPUPlace, stop_gradient=False,
#       [[(-0.5061363550800655+0j) , (-0.7971760990842826+0j) ,
#         (0.18518077798279986+0j)],
#        [(-0.8308237755993192+0j) ,  (0.3463813401919749+0j) ,
#         (-0.6837005269141947+0j) ],
#        [(-0.23142567697893396+0j),  (0.4944999840400175+0j) ,
#         (0.7058765252952796+0j) ]])

print(v)
# Tensor(shape=[3], dtype=complex128, place=CPUPlace, stop_gradient=False,
#       [ (16.50471283351188+0j)  , (-5.5034820550763515+0j) ,
#         (-0.21026087843552282+0j)])
import paddle

paddle.set_device("cpu")
paddle.seed(1234)

x = paddle.rand(shape=[3, 3], dtype='float64')
# [[0.02773777, 0.93004224, 0.06911496],
#  [0.24831591, 0.45733623, 0.07717843],
#  [0.48016702, 0.14235102, 0.42620817]])

print(paddle.linalg.eigvals(x))
# [(-0.27078833542132674+0j), (0.29962280156230725+0j), (0.8824477020120244+0j)] #complex128
import numpy as np
import paddle

x_data = np.array([[1, -2j], [2j, 5]])
x = paddle.to_tensor(x_data)
out_value = paddle.eigvalsh(x, UPLO='L')
print(out_value)
#[0.17157288, 5.82842712]
import paddle

x = paddle.to_tensor([1, 2, 3])
y = paddle.to_tensor([1, 3, 2])
result1 = paddle.equal(x, y)
print(result1)  # result1 = [True False False]
import paddle

x = paddle.to_tensor([1, 2, 3])
y = paddle.to_tensor([1, 2, 3])
z = paddle.to_tensor([1, 4, 3])
result1 = paddle.equal_all(x, y)
print(result1) # result1 = [True ]
result2 = paddle.equal_all(x, z)
print(result2) # result2 = [False ]
import paddle
x = paddle.to_tensor([-0.4, -0.2, 0.1, 0.3])
out = paddle.erf(x)
print(out)
# [-0.42839236 -0.22270259  0.11246292  0.32862676]
import paddle

x = paddle.to_tensor([-0.4, -0.2, 0.1, 0.3])
out = paddle.exp(x)
print(out)
# [0.67032005 0.81873075 1.10517092 1.34985881]
import paddle

data = paddle.to_tensor([1, 2, 3], dtype='int32')
out = paddle.expand(data, shape=[2, 3])
print(out)
# [[1, 2, 3], [1, 2, 3]]
import paddle

data_x = paddle.to_tensor([1, 2, 3], 'int32')
data_y = paddle.to_tensor([[1, 2, 3], [4, 5, 6]], 'int32')
out = paddle.expand_as(data_x, data_y)
np_out = out.numpy()
# [[1, 2, 3], [1, 2, 3]]
import paddle

tensor = paddle.to_tensor([0, 1, 2, 3, 4])

tensor.fill_(0)
print(tensor.tolist())   #[0, 0, 0, 0, 0]
.. code-block:: python
    import paddle
    x = paddle.ones((4, 3)) * 2
    x.fill_diagonal_(1.0)
    print(x.tolist())   #[[1.0, 2.0, 2.0], [2.0, 1.0, 2.0], [2.0, 2.0, 1.0], [2.0, 2.0, 2.0]]
import paddle

x = paddle.ones((4, 3)) * 2
y = paddle.ones((3,))
nx = x.fill_diagonal_tensor(y)
print(nx.tolist())   #[[1.0, 2.0, 2.0], [2.0, 1.0, 2.0], [2.0, 2.0, 1.0], [2.0, 2.0, 2.0]]
import paddle

x = paddle.ones((4, 3)) * 2
y = paddle.ones((3,))
x.fill_diagonal_tensor_(y)
print(x.tolist())   #[[1.0, 2.0, 2.0], [2.0, 1.0, 2.0], [2.0, 2.0, 1.0], [2.0, 2.0, 2.0]]
import paddle

image_shape=(2, 3, 4, 4)

x = paddle.arange(end=image_shape[0] * image_shape[1] * image_shape[2] * image_shape[3])
img = paddle.reshape(x, image_shape)

out = paddle.flatten(img, start_axis=1, stop_axis=2)
# out shape is [2, 12, 4]

# out shares data with img in dygraph mode
img[0, 0, 0, 0] = -1
print(out[0, 0, 0]) # [-1]
import paddle
import numpy as np

image_shape=(3, 2, 2)
x = np.arange(image_shape[0] * image_shape[1] * image_shape[2]).reshape(image_shape)
x = x.astype('float32')
img = paddle.to_tensor(x)
tmp = paddle.flip(img, [0,1])
print(tmp) # [[[10,11],[8, 9]], [[6, 7],[4, 5]], [[2, 3],[0, 1]]]

out = paddle.flip(tmp,-1)
print(out) # [[[11,10],[9, 8]], [[7, 6],[5, 4]], [[3, 2],[1, 0]]]
import paddle

x = paddle.to_tensor([-0.4, -0.2, 0.1, 0.3])
out = paddle.floor(x)
print(out)
# [-1. -1.  0.  0.]
import paddle

x = paddle.to_tensor([2, 3, 8, 7])
y = paddle.to_tensor([1, 5, 3, 3])
z = paddle.floor_divide(x, y)
print(z)  # [2, 0, 2, 2]
import paddle

x = paddle.to_tensor([2, 3, 8, 7])
y = paddle.to_tensor([1, 5, 3, 3])
z = paddle.remainder(x, y)
print(z)  # [0, 3, 2, 1]
import paddle

input = paddle.to_tensor([[1,2],[3,4],[5,6]])
index = paddle.to_tensor([0,1])
output = paddle.gather(input, index, axis=0)
# expected output: [[1,2],[3,4]]
import paddle

x = paddle.to_tensor([[[1, 2], [3, 4], [5, 6]],
                      [[7, 8], [9, 10], [11, 12]]])
index = paddle.to_tensor([[0, 1]])

output = paddle.gather_nd(x, index) #[[3, 4]]
import paddle

x = paddle.to_tensor(5., stop_gradient=False)
y = paddle.pow(x, 4.0)
y.backward()
print("grad of x: {}".format(x.grad))
# Tensor(shape=[1], dtype=float32, place=CUDAPlace(0), stop_gradient=False, [500.])
import paddle

x = paddle.to_tensor(5., stop_gradient=False)
y = paddle.pow(x, 4.0)
y.backward()
print("grad of x: {}".format(x.gradient()))
# [500.]
import paddle

x = paddle.to_tensor([1, 2, 3])
y = paddle.to_tensor([1, 3, 2])
result1 = paddle.greater_equal(x, y)
print(result1)  # result1 = [True False True]
import paddle

x = paddle.to_tensor([1, 2, 3])
y = paddle.to_tensor([1, 3, 2])
result1 = paddle.greater_than(x, y)
print(result1)  # result1 = [False False True]
import paddle

inputs = paddle.to_tensor([1, 2, 1])
result = paddle.histogram(inputs, bins=4, min=0, max=3)
print(result) # [0, 2, 1, 0]
import paddle

x = paddle.to_tensor(
    [[1 + 6j, 2 + 5j, 3 + 4j], [4 + 3j, 5 + 2j, 6 + 1j]])
# Tensor(shape=[2, 3], dtype=complex64, place=CUDAPlace(0), stop_gradient=True,
#        [[(1+6j), (2+5j), (3+4j)],
#         [(4+3j), (5+2j), (6+1j)]])

imag_res = paddle.imag(x)
# Tensor(shape=[2, 3], dtype=float32, place=CUDAPlace(0), stop_gradient=True,
#        [[6., 5., 4.],
#         [3., 2., 1.]])

imag_t = x.imag()
# Tensor(shape=[2, 3], dtype=float32, place=CUDAPlace(0), stop_gradient=True,
#        [[6., 5., 4.],
#         [3., 2., 1.]])
import paddle

data = paddle.zeros(shape=[1], dtype='float32')
counter = paddle.increment(data)
# [1.]
import paddle

x = paddle.to_tensor([[1.0, 2.0, 3.0, 4.0],
                      [5.0, 6.0, 7.0, 8.0],
                      [9.0, 10.0, 11.0, 12.0]], dtype='float32')
index = paddle.to_tensor([[0, 1, 2],
                          [1, 2, 3],
                          [0, 0, 0]], dtype='int32')
target = paddle.to_tensor([[100, 200, 300, 400],
                           [500, 600, 700, 800],
                           [900, 1000, 1100, 1200]], dtype='int32')
out_z1 = paddle.index_sample(x, index)
print(out_z1)
#[[1. 2. 3.]
# [6. 7. 8.]
# [9. 9. 9.]]

# Use the index of the maximum value by topk op
# get the value of the element of the corresponding index in other tensors
top_value, top_index = paddle.topk(x, k=2)
out_z2 = paddle.index_sample(target, top_index)
print(top_value)
#[[ 4.  3.]
# [ 8.  7.]
# [12. 11.]]

print(top_index)
#[[3 2]
# [3 2]
# [3 2]]

print(out_z2)
#[[ 400  300]
# [ 800  700]
# [1200 1100]]
import paddle

x = paddle.to_tensor([[1.0, 2.0, 3.0, 4.0],
                      [5.0, 6.0, 7.0, 8.0],
                      [9.0, 10.0, 11.0, 12.0]])
index = paddle.to_tensor([0, 1, 1], dtype='int32')
out_z1 = paddle.index_select(x=x, index=index)
#[[1. 2. 3. 4.]
# [5. 6. 7. 8.]
# [5. 6. 7. 8.]]
out_z2 = paddle.index_select(x=x, index=index, axis=1)
#[[ 1.  2.  2.]
# [ 5.  6.  6.]
# [ 9. 10. 10.]]
import paddle
var = paddle.ones(shape=[4, 2, 3], dtype="float32")
print(var.inplace_version)  # 0

var[1] = 2.2
print(var.inplace_version)  # 1
import paddle

mat = paddle.to_tensor([[2, 0], [0, 2]], dtype='float32')
inv = paddle.inverse(mat)
print(inv) # [[0.5, 0], [0, 0.5]]
import paddle

input = paddle.rand(shape=[4, 32, 32], dtype='float32')
res = paddle.is_empty(x=input)
print("res:", res)
# ('res:', Tensor: eager_tmp_1
#    - place: CPUPlace
#    - shape: [1]
#    - layout: NCHW
#    - dtype: bool
#    - data: [0])
import paddle

x = paddle.to_tensor(1.)
print(x.is_leaf) # True

x = paddle.to_tensor(1., stop_gradient=True)
y = x + 1
print(x.is_leaf) # True
print(y.is_leaf) # True

x = paddle.to_tensor(1., stop_gradient=False)
y = x + 1
print(x.is_leaf) # True
print(y.is_leaf) # False
import paddle

input1 = paddle.rand(shape=[2, 3, 5], dtype='float32')
check = paddle.is_tensor(input1)
print(check)  #True

input3 = [1, 4]
check = paddle.is_tensor(input3)
print(check)  #False
import paddle

x = paddle.to_tensor([float('-inf'), -2, 3.6, float('inf'), 0, float('-nan'), float('nan')])
out = paddle.tensor.isfinite(x)
print(out)  # [False  True  True False  True False False]
import paddle
x = paddle.to_tensor([float('-inf'), -2, 3.6, float('inf'), 0, float('-nan'), float('nan')])
out = paddle.tensor.isinf(x)
print(out)  # [ True False False  True False False False]
import paddle
x = paddle.to_tensor([float('-inf'), -2, 3.6, float('inf'), 0, float('-nan'), float('nan')])
out = paddle.tensor.isnan(x)
print(out)  # [False False False False False  True  True]
import paddle

x = paddle.to_tensor(1)
print(x.item())             #1
print(type(x.item()))       #<class 'int'>

x = paddle.to_tensor(1.0)
print(x.item())             #1.0
print(type(x.item()))       #<class 'float'>

x = paddle.to_tensor(True)
print(x.item())             #True
print(type(x.item()))       #<class 'bool'>

x = paddle.to_tensor(1+1j)
print(x.item())             #(1+1j)
print(type(x.item()))       #<class 'complex'>

x = paddle.to_tensor([[1.1, 2.2, 3.3]])
print(x.item(2))            #3.3
print(x.item(0, 2))         #3.3
import paddle
x = paddle.to_tensor([[1, 2], [3, 4]], dtype='int64')
y = paddle.to_tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9]], dtype='int64')
out = paddle.kron(x, y)
print(out)
#        [[1, 2, 3, 2, 4, 6],
#         [ 4,  5,  6,  8, 10, 12],
#         [ 7,  8,  9, 14, 16, 18],
#         [ 3,  6,  9,  4,  8, 12],
#         [12, 15, 18, 16, 20, 24],
#         [21, 24, 27, 28, 32, 36]])
import paddle

x = paddle.to_tensor([1, 2, 3])
y = paddle.to_tensor([1, 3, 2])
result1 = paddle.less_equal(x, y)
print(result1)  # result1 = [True True False]
import paddle

x = paddle.to_tensor([1, 2, 3])
y = paddle.to_tensor([1, 3, 2])
result1 = paddle.less_than(x, y)
print(result1)  # result1 = [False True False]
import paddle

x = paddle.to_tensor([-0.4, -0.2, 0.1, 0.3])
out = paddle.lgamma(x)
print(out)
# [1.31452441, 1.76149750, 2.25271273, 1.09579802]
import paddle

x = [[2,3,4], [7,8,9]]
x = paddle.to_tensor(x, dtype='float32')
res = paddle.log(x)
# [[0.693147, 1.09861, 1.38629], [1.94591, 2.07944, 2.19722]]
import paddle

# example 1: x is a float
x_i = paddle.to_tensor([[1.0], [10.0]])
res = paddle.log10(x_i) # [[0.], [1.0]]

# example 2: x is float32
x_i = paddle.full(shape=[1], fill_value=10, dtype='float32')
paddle.to_tensor(x_i)
res = paddle.log10(x_i)
print(res) # [1.0]

# example 3: x is float64
x_i = paddle.full(shape=[1], fill_value=10, dtype='float64')
paddle.to_tensor(x_i)
res = paddle.log10(x_i)
print(res) # [1.0]
import paddle

data = paddle.to_tensor([[0], [1]], dtype='float32')
res = paddle.log1p(data)
# [[0.], [0.6931472]]
import paddle

# example 1: x is a float
x_i = paddle.to_tensor([[1.0], [2.0]])
res = paddle.log2(x_i) # [[0.], [1.0]]

# example 2: x is float32
x_i = paddle.full(shape=[1], fill_value=2, dtype='float32')
paddle.to_tensor(x_i)
res = paddle.log2(x_i)
print(res) # [1.0]

# example 3: x is float64
x_i = paddle.full(shape=[1], fill_value=2, dtype='float64')
paddle.to_tensor(x_i)
res = paddle.log2(x_i)
print(res) # [1.0]
import paddle

x = paddle.to_tensor([True])
y = paddle.to_tensor([True, False, True, False])
res = paddle.logical_and(x, y)
print(res) # [True False True False]
import paddle

x = paddle.to_tensor([True, False, True, False])
res = paddle.logical_not(x)
print(res) # [False  True False  True]
import paddle
import numpy as np

x_data = np.array([True, False], dtype=np.bool).reshape(2, 1)
y_data = np.array([True, False, True, False], dtype=np.bool).reshape(2, 2)
x = paddle.to_tensor(x_data)
y = paddle.to_tensor(y_data)
res = paddle.logical_or(x, y)
print(res) # [[ True  True] [ True False]]
import paddle
import numpy as np

x_data = np.array([True, False], dtype=np.bool).reshape([2, 1])
y_data = np.array([True, False, True, False], dtype=np.bool).reshape([2, 2])
x = paddle.to_tensor(x_data)
y = paddle.to_tensor(y_data)
res = paddle.logical_xor(x, y)
print(res) # [[False,  True], [ True, False]]
import paddle

x = paddle.to_tensor([[-1.5, 0., 2.], [3., 1.2, -2.4]])
out1 = paddle.logsumexp(x) # [3.4691226]
out2 = paddle.logsumexp(x, 1) # [2.15317821, 3.15684602]
import paddle

x = paddle.to_tensor([[1.0, 2.0, 3.0, 4.0],
                      [5.0, 6.0, 7.0, 8.0],
                      [9.0, 10.0, 11.0, 12.0]])
mask = paddle.to_tensor([[True, False, False, False],
                         [True, True, False, False],
                         [True, False, False, False]])
out = paddle.masked_select(x, mask)
#[1.0 5.0 6.0 9.0]
import paddle
import numpy as np

# vector * vector
x_data = np.random.random([10]).astype(np.float32)
y_data = np.random.random([10]).astype(np.float32)
x = paddle.to_tensor(x_data)
y = paddle.to_tensor(y_data)
z = paddle.matmul(x, y)
print(z.numpy().shape)
# [1]

# matrix * vector
x_data = np.random.random([10, 5]).astype(np.float32)
y_data = np.random.random([5]).astype(np.float32)
x = paddle.to_tensor(x_data)
y = paddle.to_tensor(y_data)
z = paddle.matmul(x, y)
print(z.numpy().shape)
# [10]

# batched matrix * broadcasted vector
x_data = np.random.random([10, 5, 2]).astype(np.float32)
y_data = np.random.random([2]).astype(np.float32)
x = paddle.to_tensor(x_data)
y = paddle.to_tensor(y_data)
z = paddle.matmul(x, y)
print(z.numpy().shape)
# [10, 5]

# batched matrix * batched matrix
x_data = np.random.random([10, 5, 2]).astype(np.float32)
y_data = np.random.random([10, 2, 5]).astype(np.float32)
x = paddle.to_tensor(x_data)
y = paddle.to_tensor(y_data)
z = paddle.matmul(x, y)
print(z.numpy().shape)
# [10, 5, 5]

# batched matrix * broadcasted matrix
x_data = np.random.random([10, 1, 5, 2]).astype(np.float32)
y_data = np.random.random([1, 3, 2, 5]).astype(np.float32)
x = paddle.to_tensor(x_data)
y = paddle.to_tensor(y_data)
z = paddle.matmul(x, y)
print(z.numpy().shape)
# [10, 3, 5, 5]
import paddle

x = paddle.to_tensor([[1, 2, 3],
                      [1, 4, 9],
                      [1, 8, 27]], dtype='float64')
print(paddle.linalg.matrix_power(x, 2))
# [[6.  , 34. , 102.],
#  [14. , 90. , 282.],
#  [36. , 250., 804.]]

print(paddle.linalg.matrix_power(x, 0))
# [[1., 0., 0.],
#  [0., 1., 0.],
#  [0., 0., 1.]]

print(paddle.linalg.matrix_power(x, -2))
# [[ 12.91666667, -12.75000000,  2.83333333 ],
#  [-7.66666667 ,  8.         , -1.83333333 ],
#  [ 1.80555556 , -1.91666667 ,  0.44444444 ]]
import paddle

# data_x is a Tensor with shape [2, 4]
# the axis is a int element

x = paddle.to_tensor([[0.2, 0.3, 0.5, 0.9],
                      [0.1, 0.2, 0.6, 0.7]])
result1 = paddle.max(x)
print(result1)
#[0.9]
result2 = paddle.max(x, axis=0)
print(result2)
#[0.2 0.3 0.6 0.9]
result3 = paddle.max(x, axis=-1)
print(result3)
#[0.9 0.7]
result4 = paddle.max(x, axis=1, keepdim=True)
print(result4)
#[[0.9]
# [0.7]]

# data_y is a Tensor with shape [2, 2, 2]
# the axis is list

y = paddle.to_tensor([[[1.0, 2.0], [3.0, 4.0]],
                      [[5.0, 6.0], [7.0, 8.0]]])
result5 = paddle.max(y, axis=[1, 2])
print(result5)
#[4. 8.]
result6 = paddle.max(y, axis=[0, 1])
print(result6)
#[7. 8.]
import numpy as np
import paddle

x = paddle.to_tensor([[1, 2], [7, 8]])
y = paddle.to_tensor([[3, 4], [5, 6]])
res = paddle.maximum(x, y)
print(res)
#    [[3, 4],
#     [7, 8]]

x = paddle.to_tensor([[1, 2, 3], [1, 2, 3]])
y = paddle.to_tensor([3, 0, 4])
res = paddle.maximum(x, y)
print(res)
#    [[3, 2, 4],
#     [3, 2, 4]]

x = paddle.to_tensor([2, 3, 5], dtype='float32')
y = paddle.to_tensor([1, np.nan, np.nan], dtype='float32')
res = paddle.maximum(x, y)
print(res)
#    [ 2., nan, nan]

x = paddle.to_tensor([5, 3, np.inf], dtype='float32')
y = paddle.to_tensor([1, -np.inf, 5], dtype='float32')
res = paddle.maximum(x, y)
print(res)
#    [  5.,   3., inf.]
import paddle

x = paddle.to_tensor([[[1., 2., 3., 4.],
                       [5., 6., 7., 8.],
                       [9., 10., 11., 12.]],
                      [[13., 14., 15., 16.],
                       [17., 18., 19., 20.],
                       [21., 22., 23., 24.]]])
out1 = paddle.mean(x)
# [12.5]
out2 = paddle.mean(x, axis=-1)
# [[ 2.5  6.5 10.5]
#  [14.5 18.5 22.5]]
out3 = paddle.mean(x, axis=-1, keepdim=True)
# [[[ 2.5]
#   [ 6.5]
#   [10.5]]
#  [[14.5]
#   [18.5]
#   [22.5]]]
out4 = paddle.mean(x, axis=[0, 2])
# [ 8.5 12.5 16.5]
import paddle

x = paddle.arange(12).reshape([3, 4])
# x is [[0 , 1 , 2 , 3 ],
#       [4 , 5 , 6 , 7 ],
#       [8 , 9 , 10, 11]]

y1 = paddle.median(x)
# y1 is [5.5]

y2 = paddle.median(x, axis=0)
# y2 is [4., 5., 6., 7.]

y3 = paddle.median(x, axis=1)
# y3 is [1.5, 5.5, 9.5]

y4 = paddle.median(x, axis=0, keepdim=True)
# y4 is [[4., 5., 6., 7.]]
import paddle

# x is a tensor with shape [2, 4]
# the axis is a int element
x = paddle.to_tensor([[0.2, 0.3, 0.5, 0.9],
                      [0.1, 0.2, 0.6, 0.7]])
result1 = paddle.min(x)
print(result1)
#[0.1]
result2 = paddle.min(x, axis=0)
print(result2)
#[0.1 0.2 0.5 0.7]
result3 = paddle.min(x, axis=-1)
print(result3)
#[0.2 0.1]
result4 = paddle.min(x, axis=1, keepdim=True)
print(result4)
#[[0.2]
# [0.1]]

# y is a Tensor with shape [2, 2, 2]
# the axis is list
y = paddle.to_tensor([[[1.0, 2.0], [3.0, 4.0]],
                      [[5.0, 6.0], [7.0, 8.0]]])
result5 = paddle.min(y, axis=[1, 2])
print(result5)
#[1. 5.]
result6 = paddle.min(y, axis=[0, 1])
print(result6)
#[1. 2.]
import numpy as np
import paddle

x = paddle.to_tensor([[1, 2], [7, 8]])
y = paddle.to_tensor([[3, 4], [5, 6]])
res = paddle.minimum(x, y)
print(res)
#       [[1, 2],
#        [5, 6]]

x = paddle.to_tensor([[[1, 2, 3], [1, 2, 3]]])
y = paddle.to_tensor([3, 0, 4])
res = paddle.minimum(x, y)
print(res)
#       [[[1, 0, 3],
#         [1, 0, 3]]]

x = paddle.to_tensor([2, 3, 5], dtype='float32')
y = paddle.to_tensor([1, np.nan, np.nan], dtype='float32')
res = paddle.minimum(x, y)
print(res)
#       [ 1., nan, nan]

x = paddle.to_tensor([5, 3, np.inf], dtype='float64')
y = paddle.to_tensor([1, -np.inf, 5], dtype='float64')
res = paddle.minimum(x, y)
print(res)
#       [   1., -inf.,    5.]
import paddle
input = paddle.arange(1, 7).reshape((3, 2)).astype('float32')
mat2 = paddle.arange(1, 9).reshape((2, 4)).astype('float32')
out = paddle.mm(input, mat2)
print(out)
#        [[11., 14., 17., 20.],
#         [23., 30., 37., 44.],
#         [35., 46., 57., 68.]])
import paddle

x = paddle.to_tensor([2, 3, 8, 7])
y = paddle.to_tensor([1, 5, 3, 3])
z = paddle.remainder(x, y)
print(z)  # [0, 3, 2, 1]
import paddle
import numpy as np

# A * B
A_data = np.random.random([3, 4]).astype(np.float32)
B_data = np.random.random([4, 5]).astype(np.float32)
A = paddle.to_tensor(A_data)
B = paddle.to_tensor(B_data)
out = paddle.linalg.multi_dot([A, B])
print(out.numpy().shape)
# [3, 5]

# A * B * C
A_data = np.random.random([10, 5]).astype(np.float32)
B_data = np.random.random([5, 8]).astype(np.float32)
C_data = np.random.random([8, 7]).astype(np.float32)
A = paddle.to_tensor(A_data)
B = paddle.to_tensor(B_data)
C = paddle.to_tensor(C_data)
out = paddle.linalg.multi_dot([A, B, C])
print(out.numpy().shape)
# [10, 7]
import paddle
import numpy as np
img1 = np.array([[1, 2], [3, 4]]).astype(np.float32)
img2 = np.array([[5, 6], [7, 8]]).astype(np.float32)
inputs = [paddle.to_tensor(img1), paddle.to_tensor(img2)]
index = paddle.to_tensor(np.array([[1], [0]]).astype(np.int32))
res = paddle.multiplex(inputs, index)
print(res) # [array([[5., 6.], [3., 4.]], dtype=float32)]
import paddle

x = paddle.to_tensor([[1, 2], [3, 4]])
y = paddle.to_tensor([[5, 6], [7, 8]])
res = paddle.multiply(x, y)
print(res) # [[5, 12], [21, 32]]

x = paddle.to_tensor([[[1, 2, 3], [1, 2, 3]]])
y = paddle.to_tensor([2])
res = paddle.multiply(x, y)
print(res) # [[[2, 4, 6], [2, 4, 6]]]
# x: [M, N], vec: [N]
# paddle.mv(x, vec)  # out: [M]

import numpy as np
import paddle

x_data = np.array([[2, 1, 3], [3, 0, 1]]).astype("float64")
x = paddle.to_tensor(x_data)
vec_data = np.array([3, 5, 1])
vec = paddle.to_tensor(vec_data).astype("float64")
out = paddle.mv(x, vec)
import paddle

x = paddle.to_tensor([-0.4, -0.2, 0.1, 0.3])
out = paddle.neg(x)
print(out)
# [0.4 0.2 -0.1 -0.3]
import paddle

x1 = paddle.to_tensor([[1.0, 0.0, 0.0],
                       [0.0, 2.0, 0.0],
                       [0.0, 0.0, 3.0]])
x2 = paddle.to_tensor([0.0, 1.0, 0.0, 3.0])
out_z1 = paddle.nonzero(x1)
print(out_z1)
#[[0 0]
# [1 1]
# [2 2]]
out_z1_tuple = paddle.nonzero(x1, as_tuple=True)
for out in out_z1_tuple:
    print(out)
#[[0]
# [1]
# [2]]
#[[0]
# [1]
# [2]]
out_z2 = paddle.nonzero(x2)
print(out_z2)
#[[1]
# [3]]
out_z2_tuple = paddle.nonzero(x2, as_tuple=True)
for out in out_z2_tuple:
    print(out)
#[[1]
# [3]]
import paddle
import numpy as np
shape=[2, 3, 4]
np_input = np.arange(24).astype('float32') - 12
np_input = np_input.reshape(shape)
x = paddle.to_tensor(np_input)
#[[[-12. -11. -10.  -9.] [ -8.  -7.  -6.  -5.] [ -4.  -3.  -2.  -1.]]
# [[  0.   1.   2.   3.] [  4.   5.   6.   7.] [  8.   9.  10.  11.]]]

# compute frobenius norm along last two dimensions.
out_fro = paddle.norm(x, p='fro', axis=[0,1])
# out_fro.numpy() [17.435596 16.911535 16.7332   16.911535]

# compute 2-order vector norm along last dimension.
out_pnorm = paddle.norm(x, p=2, axis=-1)
#out_pnorm.numpy(): [[21.118711  13.190906   5.477226]
#                    [ 3.7416575 11.224972  19.131126]]

# compute 2-order  norm along [0,1] dimension.
out_pnorm = paddle.norm(x, p=2, axis=[0,1])
#out_pnorm.numpy(): [17.435596 16.911535 16.7332   16.911535]

# compute inf-order  norm
out_pnorm = paddle.norm(x, p=np.inf)
#out_pnorm.numpy()  = [12.]
out_pnorm = paddle.norm(x, p=np.inf, axis=0)
#out_pnorm.numpy(): [[12. 11. 10. 9.] [8. 7. 6. 7.] [8. 9. 10. 11.]]

# compute -inf-order  norm
out_pnorm = paddle.norm(x, p=-np.inf)
#out_pnorm.numpy(): [0.]
out_pnorm = paddle.norm(x, p=-np.inf, axis=0)
#out_pnorm.numpy(): [[0. 1. 2. 3.] [4. 5. 6. 5.] [4. 3. 2. 1.]]
import paddle

x = paddle.to_tensor([1, 2, 3])
y = paddle.to_tensor([1, 3, 2])
result1 = paddle.not_equal(x, y)
print(result1)  # result1 = [False True True]
import paddle

x = paddle.full(shape=[4, 5, 7], fill_value=0, dtype='int32')
numel = paddle.numel(x) # 140
import paddle
import numpy as np
data = np.random.uniform(-1, 1, [30, 10, 32]).astype('float32')
linear = paddle.nn.Linear(32, 64)
data = paddle.to_tensor(data)
x = linear(data)
print(x.numpy())
import paddle
x = paddle.to_tensor(1.0, place=paddle.CUDAPlace(0))
print(x.place)      # CUDAPlace(0)

y = x.pin_memory()
print(y.place)      # CUDAPinnedPlace
import paddle

x = paddle.to_tensor([1, 2, 3], dtype='float32')

# example 1: y is a float or int
res = paddle.pow(x, 2)
print(res)
# Tensor(shape=[3], dtype=float32, place=CUDAPlace(0), stop_gradient=True,
#        [1., 4., 9.])
res = paddle.pow(x, 2.5)
print(res)
# Tensor(shape=[3], dtype=float32, place=CUDAPlace(0), stop_gradient=True,
#        [1.         , 5.65685415 , 15.58845711])

# example 2: y is a Tensor
y = paddle.to_tensor([2], dtype='float32')
res = paddle.pow(x, y)
print(res)
# Tensor(shape=[3], dtype=float32, place=CUDAPlace(0), stop_gradient=True,
#        [1., 4., 9.])
import paddle

# the axis is a int element
x = paddle.to_tensor([[0.2, 0.3, 0.5, 0.9],
                      [0.1, 0.2, 0.6, 0.7]])
out1 = paddle.prod(x)
# [0.0002268]

out2 = paddle.prod(x, -1)
# [0.027  0.0084]

out3 = paddle.prod(x, 0)
# [0.02 0.06 0.3  0.63]

out4 = paddle.prod(x, 0, keepdim=True)
# [[0.02 0.06 0.3  0.63]]

out5 = paddle.prod(x, 0, dtype='int64')
# [0 0 0 0]

# the axis is list
y = paddle.to_tensor([[[1.0, 2.0], [3.0, 4.0]],
                      [[5.0, 6.0], [7.0, 8.0]]])
out6 = paddle.prod(y, [0, 1])
# [105. 384.]

out7 = paddle.prod(y, (1, 2))
# [  24. 1680.]
import paddle

x = paddle.to_tensor([[1.0, 2.0], [3.0, 4.0], [5.0, 6.0]]).astype('float64')
q, r = paddle.linalg.qr(x)
print (q)
print (r)

# Q = [[-0.16903085,  0.89708523],
#      [-0.50709255,  0.27602622],
#      [-0.84515425, -0.34503278]])

# R = [[-5.91607978, -7.43735744],
#      [ 0.        ,  0.82807867]])

# one can verify : X = Q * R ;
import paddle

input = paddle.rand((3, 100, 100))
rank = paddle.rank(input)
print(rank)
# 3
import paddle

x = paddle.to_tensor(
    [[1 + 6j, 2 + 5j, 3 + 4j], [4 + 3j, 5 + 2j, 6 + 1j]])
# Tensor(shape=[2, 3], dtype=complex64, place=CUDAPlace(0), stop_gradient=True,
#        [[(1+6j), (2+5j), (3+4j)],
#         [(4+3j), (5+2j), (6+1j)]])

real_res = paddle.real(x)
# Tensor(shape=[2, 3], dtype=float32, place=CUDAPlace(0), stop_gradient=True,
#        [[1., 2., 3.],
#         [4., 5., 6.]])

real_t = x.real()
# Tensor(shape=[2, 3], dtype=float32, place=CUDAPlace(0), stop_gradient=True,
#        [[1., 2., 3.],
#         [4., 5., 6.]])
import paddle

x = paddle.to_tensor([-0.4, -0.2, 0.1, 0.3])
out = paddle.reciprocal(x)
print(out)
# [-2.5        -5.         10.          3.33333333]
import paddle

# hook function return None
def print_hook_fn(grad):
    print(grad)

# hook function return Tensor
def double_hook_fn(grad):
    grad = grad * 2
    return grad

x = paddle.to_tensor([0., 1., 2., 3.], stop_gradient=False)
y = paddle.to_tensor([4., 5., 6., 7.], stop_gradient=False)
z = paddle.to_tensor([1., 2., 3., 4.])

# one Tensor can register multiple hooks
h = x.register_hook(print_hook_fn)
x.register_hook(double_hook_fn)

w = x + y
# register hook by lambda function
w.register_hook(lambda grad: grad * 2)

o = z.matmul(w)
o.backward()
# print_hook_fn print content in backward
# Tensor(shape=[4], dtype=float32, place=CUDAPlace(0), stop_gradient=False,
#        [2., 4., 6., 8.])

print("w.grad:", w.grad) # w.grad: [1. 2. 3. 4.]
print("x.grad:", x.grad) # x.grad: [ 4.  8. 12. 16.]
print("y.grad:", y.grad) # y.grad: [2. 4. 6. 8.]

# remove hook
h.remove()
import paddle

x = paddle.to_tensor([2, 3, 8, 7])
y = paddle.to_tensor([1, 5, 3, 3])
z = paddle.remainder(x, y)
print(z)  # [0, 3, 2, 1]
import numpy as np
import paddle

x = paddle.rand([2, 4, 6], dtype="float32")
positive_four = paddle.full([1], 4, "int32")

out = paddle.reshape(x, [-1, 0, 3, 2])
print(out)
# the shape is [2,4,3,2].

out = paddle.reshape(x, shape=[positive_four, 12])
print(out)
# the shape of out_2 is [4, 12].

shape_tensor = paddle.to_tensor(np.array([8, 6]).astype("int32"))
out = paddle.reshape(x, shape=shape_tensor)
print(out)
# the shape is [8, 6].
# out shares data with x in dygraph mode
x[0, 0, 0] = 10.
print(out[0, 0])
# the value is [10.]
import paddle
import numpy as np

image_shape=(3, 2, 2)
x = np.arange(image_shape[0] * image_shape[1] * image_shape[2]).reshape(image_shape)
x = x.astype('float32')
img = paddle.to_tensor(x)
tmp = paddle.flip(img, [0,1])
print(tmp) # [[[10,11],[8, 9]], [[6, 7],[4, 5]], [[2, 3],[0, 1]]]

out = paddle.flip(tmp,-1)
print(out) # [[[11,10],[9, 8]], [[7, 6],[5, 4]], [[3, 2],[1, 0]]]
import paddle

x = paddle.to_tensor([[1.0, 2.0, 3.0],
                      [4.0, 5.0, 6.0],
                      [7.0, 8.0, 9.0]])
out_z1 = paddle.roll(x, shifts=1)
print(out_z1)
#[[9. 1. 2.]
# [3. 4. 5.]
# [6. 7. 8.]]
out_z2 = paddle.roll(x, shifts=1, axis=0)
print(out_z2)
#[[7. 8. 9.]
# [1. 2. 3.]
# [4. 5. 6.]]
import paddle

x = paddle.to_tensor([-0.5, -0.2, 0.6, 1.5])
out = paddle.round(x)
print(out)
# [-1. -0.  1.  2.]
import paddle

x = paddle.to_tensor([0.1, 0.2, 0.3, 0.4])
out = paddle.rsqrt(x)
print(out)
# [3.16227766 2.23606798 1.82574186 1.58113883]
# scale as a float32 number
import paddle

data = paddle.randn(shape=[2,3], dtype='float32')
res = paddle.scale(data, scale=2.0, bias=1.0)
# scale with parameter scale as a Tensor
import paddle

data = paddle.randn(shape=[2, 3], dtype='float32')
factor = paddle.to_tensor([2], dtype='float32')
res = paddle.scale(data, scale=factor, bias=1.0)
import numpy as np
#input:
x = np.array([[1, 1], [2, 2], [3, 3]])
index = np.array([2, 1, 0, 1])
# shape of updates should be the same as x
# shape of updates with dim > 1 should be the same as input
updates = np.array([[1, 1], [2, 2], [3, 3], [4, 4]])
overwrite = False
# calculation:
if not overwrite:
    for i in range(len(index)):
        x[index[i]] = np.zeros((2))
for i in range(len(index)):
    if (overwrite):
        x[index[i]] = updates[i]
    else:
        x[index[i]] += updates[i]
# output:
out = np.array([[3, 3], [6, 6], [1, 1]])
out.shape # [3, 2]
import paddle

x = paddle.to_tensor([[1, 1], [2, 2], [3, 3]], dtype='float32')
index = paddle.to_tensor([2, 1, 0, 1], dtype='int64')
updates = paddle.to_tensor([[1, 1], [2, 2], [3, 3], [4, 4]], dtype='float32')

output1 = paddle.scatter(x, index, updates, overwrite=False)
# [[3., 3.],
#  [6., 6.],
#  [1., 1.]]

output2 = paddle.scatter(x, index, updates, overwrite=True)
# CPU device:
# [[3., 3.],
#  [4., 4.],
#  [1., 1.]]
# GPU device maybe have two results because of the repeated numbers in index
# result 1:
# [[3., 3.],
#  [4., 4.],
#  [1., 1.]]
# result 2:
# [[3., 3.],
#  [2., 2.],
#  [1., 1.]]
import paddle
import numpy as np

index_data = np.array([[1, 1],
                       [0, 1],
                       [1, 3]]).astype(np.int64)
index = paddle.to_tensor(index_data)
updates = paddle.rand(shape=[3, 9, 10], dtype='float32')
shape = [3, 5, 9, 10]

output = paddle.scatter_nd(index, updates, shape)
import paddle
import numpy as np

x = paddle.rand(shape=[3, 5, 9, 10], dtype='float32')
updates = paddle.rand(shape=[3, 9, 10], dtype='float32')
index_data = np.array([[1, 1],
                       [0, 1],
                       [1, 3]]).astype(np.int64)
index = paddle.to_tensor(index_data)
output = paddle.scatter_nd_add(x, index, updates)
import paddle.fluid as fluid
from paddle.fluid.dygraph.base import to_variable
from paddle.fluid.dygraph import Linear
import numpy as np

data = np.ones([3, 1024], dtype='float32')
with fluid.dygraph.guard():
    linear = fluid.dygraph.Linear(1024, 4)
    t = to_variable(data)
    linear(t)  # call with default weight
    custom_weight = np.random.randn(1024, 4).astype("float32")
    linear.weight.set_value(custom_weight)  # change existing weight
    out = linear(t)  # call with different weight
shard_size = (index_num + nshards - 1) // nshards
v = v - shard_id * shard_size if shard_id * shard_size <= v < (shard_id+1) * shard_size else ignore_value
import paddle
label = paddle.to_tensor([[16], [1]], "int64")
shard_label = paddle.shard_index(input=label,
                                 index_num=20,
                                 nshards=2,
                                 shard_id=0)
print(shard_label)
# [[-1], [1]]
import paddle

x = paddle.to_tensor([3.0, 0.0, -2.0, 1.7], dtype='float32')
out = paddle.sign(x=x)
print(out)  # [1.0, 0.0, -1.0, 1.0]
import paddle

x = paddle.to_tensor([-0.4, -0.2, 0.1, 0.3])
out = paddle.sin(x)
print(out)
# [-0.38941834 -0.19866933  0.09983342  0.29552021]
import paddle

x = paddle.to_tensor([-0.4, -0.2, 0.1, 0.3])
out = paddle.sinh(x)
print(out)
# [-0.41075233 -0.201336    0.10016675  0.30452029]
import paddle

input = paddle.rand(shape=[4, 5, 6], dtype='float32')
# example 1:
# attr starts is a list which doesn't contain tensor.
axes = [0, 1, 2]
starts = [-3, 0, 2]
ends = [3, 2, 4]
sliced_1 = paddle.slice(input, axes=axes, starts=starts, ends=ends)
# sliced_1 is input[0:3, 0:2, 2:4].

# example 2:
# attr starts is a list which contain tensor.
minus_3 = paddle.full([1], -3, "int32")
sliced_2 = paddle.slice(input, axes=axes, starts=[minus_3, 0, 2], ends=ends)
# sliced_2 is input[0:3, 0:2, 2:4].
import paddle

x = paddle.to_tensor([[[5,8,9,5],
                       [0,0,1,7],
                       [6,9,2,4]],
                      [[5,2,4,2],
                       [4,7,7,9],
                       [1,7,0,6]]],
                     dtype='float32')
out1 = paddle.sort(x=x, axis=-1)
out2 = paddle.sort(x=x, axis=0)
out3 = paddle.sort(x=x, axis=1)
print(out1)
#[[[5. 5. 8. 9.]
#  [0. 0. 1. 7.]
#  [2. 4. 6. 9.]]
# [[2. 2. 4. 5.]
#  [4. 7. 7. 9.]
#  [0. 1. 6. 7.]]]
print(out2)
#[[[5. 2. 4. 2.]
#  [0. 0. 1. 7.]
#  [1. 7. 0. 4.]]
# [[5. 8. 9. 5.]
#  [4. 7. 7. 9.]
#  [6. 9. 2. 6.]]]
print(out3)
#[[[0. 0. 1. 4.]
#  [5. 8. 2. 5.]
#  [6. 9. 9. 7.]]
# [[1. 2. 0. 2.]
#  [4. 7. 4. 6.]
#  [5. 7. 7. 9.]]]
import paddle

# x is a Tensor of shape [3, 9, 5]
x = paddle.rand([3, 9, 5])

out0, out1, out2 = paddle.split(x, num_or_sections=3, axis=1)
print(out0.shape)  # [3, 3, 5]
print(out1.shape)  # [3, 3, 5]
print(out2.shape)  # [3, 3, 5]

out0, out1, out2 = paddle.split(x, num_or_sections=[2, 3, 4], axis=1)
print(out0.shape)  # [3, 2, 5]
print(out1.shape)  # [3, 3, 5]
print(out2.shape)  # [3, 4, 5]

out0, out1, out2 = paddle.split(x, num_or_sections=[2, 3, -1], axis=1)
print(out0.shape)  # [3, 2, 5]
print(out1.shape)  # [3, 3, 5]
print(out2.shape)  # [3, 4, 5]

# axis is negative, the real axis is (rank(x) + axis)=1
out0, out1, out2 = paddle.split(x, num_or_sections=3, axis=-2)
print(out0.shape)  # [3, 3, 5]
print(out1.shape)  # [3, 3, 5]
print(out2.shape)  # [3, 3, 5]
import paddle

x = paddle.to_tensor([0.1, 0.2, 0.3, 0.4])
out = paddle.sqrt(x)
print(out)
# [0.31622777 0.4472136  0.54772256 0.63245553]
import paddle

x = paddle.to_tensor([-0.4, -0.2, 0.1, 0.3])
out = paddle.square(x)
print(out)
# [0.16 0.04 0.01 0.09]
import paddle

x = paddle.rand([5, 1, 10])
output = paddle.squeeze(x, axis=1)

print(x.shape)  # [5, 1, 10]
print(output.shape)  # [5, 10]

# output shares data with x in dygraph mode
x[0, 0, 0] = 10.
print(output[0, 0]) # [10.]
import paddle

x1 = paddle.to_tensor([[1.0, 2.0]])
x2 = paddle.to_tensor([[3.0, 4.0]])
x3 = paddle.to_tensor([[5.0, 6.0]])
out = paddle.stack([x1, x2, x3], axis=0)
print(out.shape)  # [3, 1, 2]
print(out)
# [[[1., 2.]],
#  [[3., 4.]],
#  [[5., 6.]]]
import paddle

x = paddle.to_tensor([1.0, 2.0, 3.0, 4.0])
out = paddle.stanh(x, scale_a=0.67, scale_b=1.72) # [1.00616539, 1.49927628, 1.65933108, 1.70390463]
import paddle

x = paddle.to_tensor([[1.0, 2.0, 3.0], [1.0, 4.0, 5.0]])
out1 = paddle.std(x)
# [1.63299316]
out2 = paddle.std(x, axis=1)
# [1.       2.081666]
import paddle
x = paddle.zeros(shape=[3,4,5,6], dtype="float32")
# example 1:
# attr starts is a list which doesn't contain Tensor.
axes = [1, 2, 3]
starts = [-3, 0, 2]
ends = [3, 2, 4]
strides_1 = [1, 1, 1]
strides_2 = [1, 1, 2]
sliced_1 = paddle.strided_slice(x, axes=axes, starts=starts, ends=ends, strides=strides_1)
# sliced_1 is x[:, 1:3:1, 0:2:1, 2:4:1].
# example 2:
# attr starts is a list which contain tensor Tensor.
minus_3 = paddle.full(shape=[1], fill_value=-3, dtype='int32')
sliced_2 = paddle.strided_slice(x, axes=axes, starts=[minus_3, 0, 2], ends=ends, strides=strides_2)
# sliced_2 is x[:, 1:3:1, 0:2:1, 2:4:2].
import numpy as np
import paddle

x = paddle.to_tensor([[1, 2], [7, 8]])
y = paddle.to_tensor([[5, 6], [3, 4]])
res = paddle.subtract(x, y)
print(res)
#       [[-4, -4],
#        [4, 4]]

x = paddle.to_tensor([[[1, 2, 3], [1, 2, 3]]])
y = paddle.to_tensor([1, 0, 4])
res = paddle.subtract(x, y)
print(res)
#       [[[ 0,  2, -1],
#         [ 0,  2, -1]]]

x = paddle.to_tensor([2, np.nan, 5], dtype='float32')
y = paddle.to_tensor([1, 4, np.nan], dtype='float32')
res = paddle.subtract(x, y)
print(res)
#       [ 1., nan, nan]

x = paddle.to_tensor([5, np.inf, -np.inf], dtype='float64')
y = paddle.to_tensor([1, 4, 5], dtype='float64')
res = paddle.subtract(x, y)
print(res)
#       [   4.,  inf., -inf.]
import paddle

# x is a Tensor with following elements:
#    [[0.2, 0.3, 0.5, 0.9]
#     [0.1, 0.2, 0.6, 0.7]]
# Each example is followed by the corresponding output tensor.
x = paddle.to_tensor([[0.2, 0.3, 0.5, 0.9],
                      [0.1, 0.2, 0.6, 0.7]])
out1 = paddle.sum(x)  # [3.5]
out2 = paddle.sum(x, axis=0)  # [0.3, 0.5, 1.1, 1.6]
out3 = paddle.sum(x, axis=-1)  # [1.9, 1.6]
out4 = paddle.sum(x, axis=1, keepdim=True)  # [[1.9], [1.6]]

# y is a Tensor with shape [2, 2, 2] and elements as below:
#      [[[1, 2], [3, 4]],
#      [[5, 6], [7, 8]]]
# Each example is followed by the corresponding output tensor.
y = paddle.to_tensor([[[1, 2], [3, 4]],
                      [[5, 6], [7, 8]]])
out5 = paddle.sum(y, axis=[1, 2]) # [10, 26]
out6 = paddle.sum(y, axis=[0, 1]) # [16, 20]

# x is a Tensor with following elements:
#    [[True, True, True, True]
#     [False, False, False, False]]
# Each example is followed by the corresponding output tensor.
x = paddle.to_tensor([[True, True, True, True],
                      [False, False, False, False]])
out7 = paddle.sum(x)  # [4]
out8 = paddle.sum(x, axis=0)  # [1, 1, 1, 1]
out9 = paddle.sum(x, axis=1)  # [4, 0]
import paddle
x = paddle.ones(shape=[2, 3], dtype='int32')
x_transposed = paddle.t(x)
print(x_transposed.shape)
# [3, 2]
import paddle

x = paddle.to_tensor([-0.4, -0.2, 0.1, 0.3])
out = paddle.tanh(x)
print(out)
# [-0.37994896 -0.19737532  0.09966799  0.29131261]
import paddle

data_type = 'float64'

# For two 2-d tensor x and y, the case axes=0 is equivalent to outer product.
# Note that tensordot supports empty axis sequence, so all the axes=0, axes=[], axes=[[]], and axes=[[],[]] are equivalent cases.
x = paddle.arange(4, dtype=data_type).reshape([2, 2])
y = paddle.arange(4, dtype=data_type).reshape([2, 2])
z = paddle.tensordot(x, y, axes=0)
# z = [[[[0., 0.],
#        [0., 0.]],
#
#       [[0., 1.],
#        [2., 3.]]],
#
#
#      [[[0., 2.],
#        [4., 6.]],
#
#       [[0., 3.],
#        [6., 9.]]]]


# For two 1-d tensor x and y, the case axes=1 is equivalent to inner product.
x = paddle.arange(10, dtype=data_type)
y = paddle.arange(10, dtype=data_type)
z1 = paddle.tensordot(x, y, axes=1)
z2 = paddle.dot(x, y)
# z1 = z2 = [285.]


# For two 2-d tensor x and y, the case axes=1 is equivalent to matrix multiplication.
x = paddle.arange(6, dtype=data_type).reshape([2, 3])
y = paddle.arange(12, dtype=data_type).reshape([3, 4])
z1 = paddle.tensordot(x, y, axes=1)
z2 = paddle.matmul(x, y)
# z1 = z2 =  [[20., 23., 26., 29.],
#             [56., 68., 80., 92.]]


# When axes is a 1-d int list, x and y will be contracted along the same given axes.
# Note that axes=[1, 2] is equivalent to axes=[[1, 2]], axes=[[1, 2], []], axes=[[1, 2], [1]], and axes=[[1, 2], [1, 2]].
x = paddle.arange(24, dtype=data_type).reshape([2, 3, 4])
y = paddle.arange(36, dtype=data_type).reshape([3, 3, 4])
z = paddle.tensordot(x, y, axes=[1, 2])
# z =  [[506. , 1298., 2090.],
#       [1298., 3818., 6338.]]


# When axes is a list containing two 1-d int list, the first will be applied to x and the second to y.
x = paddle.arange(60, dtype=data_type).reshape([3, 4, 5])
y = paddle.arange(24, dtype=data_type).reshape([4, 3, 2])
z = paddle.tensordot(x, y, axes=([1, 0], [0, 1]))
# z =  [[4400., 4730.],
#       [4532., 4874.],
#       [4664., 5018.],
#       [4796., 5162.],
#       [4928., 5306.]]


# Thanks to the support of axes expansion, axes=[[0, 1, 3, 4], [1, 0, 3, 4]] can be abbreviated as axes= [[0, 1, 3, 4], [1, 0]].
x = paddle.arange(720, dtype=data_type).reshape([2, 3, 4, 5, 6])
y = paddle.arange(720, dtype=data_type).reshape([3, 2, 4, 5, 6])
z = paddle.tensordot(x, y, axes=[[0, 1, 3, 4], [1, 0]])
# z = [[23217330., 24915630., 26613930., 28312230.],
#      [24915630., 26775930., 28636230., 30496530.],
#      [26613930., 28636230., 30658530., 32680830.],
#      [28312230., 30496530., 32680830., 34865130.]]
import paddle

data = paddle.to_tensor([1, 2, 3], dtype='int32')
out = paddle.tile(data, repeat_times=[2, 1])
np_out = out.numpy()
# [[1, 2, 3], [1, 2, 3]]

out = paddle.tile(data, repeat_times=[2, 2])
np_out = out.numpy()
# [[1, 2, 3, 1, 2, 3], [1, 2, 3, 1, 2, 3]]

repeat_times = paddle.to_tensor([2, 1], dtype='int32')
out = paddle.tile(data, repeat_times=repeat_times)
np_out = out.numpy()
# [[1, 2, 3], [1, 2, 3]]
import paddle

t = paddle.to_tensor([0,1,2,3,4])
expectlist = t.tolist()
print(expectlist)   #[0, 1, 2, 3, 4]

expectlist = paddle.tolist(t)
print(expectlist)   #[0, 1, 2, 3, 4]
import paddle

tensor_1 = paddle.to_tensor([1, 4, 5, 7])
value_1, indices_1 = paddle.topk(tensor_1, k=1)
print(value_1)
# [7]
print(indices_1)
# [3]
tensor_2 = paddle.to_tensor([[1, 4, 5, 7], [2, 6, 2, 5]])
value_2, indices_2 = paddle.topk(tensor_2, k=1)
print(value_2)
# [[7]
#  [6]]
print(indices_2)
# [[3]
#  [1]]
value_3, indices_3 = paddle.topk(tensor_2, k=1, axis=-1)
print(value_3)
# [[7]
#  [6]]
print(indices_3)
# [[3]
#  [1]]
value_4, indices_4 = paddle.topk(tensor_2, k=1, axis=0)
print(value_4)
# [[2 6 5 7]]
print(indices_4)
# [[1 1 0 0]]
import paddle

case1 = paddle.randn([2, 3])
case2 = paddle.randn([3, 10, 10])
case3 = paddle.randn([3, 10, 5, 10])
data1 = paddle.trace(case1) # data1.shape = [1]
data2 = paddle.trace(case2, offset=1, axis1=1, axis2=2) # data2.shape = [3]
data3 = paddle.trace(case3, offset=-3, axis1=1, axis2=-1) # data2.shape = [3, 5]
import paddle

x = paddle.randn([2, 3, 4])
x_transposed = paddle.transpose(x, perm=[1, 0, 2])
print(x_transposed.shape)
# [3L, 2L, 4L]
import paddle

input = paddle.rand([2,2],'float32')
print(input)
# Tensor(shape=[2, 2], dtype=float32, place=CUDAPlace(0), stop_gradient=True,
#         [[0.02331470, 0.42374918],
#         [0.79647720, 0.74970269]])

output = paddle.trunc(input)
print(output)
# Tensor(shape=[2, 2], dtype=float32, place=CUDAPlace(0), stop_gradient=True,
#         [[0., 0.],
#         [0., 0.]]))
import paddle
import numpy as np
# input is a variable which shape is [3, 4, 5]
np_input = np.random.rand(3, 4, 5).astype('float32')
input = paddle.to_tensor(np_input)
[x0, x1, x2] = paddle.unbind(input, axis=0)
# x0.shape [4, 5]
# x1.shape [4, 5]
# x2.shape [4, 5]
[x0, x1, x2, x3] = paddle.unbind(input, axis=1)
# x0.shape [3, 5]
# x1.shape [3, 5]
# x2.shape [3, 5]
# x3.shape [3, 5]
import paddle
# example:
x = paddle.ones(shape=[3, 4])
x.uniform_()
print(x)
# [[ 0.84524226,  0.6921872,   0.56528175,  0.71690357], # random
#  [-0.34646994, -0.45116323, -0.09902662, -0.11397249], # random
#  [ 0.433519,    0.39483607, -0.8660099,   0.83664286]] # random
import paddle

x = paddle.to_tensor([2, 3, 3, 1, 5, 3])
unique = paddle.unique(x)
np_unique = unique.numpy() # [1 2 3 5]
_, indices, inverse, counts = paddle.unique(x, return_index=True, return_inverse=True, return_counts=True)
np_indices = indices.numpy() # [3 0 1 4]
np_inverse = inverse.numpy() # [1 2 2 0 3 2]
np_counts = counts.numpy() # [1 1 3 1]

x = paddle.to_tensor([[2, 1, 3], [3, 0, 1], [2, 1, 3]])
unique = paddle.unique(x)
np_unique = unique.numpy() # [0 1 2 3]

unique = paddle.unique(x, axis=0)
np_unique = unique.numpy()
# [[2 1 3]
#  [3 0 1]]
import paddle

x = paddle.to_tensor([1, 1, 2, 2, 3, 1, 1, 2])
output = paddle.unique_consecutive(x) #
np_output = output.numpy() # [1 2 3 1 2]
_, inverse, counts = paddle.unique_consecutive(x, return_inverse=True, return_counts=True)
np_inverse = inverse.numpy() # [0 0 1 1 2 3 3 4]
np_counts = inverse.numpy() # [2 2 1 2 1]

x = paddle.to_tensor([[2, 1, 3], [3, 0, 1], [2, 1, 3], [2, 1, 3]])
output = paddle.unique_consecutive(x, axis=0) #
np_output = output.numpy() # [2 1 3 0 1 2 1 3 2 1 3]

x = paddle.to_tensor([[2, 1, 3], [3, 0, 1], [2, 1, 3], [2, 1, 3]])
output = paddle.unique_consecutive(x, axis=0) #
np_output = output.numpy()
# [[2 1 3]
#  [3 0 1]
#  [2 1 3]]
import paddle

x = paddle.rand([5, 10])
print(x.shape)  # [5, 10]

out1 = paddle.unsqueeze(x, axis=0)
print(out1.shape)  # [1, 5, 10]

out2 = paddle.unsqueeze(x, axis=[0, 2])
print(out2.shape)  # [1, 5, 1, 10]

axis = paddle.to_tensor([0, 1, 2])
out3 = paddle.unsqueeze(x, axis=axis)
print(out3.shape)  # [1, 1, 1, 5, 10]

# out1, out2, out3 share data with x in dygraph mode
x[0, 0] = 10.
print(out1[0, 0, 0]) # [10.]
print(out2[0, 0, 0, 0]) # [10.]
print(out3[0, 0, 0, 0, 0]) # [10.]
import paddle
x = paddle.ones(name='x', shape=[2, 3, 5], dtype='float32')  # create a tensor with shape=[2, 3, 5]
y = paddle.unstack(x, axis=1)  # unstack with second axis, which results 3 tensors with shape=[2, 5]
import paddle

x = paddle.to_tensor([[1.0, 2.0, 3.0], [1.0, 4.0, 5.0]])
out1 = paddle.var(x)
# [2.66666667]
out2 = paddle.var(x, axis=1)
# [1.         4.33333333]
import paddle

x = paddle.to_tensor([0.9383, 0.1983, 3.2, 1.2])
y = paddle.to_tensor([1.0, 1.0, 1.0, 1.0])
out = paddle.where(x>1, x, y)

print(out)
#out: [1.0, 1.0, 3.2, 1.2]
import paddle

tensor = paddle.to_tensor([0, 1, 2, 3, 4])

tensor.zero_()
print(tensor.tolist())   #[0, 0, 0, 0, 0]
import paddle

data_type = 'float64'

# For two 2-d tensor x and y, the case axes=0 is equivalent to outer product.
# Note that tensordot supports empty axis sequence, so all the axes=0, axes=[], axes=[[]], and axes=[[],[]] are equivalent cases.
x = paddle.arange(4, dtype=data_type).reshape([2, 2])
y = paddle.arange(4, dtype=data_type).reshape([2, 2])
z = paddle.tensordot(x, y, axes=0)
# z = [[[[0., 0.],
#        [0., 0.]],
#
#       [[0., 1.],
#        [2., 3.]]],
#
#
#      [[[0., 2.],
#        [4., 6.]],
#
#       [[0., 3.],
#        [6., 9.]]]]


# For two 1-d tensor x and y, the case axes=1 is equivalent to inner product.
x = paddle.arange(10, dtype=data_type)
y = paddle.arange(10, dtype=data_type)
z1 = paddle.tensordot(x, y, axes=1)
z2 = paddle.dot(x, y)
# z1 = z2 = [285.]


# For two 2-d tensor x and y, the case axes=1 is equivalent to matrix multiplication.
x = paddle.arange(6, dtype=data_type).reshape([2, 3])
y = paddle.arange(12, dtype=data_type).reshape([3, 4])
z1 = paddle.tensordot(x, y, axes=1)
z2 = paddle.matmul(x, y)
# z1 = z2 =  [[20., 23., 26., 29.],
#             [56., 68., 80., 92.]]


# When axes is a 1-d int list, x and y will be contracted along the same given axes.
# Note that axes=[1, 2] is equivalent to axes=[[1, 2]], axes=[[1, 2], []], axes=[[1, 2], [1]], and axes=[[1, 2], [1, 2]].
x = paddle.arange(24, dtype=data_type).reshape([2, 3, 4])
y = paddle.arange(36, dtype=data_type).reshape([3, 3, 4])
z = paddle.tensordot(x, y, axes=[1, 2])
# z =  [[506. , 1298., 2090.],
#       [1298., 3818., 6338.]]


# When axes is a list containing two 1-d int list, the first will be applied to x and the second to y.
x = paddle.arange(60, dtype=data_type).reshape([3, 4, 5])
y = paddle.arange(24, dtype=data_type).reshape([4, 3, 2])
z = paddle.tensordot(x, y, axes=([1, 0], [0, 1]))
# z =  [[4400., 4730.],
#       [4532., 4874.],
#       [4664., 5018.],
#       [4796., 5162.],
#       [4928., 5306.]]


# Thanks to the support of axes expansion, axes=[[0, 1, 3, 4], [1, 0, 3, 4]] can be abbreviated as axes= [[0, 1, 3, 4], [1, 0]].
x = paddle.arange(720, dtype=data_type).reshape([2, 3, 4, 5, 6])
y = paddle.arange(720, dtype=data_type).reshape([3, 2, 4, 5, 6])
z = paddle.tensordot(x, y, axes=[[0, 1, 3, 4], [1, 0]])
# z = [[23217330., 24915630., 26613930., 28312230.],
#      [24915630., 26775930., 28636230., 30496530.],
#      [26613930., 28636230., 30658530., 32680830.],
#      [28312230., 30496530., 32680830., 34865130.]]
import paddle

data = paddle.to_tensor([1, 2, 3], dtype='int32')
out = paddle.tile(data, repeat_times=[2, 1])
np_out = out.numpy()
# [[1, 2, 3], [1, 2, 3]]

out = paddle.tile(data, repeat_times=[2, 2])
np_out = out.numpy()
# [[1, 2, 3, 1, 2, 3], [1, 2, 3, 1, 2, 3]]

repeat_times = paddle.to_tensor([2, 1], dtype='int32')
out = paddle.tile(data, repeat_times=repeat_times)
np_out = out.numpy()
# [[1, 2, 3], [1, 2, 3]]
import paddle

type(paddle.to_tensor(1))
# <class 'paddle.Tensor'>

paddle.to_tensor(1)
# Tensor(shape=[1], dtype=int64, place=CPUPlace, stop_gradient=True,
#        [1])

x = paddle.to_tensor(1, stop_gradient=False)
print(x)
# Tensor(shape=[1], dtype=int64, place=CPUPlace, stop_gradient=False,
#        [1])

paddle.to_tensor(x)  # A new tensor will be created with default stop_gradient=True
# Tensor(shape=[1], dtype=int64, place=CPUPlace, stop_gradient=True,
#        [1])

paddle.to_tensor([[0.1, 0.2], [0.3, 0.4]], place=paddle.CPUPlace(), stop_gradient=False)
# Tensor(shape=[2, 2], dtype=float32, place=CPUPlace, stop_gradient=False,
#        [[0.10000000, 0.20000000],
#         [0.30000001, 0.40000001]])

type(paddle.to_tensor([[1+1j, 2], [3+2j, 4]], dtype='complex64'))
# <class 'paddle.Tensor'>

paddle.to_tensor([[1+1j, 2], [3+2j, 4]], dtype='complex64')
# Tensor(shape=[2, 2], dtype=complex64, place=CPUPlace, stop_gradient=True,
#        [[(1+1j), (2+0j)],
#         [(3+2j), (4+0j)]])
import paddle

t = paddle.to_tensor([0,1,2,3,4])
expectlist = t.tolist()
print(expectlist)   #[0, 1, 2, 3, 4]

expectlist = paddle.tolist(t)
print(expectlist)   #[0, 1, 2, 3, 4]
import paddle

tensor_1 = paddle.to_tensor([1, 4, 5, 7])
value_1, indices_1 = paddle.topk(tensor_1, k=1)
print(value_1)
# [7]
print(indices_1)
# [3]
tensor_2 = paddle.to_tensor([[1, 4, 5, 7], [2, 6, 2, 5]])
value_2, indices_2 = paddle.topk(tensor_2, k=1)
print(value_2)
# [[7]
#  [6]]
print(indices_2)
# [[3]
#  [1]]
value_3, indices_3 = paddle.topk(tensor_2, k=1, axis=-1)
print(value_3)
# [[7]
#  [6]]
print(indices_3)
# [[3]
#  [1]]
value_4, indices_4 = paddle.topk(tensor_2, k=1, axis=0)
print(value_4)
# [[2 6 5 7]]
print(indices_4)
# [[1 1 0 0]]
import paddle

case1 = paddle.randn([2, 3])
case2 = paddle.randn([3, 10, 10])
case3 = paddle.randn([3, 10, 5, 10])
data1 = paddle.trace(case1) # data1.shape = [1]
data2 = paddle.trace(case2, offset=1, axis1=1, axis2=2) # data2.shape = [3]
data3 = paddle.trace(case3, offset=-3, axis1=1, axis2=-1) # data2.shape = [3, 5]
import paddle

x = paddle.randn([2, 3, 4])
x_transposed = paddle.transpose(x, perm=[1, 0, 2])
print(x_transposed.shape)
# [3L, 2L, 4L]
import numpy as np
import paddle

data = np.arange(1, 13, dtype="int64").reshape(3,-1)
# array([[ 1,  2,  3,  4],
#        [ 5,  6,  7,  8],
#        [ 9, 10, 11, 12]])


x = paddle.to_tensor(data)

tril1 = paddle.tensor.tril(x)
# array([[ 1,  0,  0,  0],
#        [ 5,  6,  0,  0],
#        [ 9, 10, 11,  0]])

# example 2, positive diagonal value
tril2 = paddle.tensor.tril(x, diagonal=2)
# array([[ 1,  2,  3,  0],
#        [ 5,  6,  7,  8],
#        [ 9, 10, 11, 12]])

# example 3, negative diagonal value
tril3 = paddle.tensor.tril(x, diagonal=-1)
# array([[ 0,  0,  0,  0],
#        [ 5,  0,  0,  0],
#        [ 9, 10,  0,  0]])
import numpy as np
import paddle

data = np.arange(1, 13, dtype="int64").reshape(3,-1)
# array([[ 1,  2,  3,  4],
#        [ 5,  6,  7,  8],
#        [ 9, 10, 11, 12]])


# example 1, default diagonal
x = paddle.to_tensor(data)
triu1 = paddle.tensor.triu(x)
# array([[ 1,  2,  3,  4],
#        [ 0,  6,  7,  8],
#        [ 0,  0, 11, 12]])

# example 2, positive diagonal value
triu2 = paddle.tensor.triu(x, diagonal=2)
# array([[0, 0, 3, 4],
#        [0, 0, 0, 8],
#        [0, 0, 0, 0]])

# example 3, negative diagonal value
triu3 = paddle.tensor.triu(x, diagonal=-1)
# array([[ 1,  2,  3,  4],
#        [ 5,  6,  7,  8],
#        [ 0, 10, 11, 12]])
import paddle

input = paddle.rand([2,2],'float32')
print(input)
# Tensor(shape=[2, 2], dtype=float32, place=CUDAPlace(0), stop_gradient=True,
#         [[0.02331470, 0.42374918],
#         [0.79647720, 0.74970269]])

output = paddle.trunc(input)
print(output)
# Tensor(shape=[2, 2], dtype=float32, place=CUDAPlace(0), stop_gradient=True,
#         [[0., 0.],
#         [0., 0.]]))
import paddle
import numpy as np
# input is a variable which shape is [3, 4, 5]
np_input = np.random.rand(3, 4, 5).astype('float32')
input = paddle.to_tensor(np_input)
[x0, x1, x2] = paddle.unbind(input, axis=0)
# x0.shape [4, 5]
# x1.shape [4, 5]
# x2.shape [4, 5]
[x0, x1, x2, x3] = paddle.unbind(input, axis=1)
# x0.shape [3, 5]
# x1.shape [3, 5]
# x2.shape [3, 5]
# x3.shape [3, 5]
import paddle

# example 1:
# attr shape is a list which doesn't contain Tensor.
out1 = paddle.uniform(shape=[3, 4])
# [[ 0.84524226,  0.6921872,   0.56528175,  0.71690357], # random
#  [-0.34646994, -0.45116323, -0.09902662, -0.11397249], # random
#  [ 0.433519,    0.39483607, -0.8660099,   0.83664286]] # random

# example 2:
# attr shape is a list which contains Tensor.
dim1 = paddle.to_tensor([2], 'int64')
dim2 = paddle.to_tensor([3], 'int32')
out2 = paddle.uniform(shape=[dim1, dim2])
# [[-0.9951253,   0.30757582, 0.9899647 ], # random
#  [ 0.5864527,   0.6607096,  -0.8886161]] # random

# example 3:
# attr shape is a Tensor, the data type must be int64 or int32.
shape_tensor = paddle.to_tensor([2, 3])
out3 = paddle.uniform(shape_tensor)
# [[-0.8517412,  -0.4006908,   0.2551912 ], # random
#  [ 0.3364414,   0.36278176, -0.16085452]] # random
import paddle

x = paddle.to_tensor([2, 3, 3, 1, 5, 3])
unique = paddle.unique(x)
np_unique = unique.numpy() # [1 2 3 5]
_, indices, inverse, counts = paddle.unique(x, return_index=True, return_inverse=True, return_counts=True)
np_indices = indices.numpy() # [3 0 1 4]
np_inverse = inverse.numpy() # [1 2 2 0 3 2]
np_counts = counts.numpy() # [1 1 3 1]

x = paddle.to_tensor([[2, 1, 3], [3, 0, 1], [2, 1, 3]])
unique = paddle.unique(x)
np_unique = unique.numpy() # [0 1 2 3]

unique = paddle.unique(x, axis=0)
np_unique = unique.numpy()
# [[2 1 3]
#  [3 0 1]]
import paddle

x = paddle.to_tensor([1, 1, 2, 2, 3, 1, 1, 2])
output = paddle.unique_consecutive(x) #
np_output = output.numpy() # [1 2 3 1 2]
_, inverse, counts = paddle.unique_consecutive(x, return_inverse=True, return_counts=True)
np_inverse = inverse.numpy() # [0 0 1 1 2 3 3 4]
np_counts = inverse.numpy() # [2 2 1 2 1]

x = paddle.to_tensor([[2, 1, 3], [3, 0, 1], [2, 1, 3], [2, 1, 3]])
output = paddle.unique_consecutive(x, axis=0) #
np_output = output.numpy() # [2 1 3 0 1 2 1 3 2 1 3]

x = paddle.to_tensor([[2, 1, 3], [3, 0, 1], [2, 1, 3], [2, 1, 3]])
output = paddle.unique_consecutive(x, axis=0) #
np_output = output.numpy()
# [[2 1 3]
#  [3 0 1]
#  [2 1 3]]
import paddle

x = paddle.rand([5, 10])
print(x.shape)  # [5, 10]

out1 = paddle.unsqueeze(x, axis=0)
print(out1.shape)  # [1, 5, 10]

out2 = paddle.unsqueeze(x, axis=[0, 2])
print(out2.shape)  # [1, 5, 1, 10]

axis = paddle.to_tensor([0, 1, 2])
out3 = paddle.unsqueeze(x, axis=axis)
print(out3.shape)  # [1, 1, 1, 5, 10]

# out1, out2, out3 share data with x in dygraph mode
x[0, 0] = 10.
print(out1[0, 0, 0]) # [10.]
print(out2[0, 0, 0, 0]) # [10.]
print(out3[0, 0, 0, 0, 0]) # [10.]
import paddle
x = paddle.ones(name='x', shape=[2, 3, 5], dtype='float32')  # create a tensor with shape=[2, 3, 5]
y = paddle.unstack(x, axis=1)  # unstack with second axis, which results 3 tensors with shape=[2, 5]
import paddle

x = paddle.to_tensor([[1.0, 2.0, 3.0], [1.0, 4.0, 5.0]])
out1 = paddle.var(x)
# [2.66666667]
out2 = paddle.var(x, axis=1)
# [1.         4.33333333]
import paddle

x = paddle.to_tensor([0.9383, 0.1983, 3.2, 1.2])
y = paddle.to_tensor([1.0, 1.0, 1.0, 1.0])
out = paddle.where(x>1, x, y)

print(out)
#out: [1.0, 1.0, 3.2, 1.2]
import paddle

data = paddle.zeros(shape=[3, 2], dtype='float32')
# [[0. 0.]
#  [0. 0.]
#  [0. 0.]]
data = paddle.zeros(shape=[2, 2])
# [[0. 0.]
#  [0. 0.]]

# shape is a Tensor
shape = paddle.full(shape=[2], dtype='int32', fill_value=2)
data3 = paddle.zeros(shape=shape, dtype='int32')
# [[0 0]
#  [0 0]]
import paddle

x = paddle.to_tensor([1, 2, 3])
out1 = paddle.zeros_like(x) # [0., 0., 0.]
out2 = paddle.zeros_like(x, dtype='int32') # [0, 0, 0]
import paddle

conv2d = paddle.nn.Conv2D(3, 2, 3, bias_attr=False)
data = paddle.rand([10, 3, 32, 32])

with paddle.amp.auto_cast():
    conv = conv2d(data)
    print(conv.dtype) # FP16

with paddle.amp.auto_cast(enable=False):
    conv = conv2d(data)
    print(conv.dtype) # FP32

with paddle.amp.auto_cast(custom_black_list={'conv2d'}):
    conv = conv2d(data)
    print(conv.dtype) # FP32

a = paddle.rand([2,3])
b = paddle.rand([2,3])
with paddle.amp.auto_cast(custom_white_list={'elementwise_add'}):
    c = a + b
    print(c.dtype) # FP16

with paddle.amp.auto_cast(custom_white_list={'elementwise_add'}, level='O2'):
    d = a + b
    print(d.dtype) # FP16
# required: gpu
# Demo1: single model and optimizer:
import paddle

model = paddle.nn.Conv2D(3, 2, 3, bias_attr=False)
optimzier = paddle.optimizer.SGD(parameters=model.parameters())

model, optimizer = paddle.amp.decorate(models=model, optimizers=optimzier, level='O2')

data = paddle.rand([10, 3, 32, 32])

with paddle.amp.auto_cast(enable=True, custom_white_list=None, custom_black_list=None, level='O2'):
    output = model(data)
    print(output.dtype) # FP16

# required: gpu
# Demo2: multi models and optimizers:
model2 = paddle.nn.Conv2D(3, 2, 3, bias_attr=False)
optimizer2 = paddle.optimizer.Adam(parameters=model2.parameters())

models, optimizers = paddle.amp.decorate(models=[model, model2], optimizers=[optimzier, optimizer2], level='O2')

data = paddle.rand([10, 3, 32, 32])

with paddle.amp.auto_cast(enable=True, custom_white_list=None, custom_black_list=None, level='O2'):
    output = models[0](data)
    output2 = models[1](data)
    print(output.dtype) # FP16
    print(output2.dtype) # FP16
import paddle

model = paddle.nn.Conv2D(3, 2, 3, bias_attr=True)
optimizer = paddle.optimizer.SGD(learning_rate=0.01, parameters=model.parameters())
scaler = paddle.amp.GradScaler(init_loss_scaling=1024)
data = paddle.rand([10, 3, 32, 32])

with paddle.amp.auto_cast():
    conv = model(data)
    loss = paddle.mean(conv)

scaled = scaler.scale(loss)  # scale the loss
scaled.backward()            # do backward
scaler.minimize(optimizer, scaled)  # update parameters
optimizer.clear_grad()
import paddle

model = paddle.nn.Conv2D(3, 2, 3, bias_attr=True)
optimizer = paddle.optimizer.SGD(learning_rate=0.01, parameters=model.parameters())
scaler = paddle.amp.GradScaler(init_loss_scaling=1024)
data = paddle.rand([10, 3, 32, 32])

with paddle.amp.auto_cast():
    conv = model(data)
    loss = paddle.mean(conv)

scaled = scaler.scale(loss)  # scale the loss
scaled.backward()            # do backward
scaler.minimize(optimizer, scaled)  # update parameters
optimizer.clear_grad()
import paddle

model = paddle.nn.Conv2D(3, 2, 3, bias_attr=True)
optimizer = paddle.optimizer.SGD(learning_rate=0.01, parameters=model.parameters())
scaler = paddle.amp.GradScaler(init_loss_scaling=1024)
data = paddle.rand([10, 3, 32, 32])

with paddle.amp.auto_cast():
    conv = model(data)
    loss = paddle.mean(conv)

scaled = scaler.scale(loss)  # scale the loss
scaled.backward()            # do backward
scaler.minimize(optimizer, scaled)  # update parameters
optimizer.clear_grad()
# required: gpu
import paddle

model = paddle.nn.Conv2D(3, 2, 3, bias_attr=True)
optimizer = paddle.optimizer.SGD(learning_rate=0.01, parameters=model.parameters())
scaler = paddle.amp.GradScaler(init_loss_scaling=1024)
data = paddle.rand([10, 3, 32, 32])
with paddle.amp.auto_cast():
    conv = model(data)
    loss = paddle.mean(conv)
scaled = scaler.scale(loss)  # scale the loss
scaled.backward()            # do backward
scaler.step(optimizer)       # update parameters
scaler.update()              # update the loss scaling ratio
optimizer.clear_grad()
# required: gpu
import paddle

model = paddle.nn.Conv2D(3, 2, 3, bias_attr=True)
optimizer = paddle.optimizer.SGD(learning_rate=0.01, parameters=model.parameters())
scaler = paddle.amp.GradScaler(init_loss_scaling=1024)
data = paddle.rand([10, 3, 32, 32])
with paddle.amp.auto_cast():
    conv = model(data)
    loss = paddle.mean(conv)
scaled = scaler.scale(loss)     # scale the loss
scaled.backward()               # do backward
scaler.step(optimizer)          # update parameters
scaler.update()                 # update the loss scaling ratio
optimizer.clear_grad()
# required: gpu
import paddle

model = paddle.nn.Conv2D(3, 2, 3, bias_attr=True)
optimizer = paddle.optimizer.SGD(learning_rate=0.01, parameters=model.parameters())
scaler = paddle.amp.GradScaler(init_loss_scaling=1024)
data = paddle.rand([10, 3, 32, 32])
with paddle.amp.auto_cast():
    conv = model(data)
    loss = paddle.mean(conv)
scaled = scaler.scale(loss)  # scale the loss
scaled.backward()            # do backward
scaler.unscale_(optimizer)    # unscale the parameter
scaler.step(optimizer)
scaler.update()
optimizer.clear_grad()
# required: gpu,xpu
import paddle
scaler = paddle.amp.GradScaler(enable=True,
                               init_loss_scaling=1024,
                               incr_ratio=2.0,
                               decr_ratio=0.5,
                               incr_every_n_steps=1000,
                               decr_every_n_nan_or_inf=2,
                               use_dynamic_loss_scaling=True)
enable = scaler.is_enable()
print(enable) # True
# required: gpu,xpu
import paddle
scaler = paddle.amp.GradScaler(enable=True,
                               init_loss_scaling=1024,
                               incr_ratio=2.0,
                               decr_ratio=0.5,
                               incr_every_n_steps=1000,
                               decr_every_n_nan_or_inf=2,
                               use_dynamic_loss_scaling=True)
use_dynamic_loss_scaling = scaler.is_use_dynamic_loss_scaling()
print(use_dynamic_loss_scaling) # True
# required: gpu,xpu
import paddle
scaler = paddle.amp.GradScaler(enable=True,
                               init_loss_scaling=1024,
                               incr_ratio=2.0,
                               decr_ratio=0.5,
                               incr_every_n_steps=1000,
                               decr_every_n_nan_or_inf=2,
                               use_dynamic_loss_scaling=True)
init_loss_scaling = scaler.get_init_loss_scaling()
print(init_loss_scaling) # 1024
# required: gpu,xpu
import paddle
scaler = paddle.amp.GradScaler(enable=True,
                               init_loss_scaling=1024,
                               incr_ratio=2.0,
                               decr_ratio=0.5,
                               incr_every_n_steps=1000,
                               decr_every_n_nan_or_inf=2,
                               use_dynamic_loss_scaling=True)
print(scaler.get_init_loss_scaling()) # 1024
new_init_loss_scaling = 1000
scaler.set_init_loss_scaling(new_init_loss_scaling)
print(scaler.get_init_loss_scaling()) # 1000
# required: gpu,xpu
import paddle
scaler = paddle.amp.GradScaler(enable=True,
                               init_loss_scaling=1024,
                               incr_ratio=2.0,
                               decr_ratio=0.5,
                               incr_every_n_steps=1000,
                               decr_every_n_nan_or_inf=2,
                               use_dynamic_loss_scaling=True)
incr_ratio = scaler.get_incr_ratio()
print(incr_ratio) # 2.0
# required: gpu,xpu
import paddle
scaler = paddle.amp.GradScaler(enable=True,
                               init_loss_scaling=1024,
                               incr_ratio=2.0,
                               decr_ratio=0.5,
                               incr_every_n_steps=1000,
                               decr_every_n_nan_or_inf=2,
                               use_dynamic_loss_scaling=True)
print(scaler.get_incr_ratio()) # 2.0
new_incr_ratio = 3.0
scaler.set_incr_ratio(new_incr_ratio)
print(scaler.get_incr_ratio()) # 3.0
# required: gpu,xpu
import paddle
scaler = paddle.amp.GradScaler(enable=True,
                               init_loss_scaling=1024,
                               incr_ratio=2.0,
                               decr_ratio=0.5,
                               incr_every_n_steps=1000,
                               decr_every_n_nan_or_inf=2,
                               use_dynamic_loss_scaling=True)
decr_ratio = scaler.get_decr_ratio()
print(decr_ratio) # 0.5
# required: gpu,xpu
import paddle
scaler = paddle.amp.GradScaler(enable=True,
                               init_loss_scaling=1024,
                               incr_ratio=2.0,
                               decr_ratio=0.5,
                               incr_every_n_steps=1000,
                               decr_every_n_nan_or_inf=2,
                               use_dynamic_loss_scaling=True)
print(scaler.get_decr_ratio()) # 0.5
new_decr_ratio = 0.1
scaler.set_decr_ratio(new_decr_ratio)
print(scaler.get_decr_ratio()) # 0.1
# required: gpu,xpu
import paddle
scaler = paddle.amp.GradScaler(enable=True,
                               init_loss_scaling=1024,
                               incr_ratio=2.0,
                               decr_ratio=0.5,
                               incr_every_n_steps=1000,
                               decr_every_n_nan_or_inf=2,
                               use_dynamic_loss_scaling=True)
incr_every_n_steps = scaler.get_incr_every_n_steps()
print(incr_every_n_steps) # 1000
# required: gpu,xpu
import paddle
scaler = paddle.amp.GradScaler(enable=True,
                               init_loss_scaling=1024,
                               incr_ratio=2.0,
                               decr_ratio=0.5,
                               incr_every_n_steps=1000,
                               decr_every_n_nan_or_inf=2,
                               use_dynamic_loss_scaling=True)
print(scaler.get_incr_every_n_steps()) # 1000
new_incr_every_n_steps = 2000
scaler.set_incr_every_n_steps(new_incr_every_n_steps)
print(scaler.get_incr_every_n_steps()) # 2000
# required: gpu,xpu
import paddle
scaler = paddle.amp.GradScaler(enable=True,
                               init_loss_scaling=1024,
                               incr_ratio=2.0,
                               decr_ratio=0.5,
                               incr_every_n_steps=1000,
                               decr_every_n_nan_or_inf=2,
                               use_dynamic_loss_scaling=True)
decr_every_n_nan_or_inf = scaler.get_decr_every_n_nan_or_inf()
print(decr_every_n_nan_or_inf) # 2
# required: gpu,xpu
import paddle
scaler = paddle.amp.GradScaler(enable=True,
                               init_loss_scaling=1024,
                               incr_ratio=2.0,
                               decr_ratio=0.5,
                               incr_every_n_steps=1000,
                               decr_every_n_nan_or_inf=2,
                               use_dynamic_loss_scaling=True)
print(scaler.get_decr_every_n_nan_or_inf()) # 2
new_decr_every_n_nan_or_inf = 3
scaler.set_decr_every_n_nan_or_inf(new_decr_every_n_nan_or_inf)
print(scaler.get_decr_every_n_nan_or_inf()) # 3
# required: gpu,xpu
import paddle

scaler = paddle.amp.GradScaler(enable=True,
                               init_loss_scaling=1024,
                               incr_ratio=2.0,
                               decr_ratio=0.5,
                               incr_every_n_steps=1000,
                               decr_every_n_nan_or_inf=2,
                               use_dynamic_loss_scaling=True)
scaler_state = scaler.state_dict()
# required: gpu,xpu
import paddle

scaler = paddle.amp.GradScaler(enable=True,
                               init_loss_scaling=1024,
                               incr_ratio=2.0,
                               decr_ratio=0.5,
                               incr_every_n_steps=1000,
                               decr_every_n_nan_or_inf=2,
                               use_dynamic_loss_scaling=True)
scaler_state = scaler.state_dict()
scaler.load_state_dict(scaler_state)
import paddle
x = paddle.to_tensor([[1, 2], [3, 4]], dtype='float32', stop_gradient=False)
y = paddle.to_tensor([[3, 2], [3, 4]], dtype='float32')

grad_tensor1 = paddle.to_tensor([[1,2], [2, 3]], dtype='float32')
grad_tensor2 = paddle.to_tensor([[1,1], [1, 1]], dtype='float32')

z1 = paddle.matmul(x, y)
z2 = paddle.matmul(x, y)

paddle.autograd.backward([z1, z2], [grad_tensor1, grad_tensor2], True)
print(x.grad)
#[[12. 18.]
# [17. 25.]]

x.clear_grad()

paddle.autograd.backward([z1, z2], [grad_tensor1, None], True)
print(x.grad)
#[[12. 18.]
# [17. 25.]]

x.clear_grad()

paddle.autograd.backward([z1, z2])
print(x.grad)
#[[10. 14.]
# [10. 14.]]
import paddle
from paddle.autograd import PyLayer

# Inherit from PyLayer
class cus_tanh(PyLayer):
    @staticmethod
    def forward(ctx, x, func1, func2=paddle.square):
        # ctx is a context object that store some objects for backward.
        ctx.func = func2
        y = func1(x)
        # Pass tensors to backward.
        ctx.save_for_backward(y)
        return y

    @staticmethod
    # forward has only one output, so there is only one gradient in the input of backward.
    def backward(ctx, dy):
        # Get the tensors passed by forward.
        y, = ctx.saved_tensor()
        grad = dy * (1 - ctx.func(y))
        # forward has only one input, so only one gradient tensor is returned.
        return grad


data = paddle.randn([2, 3], dtype="float64")
data.stop_gradient = False
z = cus_tanh.apply(data, func1=paddle.tanh)
z.mean().backward()

print(data.grad)
import paddle
from paddle.autograd import PyLayer

class cus_tanh(PyLayer):
    @staticmethod
    def forward(ctx, x):
        y = paddle.tanh(x)
        # Pass tensors to backward.
        ctx.save_for_backward(y)
        return y

    @staticmethod
    def backward(ctx, dy):
        # Get the tensors passed by forward.
        y, = ctx.saved_tensor()
        grad = dy * (1 - paddle.square(y))
        return grad
import paddle
from paddle.autograd import PyLayer

class cus_tanh(PyLayer):
    @staticmethod
    def forward(ctx, x):
        y = paddle.tanh(x)
        # Pass tensors to backward.
        ctx.save_for_backward(y)
        return y

    @staticmethod
    def backward(ctx, dy):
        # Get the tensors passed by forward.
        y, = ctx.saved_tensor()
        grad = dy * (1 - paddle.square(y))
        return grad
import paddle
from paddle.autograd import PyLayer

class cus_tanh(PyLayer):
    @staticmethod
    def forward(ctx, x, func1, func2=paddle.square):
        ctx.func = func2
        y = func1(x)
        # Pass tensors to backward.
        ctx.save_for_backward(y)
        return y

    @staticmethod
    def backward(ctx, dy):
        # Get the tensors passed by forward.
        y, = ctx.saved_tensor()
        grad = dy * (1 - ctx.func(y))
        return grad


data = paddle.randn([2, 3], dtype="float64")
data.stop_gradient = False
# run custom Layer.
z = cus_tanh.apply(data, func1=paddle.tanh)
import paddle
from paddle.autograd import PyLayer

class cus_tanh(PyLayer):
    @staticmethod
    def forward(ctx, x):
        # ctx is a object of PyLayerContext.
        y = paddle.tanh(x)
        ctx.save_for_backward(y)
        return y

    @staticmethod
    def backward(ctx, dy):
        # ctx is a object of PyLayerContext.
        y, = ctx.saved_tensor()
        grad = dy * (1 - paddle.square(y))
        return grad
import paddle
from paddle.autograd import PyLayer

class cus_tanh(PyLayer):
    @staticmethod
    def forward(ctx, x):
        # ctx is a context object that store some objects for backward.
        y = paddle.tanh(x)
        # Pass tensors to backward.
        ctx.save_for_backward(y)
        return y

    @staticmethod
    def backward(ctx, dy):
        # Get the tensors passed by forward.
        y, = ctx.saved_tensor()
        grad = dy * (1 - paddle.square(y))
        return grad
import paddle
from paddle.autograd import PyLayer

class cus_tanh(PyLayer):
    @staticmethod
    def forward(ctx, x):
        # ctx is a context object that store some objects for backward.
        y = paddle.tanh(x)
        # Pass tensors to backward.
        ctx.save_for_backward(y)
        return y

    @staticmethod
    def backward(ctx, dy):
        # Get the tensors passed by forward.
        y, = ctx.saved_tensor()
        grad = dy * (1 - paddle.square(y))
        return grad
import paddle

# build a simple model checkpoint callback
class ModelCheckpoint(paddle.callbacks.Callback):
    def __init__(self, save_freq=1, save_dir=None):
        self.save_freq = save_freq
        self.save_dir = save_dir

    def on_epoch_end(self, epoch, logs=None):
        if self.model is not None and epoch % self.save_freq == 0:
            path = '{}/{}'.format(self.save_dir, epoch)
            print('save checkpoint at {}'.format(path))
            self.model.save(path)
import paddle
from paddle import Model
from paddle.static import InputSpec
from paddle.vision.models import LeNet
from paddle.vision.datasets import MNIST
from paddle.metric import Accuracy
from paddle.nn import CrossEntropyLoss
import paddle.vision.transforms as T

device = paddle.set_device('cpu')
sample_num = 200
save_dir = './best_model_checkpoint'
transform = T.Compose(
    [T.Transpose(), T.Normalize([127.5], [127.5])])
train_dataset = MNIST(mode='train', transform=transform)
val_dataset = MNIST(mode='test', transform=transform)
net = LeNet()
optim = paddle.optimizer.Adam(
    learning_rate=0.001, parameters=net.parameters())

inputs = [InputSpec([None, 1, 28, 28], 'float32', 'x')]
labels = [InputSpec([None, 1], 'int64', 'label')]

model = Model(net, inputs=inputs, labels=labels)
model.prepare(
    optim,
    loss=CrossEntropyLoss(reduction="sum"),
    metrics=[Accuracy()])
callbacks = paddle.callbacks.EarlyStopping(
    'loss',
    mode='min',
    patience=1,
    verbose=1,
    min_delta=0,
    baseline=None,
    save_best_model=True)
model.fit(train_dataset,
          val_dataset,
          batch_size=64,
          log_freq=200,
          save_freq=10,
          save_dir=save_dir,
          epochs=20,
          callbacks=[callbacks])
import paddle
import paddle.vision.transforms as T
from paddle.static import InputSpec

inputs = [InputSpec([-1, 1, 28, 28], 'float32', 'image')]
labels = [InputSpec([None, 1], 'int64', 'label')]

transform = T.Compose([
    T.Transpose(),
    T.Normalize([127.5], [127.5])
])
train_dataset = paddle.vision.datasets.MNIST(mode='train', transform=transform)

lenet = paddle.vision.models.LeNet()
model = paddle.Model(lenet,
    inputs, labels)

base_lr = 1e-3
boundaries = [5, 8]
wamup_steps = 4

def make_optimizer(parameters=None):
    momentum = 0.9
    weight_decay = 5e-4
    values = [base_lr * (0.1**i) for i in range(len(boundaries) + 1)]
    learning_rate = paddle.optimizer.lr.PiecewiseDecay(
        boundaries=boundaries, values=values)
    learning_rate = paddle.optimizer.lr.LinearWarmup(
        learning_rate=learning_rate,
        warmup_steps=wamup_steps,
        start_lr=base_lr / 5.,
        end_lr=base_lr,
        verbose=True)
    optimizer = paddle.optimizer.Momentum(
        learning_rate=learning_rate,
        weight_decay=weight_decay,
        momentum=momentum,
        parameters=parameters)
    return optimizer

optim = make_optimizer(parameters=lenet.parameters())
model.prepare(optimizer=optim,
            loss=paddle.nn.CrossEntropyLoss(),
            metrics=paddle.metric.Accuracy())

# if LRScheduler callback not set, an instance LRScheduler update by step
# will be created auto.
model.fit(train_dataset, batch_size=64)

# create a learning rate scheduler update by epoch
callback = paddle.callbacks.LRScheduler(by_step=False, by_epoch=True)
model.fit(train_dataset, batch_size=64, callbacks=callback)
import paddle
import paddle.vision.transforms as T
from paddle.vision.datasets import MNIST
from paddle.static import InputSpec

inputs = [InputSpec([-1, 1, 28, 28], 'float32', 'image')]
labels = [InputSpec([None, 1], 'int64', 'label')]

transform = T.Compose([
    T.Transpose(),
    T.Normalize([127.5], [127.5])
])
train_dataset = MNIST(mode='train', transform=transform)

lenet = paddle.vision.models.LeNet()
model = paddle.Model(lenet,
    inputs, labels)

optim = paddle.optimizer.Adam(0.001, parameters=lenet.parameters())
model.prepare(optimizer=optim,
            loss=paddle.nn.CrossEntropyLoss(),
            metrics=paddle.metric.Accuracy())

callback = paddle.callbacks.ModelCheckpoint(save_dir='./temp')
model.fit(train_dataset, batch_size=64, callbacks=callback)
import paddle
import paddle.vision.transforms as T
from paddle.vision.datasets import MNIST
from paddle.static import InputSpec

inputs = [InputSpec([-1, 1, 28, 28], 'float32', 'image')]
labels = [InputSpec([None, 1], 'int64', 'label')]

transform = T.Compose([
    T.Transpose(),
    T.Normalize([127.5], [127.5])
])
train_dataset = MNIST(mode='train', transform=transform)

lenet = paddle.vision.models.LeNet()
model = paddle.Model(lenet,
    inputs, labels)

optim = paddle.optimizer.Adam(0.001, parameters=lenet.parameters())
model.prepare(optimizer=optim,
            loss=paddle.nn.CrossEntropyLoss(),
            metrics=paddle.metric.Accuracy())

callback = paddle.callbacks.ProgBarLogger(log_freq=10)
model.fit(train_dataset, batch_size=64, callbacks=callback)
import paddle
from paddle import Model
from paddle.static import InputSpec
from paddle.vision.models import LeNet
from paddle.vision.datasets import MNIST
from paddle.metric import Accuracy
from paddle.nn.layer.loss import CrossEntropyLoss
import paddle.vision.transforms as T
sample_num = 200
transform = T.Compose(
    [T.Transpose(), T.Normalize([127.5], [127.5])])
train_dataset = MNIST(mode='train', transform=transform)
val_dataset = MNIST(mode='test', transform=transform)
net = LeNet()
optim = paddle.optimizer.Adam(
    learning_rate=0.001, parameters=net.parameters())
inputs = [InputSpec([None, 1, 28, 28], 'float32', 'x')]
labels = [InputSpec([None, 1], 'int64', 'label')]
model = Model(net, inputs=inputs, labels=labels)
model.prepare(
    optim,
    loss=CrossEntropyLoss(),
    metrics=[Accuracy()])
callbacks = paddle.callbacks.ReduceLROnPlateau(patience=3, verbose=1)
model.fit(train_dataset,
            val_dataset,
            batch_size=64,
            log_freq=200,
            save_freq=10,
            epochs=20,
            callbacks=[callbacks])
import paddle
import paddle.vision.transforms as T
from paddle.static import InputSpec

inputs = [InputSpec([-1, 1, 28, 28], 'float32', 'image')]
labels = [InputSpec([None, 1], 'int64', 'label')]

transform = T.Compose([
    T.Transpose(),
    T.Normalize([127.5], [127.5])
])
train_dataset = paddle.vision.datasets.MNIST(mode='train', transform=transform)
eval_dataset = paddle.vision.datasets.MNIST(mode='test', transform=transform)

net = paddle.vision.models.LeNet()
model = paddle.Model(net, inputs, labels)

optim = paddle.optimizer.Adam(0.001, parameters=net.parameters())
model.prepare(optimizer=optim,
            loss=paddle.nn.CrossEntropyLoss(),
            metrics=paddle.metric.Accuracy())

## uncomment following lines to fit model with visualdl callback function
# callback = paddle.callbacks.VisualDL(log_dir='visualdl_log_dir')
# model.fit(train_dataset, eval_dataset, batch_size=64, callbacks=callback)
import paddle

cudnn_version = paddle.device.get_cudnn_version()
import paddle
device = paddle.device.get_device()
import paddle
support_gpu = paddle.device.is_compiled_with_cuda()
import paddle
support_npu = paddle.device.is_compiled_with_npu()
import paddle
support_gpu = paddle.device.is_compiled_with_rocm()
import paddle
support_xpu = paddle.device.is_compiled_with_xpu()
import paddle

paddle.device.set_device("cpu")
x1 = paddle.ones(name='x1', shape=[1, 2], dtype='int32')
x2 = paddle.zeros(name='x2', shape=[1, 2], dtype='int32')
data = paddle.stack([x1,x2], axis=1)
# required: xpu

import paddle
place = paddle.device.XPUPlace(0)
import numpy as np
import paddle
from paddle.distributed import init_parallel_env

paddle.set_device('gpu:%d'%paddle.distributed.ParallelEnv().dev_id)
init_parallel_env()
tensor_list = []
if paddle.distributed.ParallelEnv().local_rank == 0:
    np_data1 = np.array([[4, 5, 6], [4, 5, 6]])
    np_data2 = np.array([[4, 5, 6], [4, 5, 6]])
    data1 = paddle.to_tensor(np_data1)
    data2 = paddle.to_tensor(np_data2)
    paddle.distributed.all_gather(tensor_list, data1)
else:
    np_data1 = np.array([[1, 2, 3], [1, 2, 3]])
    np_data2 = np.array([[1, 2, 3], [1, 2, 3]])
    data1 = paddle.to_tensor(np_data1)
    data2 = paddle.to_tensor(np_data2)
    paddle.distributed.all_gather(tensor_list, data2)
import numpy as np
import paddle
from paddle.distributed import ReduceOp
from paddle.distributed import init_parallel_env

paddle.set_device('gpu:%d'%paddle.distributed.ParallelEnv().dev_id)
init_parallel_env()
if paddle.distributed.ParallelEnv().local_rank == 0:
    np_data = np.array([[4, 5, 6], [4, 5, 6]])
else:
    np_data = np.array([[1, 2, 3], [1, 2, 3]])
data = paddle.to_tensor(np_data)
paddle.distributed.all_reduce(data)
out = data.numpy()
# [[5, 7, 9], [5, 7, 9]]
# required: distributed
import numpy as np
import paddle
from paddle.distributed import init_parallel_env

init_parallel_env()
out_tensor_list = []
if paddle.distributed.ParallelEnv().rank == 0:
    np_data1 = np.array([[1, 2, 3], [4, 5, 6]])
    np_data2 = np.array([[7, 8, 9], [10, 11, 12]])
else:
    np_data1 = np.array([[13, 14, 15], [16, 17, 18]])
    np_data2 = np.array([[19, 20, 21], [22, 23, 24]])
data1 = paddle.to_tensor(np_data1)
data2 = paddle.to_tensor(np_data2)
paddle.distributed.alltoall([data1, data2], out_tensor_list)
# out for rank 0: [[[1, 2, 3], [4, 5, 6]], [[13, 14, 15], [16, 17, 18]]]
# out for rank 1: [[[7, 8, 9], [10, 11, 12]], [[19, 20, 21], [22, 23, 24]]]
import paddle
from paddle.distributed import init_parallel_env

paddle.set_device('gpu:%d'%paddle.distributed.ParallelEnv().dev_id)
init_parallel_env()
paddle.distributed.barrier()
import numpy as np
import paddle
from paddle.distributed import init_parallel_env

paddle.set_device('gpu:%d'%paddle.distributed.ParallelEnv().dev_id)
init_parallel_env()
if paddle.distributed.ParallelEnv().local_rank == 0:
    np_data = np.array([[4, 5, 6], [4, 5, 6]])
else:
    np_data = np.array([[1, 2, 3], [1, 2, 3]])
data = paddle.to_tensor(np_data)
paddle.distributed.broadcast(data, 1)
out = data.numpy()
# [[1, 2, 3], [1, 2, 3]]
import paddle

sparse_feature_dim = 1024
embedding_size = 64

entry = paddle.distributed.CountFilterEntry(10)

input = paddle.static.data(name='ins', shape=[1], dtype='int64')

emb = paddle.static.nn.sparse_embedding((
    input=input,
    size=[sparse_feature_dim, embedding_size],
    is_test=False,
    entry=entry,
    param_attr=paddle.ParamAttr(name="SparseFeatFactors",
                               initializer=paddle.nn.initializer.Uniform()))
...
gid = paddle.distributed.new_group([2,4,6])
paddle.distributed.get_group(gid.id)
import paddle
import paddle.distributed as dist

# execute this command in terminal: export PADDLE_TRAINER_ID=0
print("The rank is %d" % dist.get_rank())
# The rank is 0
import paddle
import paddle.distributed as dist

# execute this command in terminal: export PADDLE_TRAINERS_NUM=4
print("The world_size is %d" % dist.get_world_size())
# The world_size is 4
import paddle
import multiprocessing
from contextlib import closing
import socket

port_set = set()

def find_free_port():
    def _free_port():
        with closing(socket.socket(socket.AF_INET,
            socket.SOCK_STREAM)) as s:
            s.bind(('', 0))
            return s.getsockname()[1]
    while True:
        port = _free_port()
        if port not in port_set:
            port_set.add(port)
            return port

def test_gloo_barrier(id, rank_num, server_endpoint):
    paddle.distributed.gloo_init_parallel_env(
        id, rank_num, server_endpoint)
    paddle.distributed.gloo_barrier()

def test_gloo_barrier_with_multiprocess(num_of_ranks):
    jobs = []
    server_endpoint = "127.0.0.1:%s" % (find_free_port())
    for id in range(num_of_ranks):
        p = multiprocessing.Process(
            target=test_gloo_barrier,
            args=(id, num_of_ranks, server_endpoint))
        jobs.append(p)
        p.start()
    for proc in jobs:
        proc.join()

if __name__ == '__main__':
    # Arg: number of ranks (processes)
    test_gloo_barrier_with_multiprocess(2)
import paddle
import multiprocessing
from contextlib import closing
import socket

port_set = set()

def find_free_port():
    def _free_port():
        with closing(socket.socket(socket.AF_INET,
            socket.SOCK_STREAM)) as s:
            s.bind(('', 0))
            return s.getsockname()[1]
    while True:
        port = _free_port()
        if port not in port_set:
            port_set.add(port)
            return port

def test_gloo_init(id, rank_num, server_endpoint):
    paddle.distributed.gloo_init_parallel_env(
        id, rank_num, server_endpoint)

def test_gloo_init_with_multiprocess(num_of_ranks):
    jobs = []
    server_endpoint = "127.0.0.1:%s" % (find_free_port())
    for id in range(num_of_ranks):
        p = multiprocessing.Process(
            target=test_gloo_init,
            args=(id, num_of_ranks, server_endpoint))
        jobs.append(p)
        p.start()
    for proc in jobs:
        proc.join()

if __name__ == '__main__':
    # Arg: number of ranks (processes)
    test_gloo_init_with_multiprocess(2)
import paddle
import multiprocessing
from contextlib import closing
import socket

port_set = set()

def find_free_port():
    def _free_port():
        with closing(socket.socket(socket.AF_INET,
            socket.SOCK_STREAM)) as s:
            s.bind(('', 0))
            return s.getsockname()[1]
    while True:
        port = _free_port()
        if port not in port_set:
            port_set.add(port)
            return port

def test_gloo_release(id, rank_num, server_endpoint):
    paddle.distributed.gloo_init_parallel_env(
        id, rank_num, server_endpoint)
    paddle.distributed.gloo_barrier()
    paddle.distributed.gloo_release()

def test_gloo_release_with_multiprocess(num_of_ranks):
    jobs = []
    server_endpoint = "127.0.0.1:%s" % (find_free_port())
    for id in range(num_of_ranks):
        p = multiprocessing.Process(
            target=test_gloo_release,
            args=(id, num_of_ranks, server_endpoint))
        jobs.append(p)
        p.start()
    for proc in jobs:
        proc.join()

if __name__ == '__main__':
    # Arg: number of ranks (processes)
    test_gloo_release_with_multiprocess(2)
.. code-block:: python
    # required: gpu
    import paddle
    import paddle.nn as nn
    import paddle.optimizer as opt
    import paddle.distributed as dist

    class LinearNet(nn.Layer):
        def __init__(self):
            super(LinearNet, self).__init__()
            self._linear1 = nn.Linear(10, 10)
            self._linear2 = nn.Linear(10, 1)

        def forward(self, x):
            return self._linear2(self._linear1(x))

    def train():
        # 1. initialize parallel environment
        dist.init_parallel_env()

        # 2. create data parallel layer & optimizer
        layer = LinearNet()
        dp_layer = paddle.DataParallel(layer)

        loss_fn = nn.MSELoss()
        adam = opt.Adam(
            learning_rate=0.001, parameters=dp_layer.parameters())

        # 3. run layer
        inputs = paddle.randn([10, 10], 'float32')
        outputs = dp_layer(inputs)
        labels = paddle.randn([10, 1], 'float32')
        loss = loss_fn(outputs, labels)

        loss.backward()

        adam.step()
        adam.clear_grad()

    if __name__ == '__main__':
        dist.spawn(train)
import paddle
paddle.enable_static()
dataset = paddle.distributed.InMemoryDataset()
import paddle
paddle.enable_static()

dataset = paddle.distributed.InMemoryDataset()
dataset.init(
    batch_size=1,
    thread_num=2,
    input_type=1,
    pipe_command="cat",
    use_var=[])
dataset._init_distributed_settings(
    parse_ins_id=True,
    parse_content=True,
    fea_eval=True,
    candidate_size=10000)
dataset.update_settings(batch_size=2)
import paddle
import os
paddle.enable_static()

with open("test_queue_dataset_run_a.txt", "w") as f:
    data = "2 1 2 2 5 4 2 2 7 2 1 3"
    f.write(data)
with open("test_queue_dataset_run_b.txt", "w") as f:
    data = "2 1 2 2 5 4 2 2 7 2 1 3"
    f.write(data)

slots = ["slot1", "slot2", "slot3", "slot4"]
slots_vars = []
for slot in slots:
    var = paddle.static.data(
        name=slot, shape=[None, 1], dtype="int64", lod_level=1)
    slots_vars.append(var)

dataset = paddle.distributed.InMemoryDataset()
dataset.init(
    batch_size=1,
    thread_num=2,
    input_type=1,
    pipe_command="cat",
    use_var=slots_vars)
dataset.set_filelist(
    ["test_queue_dataset_run_a.txt", "test_queue_dataset_run_b.txt"])
dataset.load_into_memory()

place = paddle.CPUPlace()
exe = paddle.static.Executor(place)
startup_program = paddle.static.Program()
main_program = paddle.static.Program()
exe.run(startup_program)

exe.train_from_dataset(main_program, dataset)

os.remove("./test_queue_dataset_run_a.txt")
os.remove("./test_queue_dataset_run_b.txt")
import paddle
paddle.enable_static()

dataset = paddle.distributed.InMemoryDataset()
slots = ["slot1", "slot2", "slot3", "slot4"]
slots_vars = []
for slot in slots:
    var = paddle.static.data(
        name=slot, shape=[None, 1], dtype="int64", lod_level=1)
    slots_vars.append(var)
dataset.init(
    batch_size=1,
    thread_num=2,
    input_type=1,
    pipe_command="cat",
    use_var=slots_vars)
filelist = ["a.txt", "b.txt"]
dataset.set_filelist(filelist)
dataset.load_into_memory()
import paddle
paddle.enable_static()

dataset = paddle.distributed.InMemoryDataset()
slots = ["slot1", "slot2", "slot3", "slot4"]
slots_vars = []
for slot in slots:
    var = paddle.static.data(
        name=slot, shape=[None, 1], dtype="int64", lod_level=1)
    slots_vars.append(var)
dataset.init(
    batch_size=1,
    thread_num=2,
    input_type=1,
    pipe_command="cat",
    use_var=slots_vars)
filelist = ["a.txt", "b.txt"]
dataset.set_filelist(filelist)
dataset.preload_into_memory()
dataset.wait_preload_done()
import paddle
paddle.enable_static()

dataset = paddle.distributed.InMemoryDataset()
slots = ["slot1", "slot2", "slot3", "slot4"]
slots_vars = []
for slot in slots:
    var = paddle.static.data(
        name=slot, shape=[None, 1], dtype="int64", lod_level=1)
    slots_vars.append(var)
dataset.init(
    batch_size=1,
    thread_num=2,
    input_type=1,
    pipe_command="cat",
    use_var=slots_vars)
filelist = ["a.txt", "b.txt"]
dataset.set_filelist(filelist)
dataset.preload_into_memory()
dataset.wait_preload_done()
import paddle
paddle.enable_static()

dataset = paddle.distributed.InMemoryDataset()
slots = ["slot1", "slot2", "slot3", "slot4"]
slots_vars = []
for slot in slots:
    var = paddle.static.data(
        name=slot, shape=[None, 1], dtype="int64", lod_level=1)
    slots_vars.append(var)
dataset.init(
    batch_size=1,
    thread_num=2,
    input_type=1,
    pipe_command="cat",
    use_var=slots_vars)
filelist = ["a.txt", "b.txt"]
dataset.set_filelist(filelist)
dataset.load_into_memory()
dataset.local_shuffle()
import paddle
paddle.enable_static()

dataset = paddle.distributed.InMemoryDataset()
slots = ["slot1", "slot2", "slot3", "slot4"]
slots_vars = []
for slot in slots:
    var = paddle.static.data(
        name=slot, shape=[None, 1], dtype="int64", lod_level=1)
    slots_vars.append(var)
dataset.init(
    batch_size=1,
    thread_num=2,
    input_type=1,
    pipe_command="cat",
    use_var=slots_vars)
filelist = ["a.txt", "b.txt"]
dataset.set_filelist(filelist)
dataset.load_into_memory()
dataset.global_shuffle()
import paddle
paddle.enable_static()

dataset = paddle.distributed.InMemoryDataset()
slots = ["slot1", "slot2", "slot3", "slot4"]
slots_vars = []
for slot in slots:
    var = paddle.static.data(
        name=slot, shape=[None, 1], dtype="int64", lod_level=1)
    slots_vars.append(var)
dataset.init(
    batch_size=1,
    thread_num=2,
    input_type=1,
    pipe_command="cat",
    use_var=slots_vars)
filelist = ["a.txt", "b.txt"]
dataset.set_filelist(filelist)
dataset.load_into_memory()
dataset.global_shuffle()
exe = paddle.static.Executor(paddle.CPUPlace())
startup_program = paddle.static.Program()
main_program = paddle.static.Program()
exe.run(startup_program)
exe.train_from_dataset(main_program, dataset)
dataset.release_memory()
import paddle
paddle.enable_static()

dataset = paddle.distributed.InMemoryDataset()
slots = ["slot1", "slot2", "slot3", "slot4"]
slots_vars = []
for slot in slots:
    var = paddle.static.data(
        name=slot, shape=[None, 1], dtype="int64", lod_level=1)
    slots_vars.append(var)
dataset.init(
    batch_size=1,
    thread_num=2,
    input_type=1,
    pipe_command="cat",
    use_var=slots_vars)
filelist = ["a.txt", "b.txt"]
dataset.set_filelist(filelist)
dataset.load_into_memory()
print dataset.get_memory_data_size()
import paddle
paddle.enable_static()

dataset = paddle.distributed.InMemoryDataset()
dataset = paddle.distributed.InMemoryDataset()
slots = ["slot1", "slot2", "slot3", "slot4"]
slots_vars = []
for slot in slots:
    var = paddle.static.data(
        name=slot, shape=[None, 1], dtype="int64", lod_level=1)
    slots_vars.append(var)
dataset.init(
    batch_size=1,
    thread_num=2,
    input_type=1,
    pipe_command="cat",
    use_var=slots_vars)
filelist = ["a.txt", "b.txt"]
dataset.set_filelist(filelist)
dataset.load_into_memory()
dataset.global_shuffle()
print dataset.get_shuffle_data_size()
import paddle
paddle.enable_static()

dataset = paddle.distributed.InMemoryDataset()
dataset._init_distributed_settings(fea_eval=True)
slots = ["slot1", "slot2", "slot3", "slot4"]
slots_vars = []
for slot in slots:
    var = paddle.static.data(
        name=slot, shape=[None, 1], dtype="int64", lod_level=1)
    slots_vars.append(var)
dataset.init(
    batch_size=1,
    thread_num=2,
    input_type=1,
    pipe_command="cat",
    use_var=slots_vars)
filelist = ["a.txt", "b.txt"]
dataset.set_filelist(filelist)
dataset.load_into_memory()
dataset.slots_shuffle(['slot1'])
import paddle
dataset = paddle.distributed.fleet.DatasetBase()
dataset.set_filelist(['a.txt', 'b.txt'])
.. code-block:: bash
    :name: code-block-example-bash5

   # To simulate distributed environment using single node, e.g., 2 servers and 4 workers, each worker use single gpu.

    export CUDA_VISIBLE_DEVICES=0,1,2,3
    python -m paddle.distributed.launch --server_num=2 --worker_num=4 train.py --lr=0.01
import paddle

paddle.distributed.init_parallel_env()
tindata = paddle.randn(shape=[2, 3])
gp = paddle.distributed.new_group([2,4,6])
paddle.distributed.all_reduce(tindata, group=gp, use_calc_stream=False)
import paddle
import paddle.distributed as dist

def train():
    # 1. initialize parallel environment
    dist.init_parallel_env()

    # 2. get current ParallelEnv
    parallel_env = dist.ParallelEnv()
    print("rank: ", parallel_env.rank)
    print("world_size: ", parallel_env.world_size)

    # print result in process 1:
    # rank: 1
    # world_size: 2
    # print result in process 2:
    # rank: 2
    # world_size: 2

if __name__ == '__main__':
    # 1. start by ``paddle.distributed.spawn`` (default)
    dist.spawn(train, nprocs=2)
    # 2. start by ``paddle.distributed.launch``
    # train()
# execute this command in terminal: export PADDLE_TRAINER_ID=0
import paddle.distributed as dist

env = dist.ParallelEnv()
print("The rank is %d" % env.rank)
# The rank is 0
# execute this command in terminal: export PADDLE_TRAINERS_NUM=4
import paddle.distributed as dist

env = dist.ParallelEnv()
print("The world_size is %d" % env.world_size)
# The world_size is 4
# execute this command in terminal: export FLAGS_selected_gpus=1
import paddle.distributed as dist

env = dist.ParallelEnv()
print("The device id are %d" % env.device_id)
# The device id are 1
# execute this command in terminal: export PADDLE_CURRENT_ENDPOINT=127.0.0.1:6170
import paddle.distributed as dist

env = dist.ParallelEnv()
print("The current endpoint are %s" % env.current_endpoint)
# The current endpoint are 127.0.0.1:6170
# execute this command in terminal: export PADDLE_TRAINER_ENDPOINTS=127.0.0.1:6170,127.0.0.1:6171
import paddle.distributed as dist

env = dist.ParallelEnv()
print("The trainer endpoints are %s" % env.trainer_endpoints)
# The trainer endpoints are ['127.0.0.1:6170', '127.0.0.1:6171']
# execute this command in terminal: export FLAGS_nccl_nrings=1
import paddle.distributed as dist

env = dist.ParallelEnv()
print("The nrings is %d" % env.nrings)
# the number of ring is 1
# execute this command in terminal: export PADDLE_TRAINER_ID=0
import paddle.distributed as dist

env = dist.ParallelEnv()
print("The rank is %d" % env.rank)
# The rank is 0
# execute this command in terminal: export PADDLE_TRAINERS_NUM=4
import paddle.distributed as dist

env = dist.ParallelEnv()
print("The world_size is %d" % env.world_size)
# The world_size is 4
# execute this command in terminal: export FLAGS_selected_gpus=1
import paddle.distributed as dist

env = dist.ParallelEnv()
print("The device id are %d" % env.device_id)
# The device id are 1
import paddle

sparse_feature_dim = 1024
embedding_size = 64

entry = paddle.distributed.ProbabilityEntry(0.1)

input = paddle.static.data(name='ins', shape=[1], dtype='int64')

emb = paddle.static.nn.sparse_embedding((
    input=input,
    size=[sparse_feature_dim, embedding_size],
    is_test=False,
    entry=entry,
    param_attr=paddle.ParamAttr(name="SparseFeatFactors",
                               initializer=paddle.nn.initializer.Uniform()))
import paddle
dataset = paddle.distributed.QueueDataset()
import paddle
dataset = paddle.distributed.fleet.DatasetBase()
dataset.set_filelist(['a.txt', 'b.txt'])
# required: distributed
import paddle
from paddle.distributed import init_parallel_env

init_parallel_env()
if paddle.distributed.ParallelEnv().rank == 0:
    data = paddle.to_tensor([7, 8, 9])
    paddle.distributed.send(data, dst=1)
else:
    data = paddle.to_tensor([1,2,3])
    paddle.distributed.recv(data, src=0)
out = data.numpy()
import numpy as np
import paddle
from paddle.distributed import init_parallel_env

paddle.set_device('gpu:%d'%paddle.distributed.ParallelEnv().dev_id)
init_parallel_env()
if paddle.distributed.ParallelEnv().local_rank == 0:
    np_data = np.array([[4, 5, 6], [4, 5, 6]])
else:
    np_data = np.array([[1, 2, 3], [1, 2, 3]])
data = paddle.to_tensor(np_data)
paddle.distributed.reduce(data, 0)
out = data.numpy()
# [[5, 7, 9], [5, 7, 9]]
import numpy as np
import paddle
from paddle.distributed import ReduceOp
from paddle.distributed import init_parallel_env

paddle.set_device('gpu:%d'%paddle.distributed.ParallelEnv().dev_id)
init_parallel_env()
if paddle.distributed.ParallelEnv().local_rank == 0:
    np_data = np.array([[4, 5, 6], [4, 5, 6]])
else:
    np_data = np.array([[1, 2, 3], [1, 2, 3]])
data = paddle.to_tensor(np_data)
paddle.distributed.all_reduce(data, op=ReduceOp.SUM)
out = data.numpy()
# [[5, 7, 9], [5, 7, 9]]
import numpy as np
import paddle
from paddle.distributed import init_parallel_env

# required: gpu

paddle.set_device('gpu:%d'%paddle.distributed.ParallelEnv().dev_id)
init_parallel_env()
if paddle.distributed.ParallelEnv().local_rank == 0:
    np_data1 = np.array([7, 8, 9])
    np_data2 = np.array([10, 11, 12])
else:
    np_data1 = np.array([1, 2, 3])
    np_data2 = np.array([4, 5, 6])
data1 = paddle.to_tensor(np_data1)
data2 = paddle.to_tensor(np_data2)
if paddle.distributed.ParallelEnv().local_rank == 0:
    paddle.distributed.scatter(data1, src=1)
else:
    paddle.distributed.scatter(data1, tensor_list=[data1, data2], src=1)
out = data1.numpy()
# required: distributed
import paddle
from paddle.distributed import init_parallel_env

init_parallel_env()
if paddle.distributed.ParallelEnv().rank == 0:
    data = paddle.to_tensor([7, 8, 9])
    paddle.distributed.send(data, dst=1)
else:
    data = paddle.to_tensor([1,2,3])
    paddle.distributed.recv(data, src=0)
out = data.numpy()
from __future__ import print_function

import paddle
import paddle.nn as nn
import paddle.optimizer as opt
import paddle.distributed as dist

class LinearNet(nn.Layer):
    def __init__(self):
        super(LinearNet, self).__init__()
        self._linear1 = nn.Linear(10, 10)
        self._linear2 = nn.Linear(10, 1)

    def forward(self, x):
        return self._linear2(self._linear1(x))

def train(print_result=False):
    # 1. initialize parallel environment
    dist.init_parallel_env()

    # 2. create data parallel layer & optimizer
    layer = LinearNet()
    dp_layer = paddle.DataParallel(layer)

    loss_fn = nn.MSELoss()
    adam = opt.Adam(
        learning_rate=0.001, parameters=dp_layer.parameters())

    # 3. run layer
    inputs = paddle.randn([10, 10], 'float32')
    outputs = dp_layer(inputs)
    labels = paddle.randn([10, 1], 'float32')
    loss = loss_fn(outputs, labels)

    if print_result is True:
        print("loss:", loss.numpy())

    loss.backward()

    adam.step()
    adam.clear_grad()

# Usage 1: only pass function.
# If your training method no need any argument, and
# use all visible devices for parallel training.
if __name__ == '__main__':
    dist.spawn(train)

# Usage 2: pass function and arguments.
# If your training method need some arguments, and
# use all visible devices for parallel training.
if __name__ == '__main__':
    dist.spawn(train, args=(True,))

# Usage 3: pass function, arguments and nprocs.
# If your training method need some arguments, and
# only use part of visible devices for parallel training.
# If your machine hold 8 cards {0,1,2,3,4,5,6,7},
# this case will use cards {0,1}; If you set
# CUDA_VISIBLE_DEVICES=4,5,6,7, this case will use
# cards {4,5}
if __name__ == '__main__':
    dist.spawn(train, args=(True,), nprocs=2)

# Usage 4: pass function, arguments, nprocs and gpus.
# If your training method need some arguments, and
# only use part of visible devices for parallel training,
# but you can't set your machine's environment variable
# CUDA_VISIBLE_DEVICES, such as it is None or all cards
# {0,1,2,3,4,5,6,7}, you can pass `gpus` to
# select the GPU cards you want to use. For example,
# this case will use cards {4,5} if your machine hold 8 cards.
if __name__ == '__main__':
    dist.spawn(train, args=(True,), nprocs=2, gpus='4,5')
# required: distributed
import paddle
import paddle.distributed.fleet as fleet

paddle.enable_static()
paddle.set_device('gpu:%d'%paddle.distributed.ParallelEnv().dev_id)
fleet.init(is_collective=True)
data = paddle.randint(0, 8, shape=[10,4])
emb_out = paddle.distributed.split(
    data,
    (8, 8),
    operation="embedding",
    num_partitions=2)
import paddle

paddle.distributed.init_parallel_env()
tindata = paddle.randn(shape=[2, 3])
paddle.distributed.all_reduce(tindata, use_calc_stream=True)
paddle.distributed.wait(tindata)
import paddle
from paddle.distribution import Categorical

paddle.seed(100) # on CPU device
x = paddle.rand([6])
print(x)
# [0.5535528  0.20714243 0.01162981
#  0.51577556 0.36369765 0.2609165 ]

paddle.seed(200) # on CPU device
y = paddle.rand([6])
print(y)
# [0.77663314 0.90824795 0.15685187
#  0.04279523 0.34468332 0.7955718 ]

cat = Categorical(x)
cat2 = Categorical(y)

paddle.seed(1000) # on CPU device
cat.sample([2,3])
# [[0, 0, 5],
#  [3, 4, 5]]

cat.entropy()
# [1.77528]

cat.kl_divergence(cat2)
# [0.071952]

value = paddle.to_tensor([2,1,3])
cat.probs(value)
# [0.00608027 0.108298 0.269656]

cat.log_prob(value)
# [-5.10271 -2.22287 -1.31061]
import paddle
from paddle.distribution import Categorical

paddle.seed(100) # on CPU device
x = paddle.rand([6])
print(x)
# [0.5535528  0.20714243 0.01162981
#  0.51577556 0.36369765 0.2609165 ]

cat = Categorical(x)

paddle.seed(1000) # on CPU device
cat.sample([2,3])
# [[0, 0, 5],
#  [3, 4, 5]]
import paddle
from paddle.distribution import Categorical

paddle.seed(100) # on CPU device
x = paddle.rand([6])
print(x)
# [0.5535528  0.20714243 0.01162981
#  0.51577556 0.36369765 0.2609165 ]

paddle.seed(200) # on CPU device
y = paddle.rand([6])
print(y)
# [0.77663314 0.90824795 0.15685187
#  0.04279523 0.34468332 0.7955718 ]

cat = Categorical(x)
cat2 = Categorical(y)

cat.kl_divergence(cat2)
# [0.071952]
import paddle
from paddle.distribution import Categorical

paddle.seed(100) # on CPU device
x = paddle.rand([6])
print(x)
# [0.5535528  0.20714243 0.01162981
#  0.51577556 0.36369765 0.2609165 ]

cat = Categorical(x)

cat.entropy()
# [1.77528]
import paddle
from paddle.distribution import Categorical

paddle.seed(100) # on CPU device
x = paddle.rand([6])
print(x)
# [0.5535528  0.20714243 0.01162981
#  0.51577556 0.36369765 0.2609165 ]

cat = Categorical(x)

value = paddle.to_tensor([2,1,3])
cat.probs(value)
# [0.00608027 0.108298 0.269656]
import paddle
from paddle.distribution import Categorical

paddle.seed(100) # on CPU device
x = paddle.rand([6])
print(x)
# [0.5535528  0.20714243 0.01162981
#  0.51577556 0.36369765 0.2609165 ]

cat = Categorical(x)

value = paddle.to_tensor([2,1,3])
cat.log_prob(value)
# [-5.10271 -2.22287 -1.31061]
import paddle
from paddle.distribution import Normal

# Define a single scalar Normal distribution.
dist = Normal(loc=0., scale=3.)
# Define a batch of two scalar valued Normals.
# The first has mean 1 and standard deviation 11, the second 2 and 22.
dist = Normal(loc=[1., 2.], scale=[11., 22.])
# Get 3 samples, returning a 3 x 2 tensor.
dist.sample([3])

# Define a batch of two scalar valued Normals.
# Both have mean 1, but different standard deviations.
dist = Normal(loc=1., scale=[11., 22.])

# Complete example
value_tensor = paddle.to_tensor([0.8], dtype="float32")

normal_a = Normal([0.], [1.])
normal_b = Normal([0.5], [2.])
sample = normal_a.sample([2])
# a random tensor created by normal distribution with shape: [2, 1]
entropy = normal_a.entropy()
# [1.4189385] with shape: [1]
lp = normal_a.log_prob(value_tensor)
# [-1.2389386] with shape: [1]
p = normal_a.probs(value_tensor)
# [0.28969154] with shape: [1]
kl = normal_a.kl_divergence(normal_b)
# [0.34939718] with shape: [1]
import paddle
from paddle.distribution import Uniform

# Without broadcasting, a single uniform distribution [3, 4]:
u1 = Uniform(low=3.0, high=4.0)
# 2 distributions [1, 3], [2, 4]
u2 = Uniform(low=[1.0, 2.0], high=[3.0, 4.0])
# 4 distributions
u3 = Uniform(low=[[1.0, 2.0], [3.0, 4.0]],
          high=[[1.5, 2.5], [3.5, 4.5]])

# With broadcasting:
u4 = Uniform(low=3.0, high=[5.0, 6.0, 7.0])

# Complete example
value_tensor = paddle.to_tensor([0.8], dtype="float32")

uniform = Uniform([0.], [2.])

sample = uniform.sample([2])
# a random tensor created by uniform distribution with shape: [2, 1]
entropy = uniform.entropy()
# [0.6931472] with shape: [1]
lp = uniform.log_prob(value_tensor)
# [-0.6931472] with shape: [1]
p = uniform.probs(value_tensor)
# [0.5] with shape: [1]
import numpy as np
import paddle

x = np.exp(3j * np.pi * np.arange(7) / 7)
xp = paddle.to_tensor(x)
fft_xp = paddle.fft.fft(xp).numpy()
print(fft_xp)
#  [1.+1.25396034e+00j 1.+4.38128627e+00j 1.-4.38128627e+00j
#   1.-1.25396034e+00j 1.-4.81574619e-01j 1.+8.88178420e-16j
#   1.+4.81574619e-01j]
import numpy as np
import paddle

x = np.mgrid[:2, :2][1]
xp = paddle.to_tensor(x)
fft2_xp = paddle.fft.fft2(xp).numpy()
print(fft2_xp)
#  [[ 2.+0.j -2.+0.j]
#   [ 0.+0.j  0.+0.j]]
f = [0, 1, ...,   n/2-1,     -n/2, ..., -1] / (d*n)   if n is even
f = [0, 1, ..., (n-1)/2, -(n-1)/2, ..., -1] / (d*n)   if n is odd
import numpy as np
import paddle

x = np.array([3, 1, 2, 2, 3], dtype=float)
scalar_temp = 0.5
n = x.size
fftfreq_xp = paddle.fft.fftfreq(n, d=scalar_temp)
print(fftfreq_xp)

#  Tensor(shape=[5], dtype=float32, place=CUDAPlace(0), stop_gradient=True,
#           [ 0.        ,  0.40000001,  0.80000001, -0.80000001, -0.40000001])
import numpy as np
import paddle

x = np.mgrid[:4, :4, :4][1]
xp = paddle.to_tensor(x)
fftn_xp = paddle.fft.fftn(xp, axes=(1, 2)).numpy()
print(fftn_xp)
#  [[[24.+0.j  0.+0.j  0.+0.j  0.-0.j]
#   [-8.+8.j  0.+0.j  0.+0.j  0.-0.j]
#   [-8.+0.j  0.+0.j  0.+0.j  0.-0.j]
#   [-8.-8.j  0.+0.j  0.+0.j  0.-0.j]]
#   [[24.+0.j  0.+0.j  0.+0.j  0.-0.j]
#   [-8.+8.j  0.+0.j  0.+0.j  0.-0.j]
#   [-8.+0.j  0.+0.j  0.+0.j  0.-0.j]
#   [-8.-8.j  0.+0.j  0.+0.j  0.-0.j]]
#   [[24.+0.j  0.+0.j  0.+0.j  0.-0.j]
#   [-8.+8.j  0.+0.j  0.+0.j  0.-0.j]
#   [-8.+0.j  0.+0.j  0.+0.j  0.-0.j]
#   [-8.-8.j  0.+0.j  0.+0.j  0.-0.j]]
#   [[24.+0.j  0.+0.j  0.+0.j  0.-0.j]
#   [-8.+8.j  0.+0.j  0.+0.j  0.-0.j]
#   [-8.+0.j  0.+0.j  0.+0.j  0.-0.j]
#   [-8.-8.j  0.+0.j  0.+0.j  0.-0.j]]]
import numpy as np
import paddle

x = np.array([3, 1, 2, 2, 3], dtype=float)
n = x.size
fftfreq_xp = paddle.fft.fftfreq(n, d=0.3)
res = paddle.fft.fftshift(fftfreq_xp).numpy()
print(res)
#  [-1.3333334 -0.6666667  0.         0.6666667  1.3333334]
import numpy as np
import paddle

x = np.array([1, -1j, -1])
xp = paddle.to_tensor(x)
hfft_xp = paddle.fft.hfft(xp).numpy()
print(hfft_xp)
#  [0. 0. 0. 4.]
import numpy as np
import paddle

x = (np.array([[3,2,3],[2, 2, 3]]) + 1j * np.array([[3,2,3],[2, 2, 3]])).astype(np.complex128)
xp = paddle.to_tensor(x)
hfft2_xp = paddle.fft.hfft2(xp).numpy()
print(hfft2_xp)
#  [[19.  7.  3. -9.]
#   [ 1.  1.  1.  1.]]
import numpy as np
import paddle

x = (np.array([2, 2, 3]) + 1j * np.array([2, 2, 3])).astype(np.complex128)
xp = paddle.to_tensor(x)
hfftn_xp = paddle.fft.hfftn(xp).numpy()
print(hfftn_xp)
#  [ 9.  3.  1. -5.]
import numpy as np
import paddle

x = np.exp(3j * np.pi * np.arange(7) / 7)
xp = paddle.to_tensor(x)
ifft_xp = paddle.fft.ifft(xp).numpy()
print(ifft_xp)
#  [0.14285714+1.79137191e-01j 0.14285714+6.87963741e-02j
#   0.14285714+1.26882631e-16j 0.14285714-6.87963741e-02j
#   0.14285714-1.79137191e-01j 0.14285714-6.25898038e-01j
#   0.14285714+6.25898038e-01j]
import numpy as np
import paddle

x = np.mgrid[:2, :2][1]
xp = paddle.to_tensor(x)
ifft2_xp = paddle.fft.ifft2(xp).numpy()
print(ifft2_xp)
#  [[ 0.5+0.j -0.5+0.j]
#   [ 0. +0.j  0. +0.j]]
import numpy as np
import paddle

x = np.eye(3)
xp = paddle.to_tensor(x)
ifftn_xp = paddle.fft.ifftn(xp, axes=(1,)).numpy()
print(ifftn_xp)

#   [[ 0.33333333+0.j          0.33333333+0.j          0.33333333-0.j        ]
#   [ 0.33333333+0.j         -0.16666667+0.28867513j -0.16666667-0.28867513j]
#   [ 0.33333333+0.j         -0.16666667-0.28867513j -0.16666667+0.28867513j]]
import numpy as np
import paddle

x = np.array([3, 1, 2, 2, 3], dtype=float)
n = x.size
fftfreq_xp = paddle.fft.fftfreq(n, d=0.3)
res = paddle.fft.ifftshift(fftfreq_xp).numpy()
print(res)
#  [ 1.3333334 -1.3333334 -0.6666667  0.         0.6666667]
import numpy as np
import paddle

x = np.mgrid[:5, :5][0].astype(np.float64)
xp = paddle.to_tensor(x)
ihfft2_xp = paddle.fft.ihfft2(xp).numpy()
print(ihfft2_xp)
# [[ 2. +0.j          0. +0.j          0. +0.j        ]
#  [-0.5-0.68819096j  0. +0.j          0. +0.j        ]
#  [-0.5-0.16245985j  0. +0.j          0. +0.j        ]
#  [-0.5+0.16245985j  0. +0.j          0. +0.j        ]
#  [-0.5+0.68819096j  0. +0.j          0. +0.j        ]]
import numpy as np
import paddle

x = np.array([1, -1j, -1])
xp = paddle.to_tensor(x)
irfft_xp = paddle.fft.irfft(xp).numpy()
print(irfft_xp)
#  [0. 1. 0. 0.]
import numpy as np
import paddle

x = (np.array([[3,2,3],[2, 2, 3]]) + 1j * np.array([[3,2,3],[2, 2, 3]])).astype(np.complex128)
xp = paddle.to_tensor(x)
irfft2_xp = paddle.fft.irfft2(xp).numpy()
print(irfft2_xp)
#  [[ 2.375 -1.125  0.375  0.875]
#   [ 0.125  0.125  0.125  0.125]]
import numpy as np
import paddle

x = (np.array([2, 2, 3]) + 1j * np.array([2, 2, 3])).astype(np.complex128)
xp = paddle.to_tensor(x)
irfftn_xp = paddle.fft.irfftn(xp).numpy()
print(irfftn_xp)
#  [ 2.25 -1.25  0.25  0.75]
.. code-block:: python
    import paddle
    import numpy as np

    x = paddle.to_tensor(np.mgrid[:5, :5][0].astype(np.float32))
    print(paddle.fft.rfft2(x))
    # Tensor(shape=[5, 3], dtype=complex64, place=CUDAPlace(0), stop_gradient=True,
    #        [[ (50+0j)                                        ,  (1.1920928955078125e-07+0j)                    ,  0j                                             ],
    #         [(-12.5+17.204774856567383j)                     , (-9.644234211236835e-08+7.006946134424652e-08j) ,  0j                                             ],
    #         [(-12.500000953674316+4.061495304107666j)        , (3.6837697336977726e-08-1.1337477445749755e-07j),  0j                                             ],
    #         [(-12.500000953674316-4.061495304107666j)        , (3.6837697336977726e-08+1.1337477445749755e-07j),  0j                                             ],
    #         [(-12.5-17.204774856567383j)                     , (-9.644234211236835e-08-7.006946134424652e-08j) ,  0j                                             ]])
f = [0, 1, ...,     n/2-1,     n/2] / (d*n)   if n is even
f = [0, 1, ..., (n-1)/2-1, (n-1)/2] / (d*n)   if n is odd
import numpy as np
import paddle

x = np.array([3, 1, 2, 2, 3], dtype=float)
scalar_temp = 0.3
n = x.size
rfftfreq_xp = paddle.fft.rfftfreq(n, d=scalar_temp)
print(rfftfreq_xp)

#  Tensor(shape=[3], dtype=float32, place=CUDAPlace(0), stop_gradient=True,
#           [0.        , 0.66666669, 1.33333337])
import paddle
import paddle.fluid as fluid
import numpy as np
paddle.enable_static()

# Creates a variable with fixed size [3, 2, 1]
# User can only feed data of the same shape to x
x = fluid.data(name='x', shape=[3, 2, 1], dtype='float32')

# Creates a variable with changeable batch size -1.
# Users can feed data of any batch size into y,
# but size of each data sample has to be [2, 1]
y = fluid.data(name='y', shape=[-1, 2, 1], dtype='float32')

z = x + y

# In this example, we will feed x and y with np-ndarray "1"
# and fetch z, like implementing "1 + 1 = 2" in PaddlePaddle
feed_data = np.ones(shape=[3, 2, 1], dtype=np.float32)

exe = fluid.Executor(fluid.CPUPlace())
out = exe.run(fluid.default_main_program(),
              feed={
                  'x': feed_data,
                  'y': feed_data
              },
              fetch_list=[z.name])

# np-ndarray of shape=[3, 2, 1], dtype=float32, whose elements are 2
print(out)
import paddle

paddle.hub.help('lyuwenyu/paddlehub_demo:main', model='MM', source='github')
import paddle

paddle.hub.list('lyuwenyu/paddlehub_demo:main', source='github', force_reload=False)
import paddle
paddle.hub.load('lyuwenyu/paddlehub_demo:main', model='MM', source='github')
import paddle

x = paddle.to_tensor([[0, 2, 3], [1, 4, 5], [2, 6, 7]], dtype="float32")
indexes = paddle.to_tensor([[0, 1], [1, 2], [2, 1], [0, 0]], dtype="int32")
src_index = indexes[:, 0]
dst_index = indexes[:, 1]
out = paddle.incubate.graph_send_recv(x, src_index, dst_index, pool_type="sum")
# Outputs: [[0., 2., 3.], [2., 8., 10.], [1., 4., 5.]]
import numpy as np
import paddle
import paddle.nn as nn

BATCH_SIZE = 16
BATCH_NUM = 4
EPOCH_NUM = 4

IMAGE_SIZE = 784
CLASS_NUM = 10
# define a random dataset
class RandomDataset(paddle.io.Dataset):
    def __init__(self, num_samples):
        self.num_samples = num_samples

    def __getitem__(self, idx):
        image = np.random.random([IMAGE_SIZE]).astype('float32')
        label = np.random.randint(0, CLASS_NUM - 1,
                                (1, )).astype('int64')
        return image, label

    def __len__(self):
        return self.num_samples

class LinearNet(nn.Layer):
    def __init__(self):
        super(LinearNet, self).__init__()
        self._linear = nn.Linear(IMAGE_SIZE, CLASS_NUM)
        self.bias = self._linear.bias

    @paddle.jit.to_static
    def forward(self, x):
        return self._linear(x)

def train(layer, loader, loss_fn, opt):
    for epoch_id in range(EPOCH_NUM):
        for batch_id, (image, label) in enumerate(loader()):
            out = layer(image)
            loss = loss_fn(out, label)
            loss.backward()
            opt.step()
            opt.clear_grad()
            print("Train Epoch {} batch {}: loss = {}".format(
                epoch_id, batch_id, np.mean(loss.numpy())))

layer = LinearNet()
loss_fn = nn.CrossEntropyLoss()
optimizer = paddle.optimizer.SGD(learning_rate=0.1, parameters=layer.parameters())
lookahead = paddle.incubate.LookAhead(optimizer, alpha=0.2, k=5)

# create data loader
dataset = RandomDataset(BATCH_NUM * BATCH_SIZE)
loader = paddle.io.DataLoader(
    dataset,
    batch_size=BATCH_SIZE,
    shuffle=True,
    drop_last=True,
    num_workers=2)

train(layer, loader, loss_fn, lookahead)
import paddle
import numpy as np
inp = paddle.to_tensor(np.random.random([1, 10]).astype('float32'))
linear = paddle.nn.Linear(10, 1)
out = linear(inp)
loss = paddle.mean(out)
sgd = paddle.optimizer.SGD(learning_rate=0.1,parameters=linear.parameters())
lookahead = paddle.incubate.LookAhead(sgd, alpha=0.2, k=5)
loss.backward()
lookahead.step()
lookahead.clear_grad()
import paddle
import numpy as np
inp = paddle.to_tensor(np.random.random([1, 10]).astype('float32'))
linear = paddle.nn.Linear(10, 1)
out = linear(inp)
loss = paddle.mean(out)
sgd = paddle.optimizer.SGD(learning_rate=0.1,parameters=linear.parameters())
lookahead = paddle.incubate.LookAhead(sgd, alpha=0.2, k=5)
loss.backward()
lookahead.minimize(loss)
lookahead.clear_grad()
import paddle
import numpy as np

inp = np.random.uniform(-0.1, 0.1, [10, 10]).astype("float32")
linear = paddle.nn.Linear(10, 10)
inp = paddle.to_tensor(inp)
out = linear(inp)
loss = paddle.mean(out)
optimizer = paddle.optimizer.Adam(learning_rate=0.1,
        parameters=linear.parameters())
params_grads = optimizer.backward(loss)
optimizer.apply_gradients(params_grads)
import paddle
import numpy as np
value = np.arange(26).reshape(2, 13).astype("float32")
a = paddle.to_tensor(value)
linear = paddle.nn.Linear(13, 5)
# This can be any optimizer supported by dygraph.
adam = paddle.optimizer.Adam(learning_rate = 0.01,
                            parameters = linear.parameters())
out = linear(a)
out.backward()
adam.step()
adam.clear_grad()
import numpy as np
import paddle

value = np.arange(26).reshape(2, 13).astype("float32")
a = paddle.to_tensor(value)
linear = paddle.nn.Linear(13, 5)
# This can be any optimizer supported by dygraph.
adam = paddle.optimizer.Adam(learning_rate = 0.01,
                            parameters = linear.parameters())
out = linear(a)
out.backward()
adam.step()
adam.clear_grad()
# train on default dynamic graph mode
import paddle
import numpy as np
emb = paddle.nn.Embedding(10, 3)

## example1: LRScheduler is not used, return the same value is all the same
adam = paddle.optimizer.Adam(0.01, parameters = emb.parameters())
for batch in range(10):
    input = paddle.randint(low=0, high=5, shape=[5])
    out = emb(input)
    out.backward()
    print("Learning rate of step{}: {}".format(batch, adam.get_lr())) # 0.01
    adam.step()

## example2: StepDecay is used, return the scheduled learning rate
scheduler = paddle.optimizer.lr.StepDecay(learning_rate=0.5, step_size=2, gamma=0.1)
adam = paddle.optimizer.Adam(scheduler, parameters = emb.parameters())
for batch in range(10):
    input = paddle.randint(low=0, high=5, shape=[5])
    out = emb(input)
    out.backward()
    print("Learning rate of step{}: {}".format(batch, adam.get_lr())) # 0.5->0.05...
    adam.step()
    scheduler.step()

# train on static graph mode
paddle.enable_static()
main_prog = paddle.static.Program()
start_prog = paddle.static.Program()
with paddle.static.program_guard(main_prog, start_prog):
    x = paddle.static.data(name='x', shape=[None, 10])
    z = paddle.static.nn.fc(x, 100)
    loss = paddle.mean(z)
    scheduler = paddle.optimizer.lr.StepDecay(learning_rate=0.5, step_size=2, gamma=0.1)
    adam = paddle.optimizer.Adam(learning_rate=scheduler)
    adam.minimize(loss)

exe = paddle.static.Executor()
exe.run(start_prog)
for batch in range(10):
    print("Learning rate of step{}: {}", adam.get_lr())     # 0.5->0.05->0.005...
    out = exe.run(main_prog, feed={'x': np.random.randn(3, 10).astype('float32')})
    scheduler.step()
import paddle
linear = paddle.nn.Linear(10, 10)

adam = paddle.optimizer.Adam(0.1, parameters=linear.parameters())

# set learning rate manually by python float value
lr_list = [0.2, 0.3, 0.4, 0.5, 0.6]
for i in range(5):
    adam.set_lr(lr_list[i])
    lr = adam.get_lr()
    print("current lr is {}".format(lr))
# Print:
#    current lr is 0.2
#    current lr is 0.3
#    current lr is 0.4
#    current lr is 0.5
#    current lr is 0.6
import paddle

emb = paddle.nn.Embedding(10, 10)

layer_state_dict = emb.state_dict()
paddle.save(layer_state_dict, "emb.pdparams")

scheduler = paddle.optimizer.lr.NoamDecay(
    d_model=0.01, warmup_steps=100, verbose=True)
adam = paddle.optimizer.Adam(
    learning_rate=scheduler,
    parameters=emb.parameters())
opt_state_dict = adam.state_dict()
paddle.save(opt_state_dict, "adam.pdopt")

opti_state_dict = paddle.load("adam.pdopt")
adam.set_state_dict(opti_state_dict)
import paddle
emb = paddle.nn.Embedding(10, 10)

adam = paddle.optimizer.Adam(0.001, parameters=emb.parameters())
state_dict = adam.state_dict()
if num_accumulates >= min_average_window and num_accumulates >= min(max_average_window, num_updates * average_window_rate):
    num_accumulates = 0
import numpy as np
import paddle
import paddle.nn as nn
import paddle.optimizer as opt

BATCH_SIZE = 16
BATCH_NUM = 4
EPOCH_NUM = 4

IMAGE_SIZE = 784
CLASS_NUM = 10

# define a random dataset
class RandomDataset(paddle.io.Dataset):
    def __init__(self, num_samples):
        self.num_samples = num_samples

    def __getitem__(self, idx):
        image = np.random.random([IMAGE_SIZE]).astype('float32')
        label = np.random.randint(0, CLASS_NUM - 1, (1, )).astype('int64')
        return image, label

    def __len__(self):
        return self.num_samples

class LinearNet(nn.Layer):
    def __init__(self):
        super(LinearNet, self).__init__()
        self._linear = nn.Linear(IMAGE_SIZE, CLASS_NUM)
        self.bias = self._linear.bias

    @paddle.jit.to_static
    def forward(self, x):
        return self._linear(x)

def train(layer, loader, loss_fn, opt, model_average):
    for epoch_id in range(EPOCH_NUM):
        for batch_id, (image, label) in enumerate(loader()):
            out = layer(image)
            loss = loss_fn(out, label)
            loss.backward()
            opt.step()
            model_average.step()
            opt.clear_grad()
            model_average.clear_grad()
            print("Train Epoch {} batch {}: loss = {}, bias = {}".format(
                epoch_id, batch_id, np.mean(loss.numpy()), layer.bias.numpy()))
def evaluate(layer, loader, loss_fn):
    for batch_id, (image, label) in enumerate(loader()):
        out = layer(image)
        loss = loss_fn(out, label)
        loss.backward()
        print("Evaluate batch {}: loss = {}, bias = {}".format(
            batch_id, np.mean(loss.numpy()), layer.bias.numpy()))

# create network
layer = LinearNet()
loss_fn = nn.CrossEntropyLoss()
optimizer = opt.Momentum(learning_rate=0.2, momentum=0.1, parameters=layer.parameters())
model_average = paddle.incubate.ModelAverage(0.15,
                                            parameters=layer.parameters(),
                                            min_average_window=2,
                                            max_average_window=10)

# create data loader
dataset = RandomDataset(BATCH_NUM * BATCH_SIZE)
loader = paddle.io.DataLoader(dataset,
    batch_size=BATCH_SIZE,
    shuffle=True,
    drop_last=True,
    num_workers=2)
# create data loader
eval_loader = paddle.io.DataLoader(dataset,
    batch_size=BATCH_SIZE,
    shuffle=True,
    drop_last=True,
    num_workers=1)

# train
train(layer, loader, loss_fn, optimizer, model_average)

print("\nEvaluate With ModelAverage")
with model_average.apply(need_restore=False):
    evaluate(layer, eval_loader, loss_fn)

print("\nEvaluate With Restored Paramters")
model_average.restore()
evaluate(layer, eval_loader, loss_fn)
import paddle
import numpy as np
inp = paddle.to_tensor(np.random.random([1, 10]).astype('float32'))
linear = paddle.nn.Linear(10, 1)
out = linear(inp)
loss = paddle.mean(out)
loss.backward()

sgd = paddle.optimizer.SGD(learning_rate=0.1,parameters=linear.parameters())
sgd.minimize(loss)

modelaverage = paddle.incubate.ModelAverage(0.15,
                                            parameters=linear.parameters(),
                                            min_average_window=2,
                                            max_average_window=4)
modelaverage.minimize(loss)
sgd.clear_grad()
modelaverage.clear_grad()
import paddle
import numpy as np
inp = paddle.to_tensor(np.random.random([1, 10]).astype('float32'))
linear = paddle.nn.Linear(10, 1)
out = linear(inp)
loss = paddle.mean(out)
sgd = paddle.optimizer.SGD(learning_rate=0.1,parameters=linear.parameters())
modelaverage = paddle.incubate.ModelAverage(0.15,
                                            parameters=linear.parameters(),
                                            min_average_window=2,
                                            max_average_window=4)
loss.backward()
sgd.step()
modelaverage.step()
sgd.clear_grad()
modelaverage.clear_grad()
import paddle
import numpy as np
inp = paddle.to_tensor(np.random.random([1, 10]).astype('float32'))
linear = paddle.nn.Linear(10, 1)
out = linear(inp)
loss = paddle.mean(out)
loss.backward()

sgd = paddle.optimizer.SGD(learning_rate=0.1,parameters=linear.parameters())

modelaverage = paddle.incubate.ModelAverage(0.15,
                                            parameters=linear.parameters(),
                                            min_average_window=2,
                                            max_average_window=4)
sgd.step()
modelaverage.step()

with modelaverage.apply():
    for param in linear.parameters():
        print(param)

for param in linear.parameters():
    print(param)
import paddle
import numpy as np
inp = paddle.to_tensor(np.random.random([1, 10]).astype('float32'))
linear = paddle.nn.Linear(10, 1)
out = linear(inp)
loss = paddle.mean(out)
loss.backward()

sgd = paddle.optimizer.SGD(learning_rate=0.1,parameters=linear.parameters())

modelaverage = paddle.incubate.ModelAverage(0.15,
                                            parameters=linear.parameters(),
                                            min_average_window=2,
                                            max_average_window=4)
sgd.step()
modelaverage.step()

with modelaverage.apply(need_restore=False):
    for param in linear.parameters():
        print(param)

for param in linear.parameters():
    print(param)

modelaverage.restore()

for param in linear.parameters():
    print(param)
import paddle
import numpy as np

inp = np.random.uniform(-0.1, 0.1, [10, 10]).astype("float32")
linear = paddle.nn.Linear(10, 10)
inp = paddle.to_tensor(inp)
out = linear(inp)
loss = paddle.mean(out)
optimizer = paddle.optimizer.Adam(learning_rate=0.1,
        parameters=linear.parameters())
params_grads = optimizer.backward(loss)
optimizer.apply_gradients(params_grads)
import paddle
import numpy as np
value = np.arange(26).reshape(2, 13).astype("float32")
a = paddle.to_tensor(value)
linear = paddle.nn.Linear(13, 5)
# This can be any optimizer supported by dygraph.
adam = paddle.optimizer.Adam(learning_rate = 0.01,
                            parameters = linear.parameters())
out = linear(a)
out.backward()
adam.step()
adam.clear_grad()
import numpy as np
import paddle

value = np.arange(26).reshape(2, 13).astype("float32")
a = paddle.to_tensor(value)
linear = paddle.nn.Linear(13, 5)
# This can be any optimizer supported by dygraph.
adam = paddle.optimizer.Adam(learning_rate = 0.01,
                            parameters = linear.parameters())
out = linear(a)
out.backward()
adam.step()
adam.clear_grad()
# train on default dynamic graph mode
import paddle
import numpy as np
emb = paddle.nn.Embedding(10, 3)

## example1: LRScheduler is not used, return the same value is all the same
adam = paddle.optimizer.Adam(0.01, parameters = emb.parameters())
for batch in range(10):
    input = paddle.randint(low=0, high=5, shape=[5])
    out = emb(input)
    out.backward()
    print("Learning rate of step{}: {}".format(batch, adam.get_lr())) # 0.01
    adam.step()

## example2: StepDecay is used, return the scheduled learning rate
scheduler = paddle.optimizer.lr.StepDecay(learning_rate=0.5, step_size=2, gamma=0.1)
adam = paddle.optimizer.Adam(scheduler, parameters = emb.parameters())
for batch in range(10):
    input = paddle.randint(low=0, high=5, shape=[5])
    out = emb(input)
    out.backward()
    print("Learning rate of step{}: {}".format(batch, adam.get_lr())) # 0.5->0.05...
    adam.step()
    scheduler.step()

# train on static graph mode
paddle.enable_static()
main_prog = paddle.static.Program()
start_prog = paddle.static.Program()
with paddle.static.program_guard(main_prog, start_prog):
    x = paddle.static.data(name='x', shape=[None, 10])
    z = paddle.static.nn.fc(x, 100)
    loss = paddle.mean(z)
    scheduler = paddle.optimizer.lr.StepDecay(learning_rate=0.5, step_size=2, gamma=0.1)
    adam = paddle.optimizer.Adam(learning_rate=scheduler)
    adam.minimize(loss)

exe = paddle.static.Executor()
exe.run(start_prog)
for batch in range(10):
    print("Learning rate of step{}: {}", adam.get_lr())     # 0.5->0.05->0.005...
    out = exe.run(main_prog, feed={'x': np.random.randn(3, 10).astype('float32')})
    scheduler.step()
import paddle
linear = paddle.nn.Linear(10, 10)

adam = paddle.optimizer.Adam(0.1, parameters=linear.parameters())

# set learning rate manually by python float value
lr_list = [0.2, 0.3, 0.4, 0.5, 0.6]
for i in range(5):
    adam.set_lr(lr_list[i])
    lr = adam.get_lr()
    print("current lr is {}".format(lr))
# Print:
#    current lr is 0.2
#    current lr is 0.3
#    current lr is 0.4
#    current lr is 0.5
#    current lr is 0.6
import paddle

emb = paddle.nn.Embedding(10, 10)

layer_state_dict = emb.state_dict()
paddle.save(layer_state_dict, "emb.pdparams")

scheduler = paddle.optimizer.lr.NoamDecay(
    d_model=0.01, warmup_steps=100, verbose=True)
adam = paddle.optimizer.Adam(
    learning_rate=scheduler,
    parameters=emb.parameters())
opt_state_dict = adam.state_dict()
paddle.save(opt_state_dict, "adam.pdopt")

opti_state_dict = paddle.load("adam.pdopt")
adam.set_state_dict(opti_state_dict)
import paddle
emb = paddle.nn.Embedding(10, 10)

adam = paddle.optimizer.Adam(0.001, parameters=emb.parameters())
state_dict = adam.state_dict()
import paddle
data = paddle.to_tensor([[1, 2, 3], [3, 2, 1], [4, 5, 6]], dtype='float32')
segment_ids = paddle.to_tensor([0, 0, 1], dtype='int32')
out = paddle.incubate.segment_max(data, segment_ids)
#Outputs: [[3., 2., 3.], [4., 5., 6.]]
import paddle
data = paddle.to_tensor([[1, 2, 3], [3, 2, 1], [4, 5, 6]], dtype='float32')
segment_ids = paddle.to_tensor([0, 0, 1], dtype='int32')
out = paddle.incubate.segment_mean(data, segment_ids)
#Outputs: [[2., 2., 2.], [4., 5., 6.]]
import paddle
data = paddle.to_tensor([[1, 2, 3], [3, 2, 1], [4, 5, 6]], dtype='float32')
segment_ids = paddle.to_tensor([0, 0, 1], dtype='int32')
out = paddle.incubate.segment_min(data, segment_ids)
#Outputs:  [[1., 2., 1.], [4., 5., 6.]]
import paddle
data = paddle.to_tensor([[1, 2, 3], [3, 2, 1], [4, 5, 6]], dtype='float32')
segment_ids = paddle.to_tensor([0, 0, 1], dtype='int32')
out = paddle.incubate.segment_sum(data, segment_ids)
#Outputs: [[4., 4., 4.], [4., 5., 6.]]
from paddle.io import RandomSampler, BatchSampler, Dataset

# init with dataset
class RandomDataset(Dataset):
    def __init__(self, num_samples):
        self.num_samples = num_samples

    def __getitem__(self, idx):
        image = np.random.random([784]).astype('float32')
        label = np.random.randint(0, 9, (1, )).astype('int64')
        return image, label

    def __len__(self):
        return self.num_samples

bs = BatchSampler(dataset=RandomDataset(100),
                  shuffle=False,
                  batch_size=16,
                  drop_last=False)

for batch_indices in bs:
    print(batch_indices)

# init with sampler
sampler = RandomSampler(RandomDataset(100))
bs = BatchSampler(sampler=sampler,
                  batch_size=8,
                  drop_last=True)

for batch_indices in bs:
    print(batch_indices)
import numpy as np
import paddle
from paddle.io import IterableDataset, ChainDataset


# define a random dataset
class RandomDataset(IterableDataset):
    def __init__(self, num_samples):
        self.num_samples = num_samples

    def __iter__(self):
        for i in range(10):
            image = np.random.random([32]).astype('float32')
            label = np.random.randint(0, 9, (1, )).astype('int64')
            yield image, label

dataset = ChainDataset([RandomDataset(10), RandomDataset(10)])
for image, label in iter(dataset):
    print(image, label)
import numpy as np
import paddle
from paddle.io import Dataset, ComposeDataset


# define a random dataset
class RandomDataset(Dataset):
    def __init__(self, num_samples):
        self.num_samples = num_samples

    def __getitem__(self, idx):
        image = np.random.random([32]).astype('float32')
        label = np.random.randint(0, 9, (1, )).astype('int64')
        return image, label

    def __len__(self):
        return self.num_samples

dataset = ComposeDataset([RandomDataset(10), RandomDataset(10)])
for i in range(len(dataset)):
    image1, label1, image2, label2 = dataset[i]
    print(image1)
    print(label1)
    print(image2)
    print(label2)
import numpy as np

import paddle
import paddle.nn as nn
import paddle.nn.functional as F
from paddle.io import Dataset, BatchSampler, DataLoader

BATCH_NUM = 20
BATCH_SIZE = 16
EPOCH_NUM = 4

IMAGE_SIZE = 784
CLASS_NUM = 10

# define a random dataset
class RandomDataset(Dataset):
    def __init__(self, num_samples):
        self.num_samples = num_samples

    def __getitem__(self, idx):
        image = np.random.random([IMAGE_SIZE]).astype('float32')
        label = np.random.randint(0, CLASS_NUM - 1, (1, )).astype('int64')
        return image, label

    def __len__(self):
        return self.num_samples

dataset = RandomDataset(BATCH_NUM * BATCH_SIZE)

class SimpleNet(nn.Layer):
    def __init__(self):
        super(SimpleNet, self).__init__()
        self.fc = nn.Linear(IMAGE_SIZE, CLASS_NUM)

    def forward(self, image, label=None):
        return self.fc(image)

simple_net = SimpleNet()
opt = paddle.optimizer.SGD(learning_rate=1e-3,
                          parameters=simple_net.parameters())

loader = DataLoader(dataset,
                    batch_size=BATCH_SIZE,
                    shuffle=True,
                    drop_last=True,
                    num_workers=2)

for e in range(EPOCH_NUM):
    for i, (image, label) in enumerate(loader()):
        out = simple_net(image)
        loss = F.cross_entropy(out, label)
        avg_loss = paddle.mean(loss)
        avg_loss.backward()
        opt.minimize(avg_loss)
        simple_net.clear_gradients()
        print("Epoch {} batch {}: loss = {}".format(e, i, np.mean(loss.numpy())))
'''
Example in static graph mode
'''
import numpy as np

import paddle
import paddle.static as static
import paddle.nn.functional as F


BATCH_NUM = 10
BATCH_SIZE = 16
EPOCH_NUM = 4

CLASS_NUM = 10

ITERABLE = True # whether the created DataLoader object is iterable
USE_GPU = False # whether to use GPU

DATA_FORMAT = 'batch_generator' # data format of data source user provides

paddle.enable_static()

def simple_net(image, label):
    fc_tmp = static.nn.fc(image, size=CLASS_NUM)
    cross_entropy = F.softmax_with_cross_entropy(image, label)
    loss = paddle.mean(cross_entropy)
    sgd = paddle.optimizer.SGD(learning_rate=1e-3)
    sgd.minimize(loss)
    return loss

def get_random_images_and_labels(image_shape, label_shape):
    image = np.random.random(size=image_shape).astype('float32')
    label = np.random.random(size=label_shape).astype('int64')
    return image, label

# If the data generator yields one sample each time,
# use DataLoader.set_sample_generator to set the data source.
def sample_generator_creator():
    def __reader__():
        for _ in range(BATCH_NUM * BATCH_SIZE):
            image, label = get_random_images_and_labels([784], [1])
            yield image, label

    return __reader__

# If the data generator yield list of samples each time,
# use DataLoader.set_sample_list_generator to set the data source.
def sample_list_generator_creator():
    def __reader__():
        for _ in range(BATCH_NUM):
            sample_list = []
            for _ in range(BATCH_SIZE):
                image, label = get_random_images_and_labels([784], [1])
                sample_list.append([image, label])

            yield sample_list

    return __reader__

# If the data generator yields a batch each time,
# use DataLoader.set_batch_generator to set the data source.
def batch_generator_creator():
    def __reader__():
        for _ in range(BATCH_NUM):
            batch_image, batch_label = get_random_images_and_labels([BATCH_SIZE, 784], [BATCH_SIZE, 1])
            yield batch_image, batch_label

    return __reader__

# If DataLoader is iterable, use for loop to train the network
def train_iterable(exe, prog, loss, loader):
    for _ in range(EPOCH_NUM):
        for data in loader():
            exe.run(prog, feed=data, fetch_list=[loss])

# If DataLoader is not iterable, use start() and reset() method to control the process
def train_non_iterable(exe, prog, loss, loader):
    for _ in range(EPOCH_NUM):
        loader.start() # call DataLoader.start() before each epoch starts
        try:
            while True:
                exe.run(prog, fetch_list=[loss])
        except paddle.core.EOFException:
            loader.reset() # call DataLoader.reset() after catching EOFException

def set_data_source(loader, places):
    if DATA_FORMAT == 'sample_generator':
        loader.set_sample_generator(sample_generator_creator(), batch_size=BATCH_SIZE, drop_last=True, places=places)
    elif DATA_FORMAT == 'sample_list_generator':
        loader.set_sample_list_generator(sample_list_generator_creator(), places=places)
    elif DATA_FORMAT == 'batch_generator':
        loader.set_batch_generator(batch_generator_creator(), places=places)
    else:
        raise ValueError('Unsupported data format')

image = static.data(name='image', shape=[None, 784], dtype='float32')
label = static.data(name='label', shape=[None, 1], dtype='int64')

# Define DataLoader
loader = paddle.io.DataLoader.from_generator(feed_list=[image, label], capacity=16, iterable=ITERABLE)

# Define network
loss = simple_net(image, label)

# Set data source of DataLoader
#
# If DataLoader is iterable, places must be given and the number of places must be the same with device number.
#  - If you are using GPU, call `paddle.static.cuda_places()` to get all GPU places.
#  - If you are using CPU, call `paddle.static.cpu_places()` to get all CPU places.
#
# If DataLoader is not iterable, places can be None.
places = static.cuda_places() if USE_GPU else static.cpu_places()
set_data_source(loader, places)

exe = static.Executor(places[0])
exe.run(static.default_startup_program())

prog = static.CompiledProgram(static.default_main_program()).with_data_parallel(loss_name=loss.name)

if loader.iterable:
    train_iterable(exe, prog, loss, loader)
else:
    train_non_iterable(exe, prog, loss, loader)
'''
Example in dynamic graph mode.
'''
import numpy as np

import paddle
import paddle.nn as nn
import paddle.optimizer as opt
import paddle.distributed as dist

BATCH_SIZE = 16
BATCH_NUM = 4
EPOCH_NUM = 4

IMAGE_SIZE = 784
CLASS_NUM = 10

USE_GPU = False # whether to use GPU

def _get_random_images_and_labels(image_shape, label_shape):
        image = np.random.random(size=image_shape).astype('float32')
        label = np.random.random(size=label_shape).astype('int64')
        return image, label

def __reader__():
        for _ in range(BATCH_NUM):
            batch_image, batch_label = _get_random_images_and_labels(
                [BATCH_SIZE, IMAGE_SIZE], [BATCH_SIZE, CLASS_NUM])
            yield batch_image, batch_label

def random_batch_reader():
    return __reader__

class LinearNet(nn.Layer):
    def __init__(self):
        super(LinearNet, self).__init__()
        self._linear = nn.Linear(IMAGE_SIZE, CLASS_NUM)

    @paddle.jit.to_static
    def forward(self, x):
        return self._linear(x)

# set device
paddle.set_device('gpu' if USE_GPU else 'cpu')

# create network
layer = LinearNet()
dp_layer = paddle.DataParallel(layer)
loss_fn = nn.CrossEntropyLoss()
adam = opt.Adam(learning_rate=0.001, parameters=dp_layer.parameters())

# create data loader
loader = paddle.io.DataLoader.from_generator(capacity=5)
loader.set_batch_generator(random_batch_reader())

for epoch_id in range(EPOCH_NUM):
    for batch_id, (image, label) in enumerate(loader()):
        out = layer(image)
        loss = loss_fn(out, label)

        loss.backward()

        adam.step()
        adam.clear_grad()
        print("Epoch {} batch {}: loss = {}".format(
            epoch_id, batch_id, np.mean(loss.numpy())))
'''
Example of `drop_last` using in static graph multi-cards mode
'''
import paddle
import paddle.static as static
import numpy as np
import os

# We use 2 CPU cores to run inference network
os.environ['CPU_NUM'] = '2'

paddle.enable_static()

# The data source has only 3 batches, which can not be
# divided evenly to each CPU core
def batch_generator():
    for i in range(3):
        yield np.array([i+1]).astype('float32'),

x = static.data(name='x', shape=[None], dtype='float32')
y = x * x

def run_inference(drop_last):
    loader = paddle.io.DataLoader.from_generator(feed_list=[x],
            capacity=8, drop_last=drop_last)
    loader.set_batch_generator(batch_generator, static.cpu_places())

    exe = static.Executor(paddle.CPUPlace())
    prog = static.CompiledProgram(static.default_main_program())
    prog = prog.with_data_parallel()

    result = []
    for data in loader():
        each_ret, = exe.run(prog, feed=data, fetch_list=[y])
        result.extend(each_ret)
    return result

# Set drop_last to True, so that the last batch whose
# number is less than CPU core number would be discarded.
print(run_inference(drop_last=True)) # [1.0, 4.0]

# Set drop_last to False, so that the last batch whose
# number is less than CPU core number can be tested.
print(run_inference(drop_last=False)) # [1.0, 4.0, 9.0]
import paddle
import paddle.static as static

paddle.enable_static()

image = static.data(name='image', shape=[None, 784], dtype='float32')
label = static.data(name='label', shape=[None, 1], dtype='int64')

dataset = paddle.distributed.QueueDataset()
dataset.init(
    batch_size=32,
    pipe_command='cat',
    use_var=[image, label])
dataset.set_filelist(['a.txt', 'b.txt', 'c.txt'])

loader = paddle.io.DataLoader.from_dataset(dataset, static.cpu_places())
import numpy as np
from paddle.io import Dataset

# define a random dataset
class RandomDataset(Dataset):
    def __init__(self, num_samples):
        self.num_samples = num_samples

    def __getitem__(self, idx):
        image = np.random.random([784]).astype('float32')
        label = np.random.randint(0, 9, (1, )).astype('int64')
        return image, label

    def __len__(self):
        return self.num_samples

dataset = RandomDataset(10)
for i in range(len(dataset)):
    print(dataset[i])
import numpy as np

from paddle.io import Dataset, DistributedBatchSampler

# init with dataset
class RandomDataset(Dataset):
    def __init__(self, num_samples):
        self.num_samples = num_samples

    def __getitem__(self, idx):
        image = np.random.random([784]).astype('float32')
        label = np.random.randint(0, 9, (1, )).astype('int64')
        return image, label

    def __len__(self):
        return self.num_samples

dataset = RandomDataset(100)
sampler = DistributedBatchSampler(dataset, batch_size=64)

for data in sampler:
    # do something
    break
import numpy as np

from paddle.io import Dataset, DistributedBatchSampler

# init with dataset
class RandomDataset(Dataset):
    def __init__(self, num_samples):
        self.num_samples = num_samples

    def __getitem__(self, idx):
        image = np.random.random([784]).astype('float32')
        label = np.random.randint(0, 9, (1, )).astype('int64')
        return image, label

    def __len__(self):
        return self.num_samples

dataset = RandomDataset(100)
sampler = DistributedBatchSampler(dataset, batch_size=64)

for epoch in range(10):
    sampler.set_epoch(epoch)
import math
import paddle
import numpy as np
from paddle.io import IterableDataset, DataLoader, get_worker_info

class SplitedIterableDataset(IterableDataset):
    def __init__(self, start, end):
        self.start = start
        self.end = end

    def __iter__(self):
        worker_info = get_worker_info()
        if worker_info is None:
            iter_start = self.start
            iter_end = self.end
        else:
            per_worker = int(
                math.ceil((self.end - self.start) / float(
                    worker_info.num_workers)))
            worker_id = worker_info.id
            iter_start = self.start + worker_id * per_worker
            iter_end = min(iter_start + per_worker, self.end)

        for i in range(iter_start, iter_end):
            yield np.array([i])

place = paddle.CPUPlace()
dataset = SplitedIterableDataset(start=2, end=9)
dataloader = DataLoader(
    dataset,
    places=place,
    num_workers=2,
    batch_size=1,
    drop_last=True)

for data in dataloader:
    print(data)
# outputs: [2, 5, 3, 6, 4, 7]
import numpy as np
from paddle.io import IterableDataset

# define a random dataset
class RandomDataset(IterableDataset):
    def __init__(self, num_samples):
        self.num_samples = num_samples

    def __iter__(self):
        for i in range(self.num_samples):
            image = np.random.random([784]).astype('float32')
            label = np.random.randint(0, 9, (1, )).astype('int64')
            yield image, label

dataset = RandomDataset(10)
for img, lbl in dataset:
    print(img, lbl)
import math
import paddle
import numpy as np
from paddle.io import IterableDataset, DataLoader, get_worker_info

class SplitedIterableDataset(IterableDataset):
    def __init__(self, start, end):
        self.start = start
        self.end = end

    def __iter__(self):
        worker_info = get_worker_info()
        if worker_info is None:
            iter_start = self.start
            iter_end = self.end
        else:
            per_worker = int(
                math.ceil((self.end - self.start) / float(
                    worker_info.num_workers)))
            worker_id = worker_info.id
            iter_start = self.start + worker_id * per_worker
            iter_end = min(iter_start + per_worker, self.end)

        for i in range(iter_start, iter_end):
            yield np.array([i])

dataset = SplitedIterableDataset(start=2, end=9)
dataloader = DataLoader(
    dataset,
    num_workers=2,
    batch_size=1,
    drop_last=True)

for data in dataloader:
    print(data)
    # outputs: [2, 5, 3, 6, 4, 7]
import math
import paddle
import numpy as np
from paddle.io import IterableDataset, DataLoader, get_worker_info

class RangeIterableDataset(IterableDataset):
    def __init__(self, start, end):
        self.start = start
        self.end = end

    def __iter__(self):
        for i in range(self.start, self.end):
            yield np.array([i])

dataset = RangeIterableDataset(start=2, end=9)

def worker_init_fn(worker_id):
    worker_info = get_worker_info()

    dataset = worker_info.dataset
    start = dataset.start
    end = dataset.end
    num_per_worker = int(
        math.ceil((end - start) / float(worker_info.num_workers)))

    worker_id = worker_info.id
    dataset.start = start + worker_id * num_per_worker
    dataset.end = min(dataset.start + num_per_worker, end)

dataloader = DataLoader(
    dataset,
    num_workers=2,
    batch_size=1,
    drop_last=True,
    worker_init_fn=worker_init_fn)

for data in dataloader:
    print(data)
# outputs: [2, 5, 3, 6, 4, 7]
import paddle
from paddle.io import random_split

a_list = paddle.io.random_split(range(10), [3, 7])
print(len(a_list))
# 2

for idx, v in enumerate(a_list[0]):
    print(idx, v)

# output of the first subset
# 0 1
# 1 3
# 2 9

for idx, v in enumerate(a_list[1]):
    print(idx, v)
# output of the second subset
# 0 5
# 1 7
# 2 8
# 3 6
# 4 0
# 5 2
# 6 4
from paddle.io import Dataset, RandomSampler

class RandomDataset(Dataset):
    def __init__(self, num_samples):
        self.num_samples = num_samples

    def __getitem__(self, idx):
        image = np.random.random([784]).astype('float32')
        label = np.random.randint(0, 9, (1, )).astype('int64')
        return image, label

    def __len__(self):
        return self.num_samples

sampler = RandomSampler(data_source=RandomDataset(100))

for index in sampler:
    print(index)
from paddle.io import Dataset, Sampler

class RandomDataset(Dataset):
    def __init__(self, num_samples):
        self.num_samples = num_samples

    def __getitem__(self, idx):
        image = np.random.random([784]).astype('float32')
        label = np.random.randint(0, 9, (1, )).astype('int64')
        return image, label

    def __len__(self):
        return self.num_samples

class MySampler(Sampler):
    def __init__(self, data_source):
        self.data_source = data_source

    def __iter__(self):
        return iter(range(len(self.data_source)))

    def __len__(self):
        return len(self.data_source)

sampler = MySampler(data_source=RandomDataset(100))

for index in sampler:
    print(index)
from paddle.io import Dataset, SequenceSampler

class RandomDataset(Dataset):
    def __init__(self, num_samples):
        self.num_samples = num_samples

    def __getitem__(self, idx):
        image = np.random.random([784]).astype('float32')
        label = np.random.randint(0, 9, (1, )).astype('int64')
        return image, label

    def __len__(self):
        return self.num_samples

sampler = SequenceSampler(data_source=RandomDataset(100))

for index in sampler:
    print(index)
import paddle
from paddle.io import Subset

# example 1:
a = paddle.io.Subset(dataset=range(1, 4), indices=[0, 2])
print(list(a))
# [1, 3]

# example 2:
b = paddle.io.Subset(dataset=range(1, 4), indices=[1, 1])
print(list(b))
# [2, 2]
import numpy as np
import paddle
from paddle.io import TensorDataset


input_np = np.random.random([2, 3, 4]).astype('float32')
input = paddle.to_tensor(input_np)
label_np = np.random.random([2, 1]).astype('int32')
label = paddle.to_tensor(label_np)

dataset = TensorDataset([input, label])

for i in range(len(dataset)):
    input, label = dataset[i]
    print(input, label)
from paddle.io import WeightedRandomSampler

sampler = WeightedRandomSampler(weights=[0.1, 0.3, 0.5, 0.7, 0.2],
                                num_samples=5,
                                replacement=True)

for index in sampler:
    print(index)
import numpy as np
import paddle
import paddle.nn as nn
import paddle.optimizer as opt

BATCH_SIZE = 16
BATCH_NUM = 4
EPOCH_NUM = 4

IMAGE_SIZE = 784
CLASS_NUM = 10

# define a random dataset
class RandomDataset(paddle.io.Dataset):
    def __init__(self, num_samples):
        self.num_samples = num_samples

    def __getitem__(self, idx):
        image = np.random.random([IMAGE_SIZE]).astype('float32')
        label = np.random.randint(0, CLASS_NUM - 1, (1, )).astype('int64')
        return image, label

    def __len__(self):
        return self.num_samples

class LinearNet(nn.Layer):
    def __init__(self):
        super(LinearNet, self).__init__()
        self._linear = nn.Linear(IMAGE_SIZE, CLASS_NUM)

    @paddle.jit.to_static
    def forward(self, x):
        return self._linear(x)

def train(layer, loader, loss_fn, opt):
    for epoch_id in range(EPOCH_NUM):
        for batch_id, (image, label) in enumerate(loader()):
            out = layer(image)
            loss = loss_fn(out, label)
            loss.backward()
            opt.step()
            opt.clear_grad()
            print("Epoch {} batch {}: loss = {}".format(
                epoch_id, batch_id, np.mean(loss.numpy())))

# 1. train & save model.

# create network
layer = LinearNet()
loss_fn = nn.CrossEntropyLoss()
adam = opt.Adam(learning_rate=0.001, parameters=layer.parameters())

# create data loader
dataset = RandomDataset(BATCH_NUM * BATCH_SIZE)
loader = paddle.io.DataLoader(dataset,
    batch_size=BATCH_SIZE,
    shuffle=True,
    drop_last=True,
    num_workers=2)

# train
train(layer, loader, loss_fn, adam)

# save
path = "example_model/linear"
paddle.jit.save(layer, path)

# 2. load model

# load
loaded_layer = paddle.jit.load(path)

# inference
loaded_layer.eval()
x = paddle.randn([1, IMAGE_SIZE], 'float32')
pred = loaded_layer(x)

# fine-tune
loaded_layer.train()
adam = opt.Adam(learning_rate=0.001, parameters=loaded_layer.parameters())
train(loaded_layer, loader, loss_fn, adam)
import numpy as np
import paddle
import paddle.static as static
import paddle.nn as nn
import paddle.optimizer as opt
import paddle.nn.functional as F

BATCH_SIZE = 16
BATCH_NUM = 4
EPOCH_NUM = 4

IMAGE_SIZE = 784
CLASS_NUM = 10

# define a random dataset
class RandomDataset(paddle.io.Dataset):
    def __init__(self, num_samples):
        self.num_samples = num_samples

    def __getitem__(self, idx):
        image = np.random.random([IMAGE_SIZE]).astype('float32')
        label = np.random.randint(0, CLASS_NUM - 1, (1, )).astype('int64')
        return image, label

    def __len__(self):
        return self.num_samples

paddle.enable_static()

image = static.data(name='image', shape=[None, 784], dtype='float32')
label = static.data(name='label', shape=[None, 1], dtype='int64')
pred = static.nn.fc(x=image, size=10, activation='softmax')
loss = F.cross_entropy(input=pred, label=label)
avg_loss = paddle.mean(loss)

optimizer = paddle.optimizer.SGD(learning_rate=0.001)
optimizer.minimize(avg_loss)

place = paddle.CPUPlace()
exe = static.Executor(place)
exe.run(static.default_startup_program())

# create data loader
dataset = RandomDataset(BATCH_NUM * BATCH_SIZE)
loader = paddle.io.DataLoader(dataset,
    feed_list=[image, label],
    places=place,
    batch_size=BATCH_SIZE,
    shuffle=True,
    drop_last=True,
    return_list=False,
    num_workers=2)

# 1. train and save inference model
for data in loader():
    exe.run(
        static.default_main_program(),
        feed=data,
        fetch_list=[avg_loss])

model_path = "fc.example.model"
paddle.fluid.io.save_inference_model(
    model_path, ["image"], [pred], exe)

# 2. load model

# enable dygraph mode
paddle.disable_static(place)

# load
fc = paddle.jit.load(model_path)

# inference
fc.eval()
x = paddle.randn([1, IMAGE_SIZE], 'float32')
pred = fc(x)

# fine-tune
fc.train()
loss_fn = nn.CrossEntropyLoss()
adam = opt.Adam(learning_rate=0.001, parameters=fc.parameters())
loader = paddle.io.DataLoader(dataset,
    places=place,
    batch_size=BATCH_SIZE,
    shuffle=True,
    drop_last=True,
    num_workers=2)
for epoch_id in range(EPOCH_NUM):
    for batch_id, (image, label) in enumerate(loader()):
        out = fc(image)
        loss = loss_fn(out, label)
        loss.backward()
        adam.step()
        adam.clear_grad()
        print("Epoch {} batch {}: loss = {}".format(
            epoch_id, batch_id, np.mean(loss.numpy())))
import paddle

@paddle.jit.not_to_static
def func_not_to_static(x):
    res = x - 1
    return res

@paddle.jit.to_static
def func(x):
    if paddle.mean(x) < 0:
        out = func_not_to_static(x)
    else:
        out = x + 1
    return out

x = paddle.ones([1, 2], dtype='float32')
out = func(x)
print(out) # [[2. 2.]]
import paddle

# Two methods get same object because ProgramTranslator is a singleton
paddle.jit.ProgramTranslator()
paddle.jit.ProgramTranslator.get_instance()
import paddle


@paddle.jit.to_static
def func(x):
    if paddle.mean(x) > 0:
        x_v = x - 1
    else:
        x_v = x + 1
    return x_v


prog_trans = paddle.jit.ProgramTranslator()
prog_trans.enable(False)

x = paddle.ones([1, 2])
# ProgramTranslator is disabled so the func is run in dygraph
print(func(x))  # [[0. 0.]]
import paddle


def func(x):
    if paddle.mean(x) > 0:
        x_v = x - 1
    else:
        x_v = x + 1
    return x_v


prog_trans = paddle.jit.ProgramTranslator()

x = paddle.ones([1, 2])
x_v = prog_trans.get_output(func, x)
print(x_v)  # [[0. 0.]]
import paddle


def func(x):
    if paddle.mean(x) > 0:
        x_v = x - 1
    else:
        x_v = x + 1
    return x_v


prog_trans = paddle.jit.ProgramTranslator()
static_func = prog_trans.get_func(func)
print(callable(static_func)) # True
import paddle


def func(x):
    if paddle.mean(x) > 0:
        x_v = x - 1
    else:
        x_v = x + 1
    return x_v


prog_trans = paddle.jit.ProgramTranslator()
x = paddle.ones([1, 2])
main_prog, start_prog, inputs, outputs = prog_trans.get_program(func, x)
print([i.name for i in inputs])
# [u'generated_tensor_0'] the feed input Tensor name representing x
print([o.name for o in outputs])
# [u'_generated_var_4'] the fetch output Tensor name representing x_v
import paddle


def func(x):
    if paddle.mean(x) > 0:
        x_v = x - 1
    else:
        x_v = x + 1
    return x_v


prog_trans = paddle.jit.ProgramTranslator()

code = prog_trans.get_code(func)
print(type(code)) # <class 'str'>
import paddle

prog_trans = paddle.jit.ProgramTranslator()
prog_cache = prog_trans.get_program_cache()
# example 1: save layer
import numpy as np
import paddle
import paddle.nn as nn
import paddle.optimizer as opt

BATCH_SIZE = 16
BATCH_NUM = 4
EPOCH_NUM = 4

IMAGE_SIZE = 784
CLASS_NUM = 10

# define a random dataset
class RandomDataset(paddle.io.Dataset):
    def __init__(self, num_samples):
        self.num_samples = num_samples

    def __getitem__(self, idx):
        image = np.random.random([IMAGE_SIZE]).astype('float32')
        label = np.random.randint(0, CLASS_NUM - 1, (1, )).astype('int64')
        return image, label

    def __len__(self):
        return self.num_samples

class LinearNet(nn.Layer):
    def __init__(self):
        super(LinearNet, self).__init__()
        self._linear = nn.Linear(IMAGE_SIZE, CLASS_NUM)

    @paddle.jit.to_static
    def forward(self, x):
        return self._linear(x)

def train(layer, loader, loss_fn, opt):
    for epoch_id in range(EPOCH_NUM):
        for batch_id, (image, label) in enumerate(loader()):
            out = layer(image)
            loss = loss_fn(out, label)
            loss.backward()
            opt.step()
            opt.clear_grad()
            print("Epoch {} batch {}: loss = {}".format(
                epoch_id, batch_id, np.mean(loss.numpy())))

# 1. train & save model.

# create network
layer = LinearNet()
loss_fn = nn.CrossEntropyLoss()
adam = opt.Adam(learning_rate=0.001, parameters=layer.parameters())

# create data loader
dataset = RandomDataset(BATCH_NUM * BATCH_SIZE)
loader = paddle.io.DataLoader(dataset,
    batch_size=BATCH_SIZE,
    shuffle=True,
    drop_last=True,
    num_workers=2)

# train
train(layer, loader, loss_fn, adam)

# save
path = "example_model/linear"
paddle.jit.save(layer, path)

# example 2: save function
import paddle
from paddle.static import InputSpec


def save_function():
    @paddle.jit.to_static
    def fun(inputs):
        return paddle.tanh(inputs)

    path = 'test_jit_save_load_function_1/func'
    inps = paddle.rand([3, 6])
    origin = fun(inps)

    paddle.jit.save(fun, path)
    load_func = paddle.jit.load(path)

    load_result = load_func(inps)
    print((load_result - origin).abs().max() < 1e-10)

save_function()
import paddle

paddle.jit.set_code_level(2)
# It will print the transformed code at level 2, which means to print the code after second transformer,
# as the date of August 28, 2020, it is CastTransformer.

os.environ['TRANSLATOR_CODE_LEVEL'] = '3'
# The code level is now 3, but it has no effect because it has a lower priority than `set_code_level`
import os
import paddle

paddle.jit.set_verbosity(1)
# The verbosity level is now 1

os.environ['TRANSLATOR_VERBOSITY'] = '3'
# The verbosity level is now 3, but it has no effect because it has a lower priority than `set_verbosity`
import paddle
from paddle.jit import to_static

@to_static
def func(x):
    if paddle.mean(x) < 0:
        x_v = x - 1
    else:
        x_v = x + 1
    return x_v

x = paddle.ones([1, 2], dtype='float32')
x_v = func(x)
print(x_v) # [[2. 2.]]
import numpy as np
import paddle
import paddle.nn as nn
import paddle.optimizer as opt

BATCH_SIZE = 16
BATCH_NUM = 4
EPOCH_NUM = 4

IMAGE_SIZE = 784
CLASS_NUM = 10

# define a random dataset
class RandomDataset(paddle.io.Dataset):
    def __init__(self, num_samples):
        self.num_samples = num_samples

    def __getitem__(self, idx):
        image = np.random.random([IMAGE_SIZE]).astype('float32')
        label = np.random.randint(0, CLASS_NUM - 1, (1, )).astype('int64')
        return image, label

    def __len__(self):
        return self.num_samples

class LinearNet(nn.Layer):
    def __init__(self):
        super(LinearNet, self).__init__()
        self._linear = nn.Linear(IMAGE_SIZE, CLASS_NUM)

    @paddle.jit.to_static
    def forward(self, x):
        return self._linear(x)

def train(layer, loader, loss_fn, opt):
    for epoch_id in range(EPOCH_NUM):
        for batch_id, (image, label) in enumerate(loader()):
            out = layer(image)
            loss = loss_fn(out, label)
            loss.backward()
            opt.step()
            opt.clear_grad()
            print("Epoch {} batch {}: loss = {}".format(
                epoch_id, batch_id, np.mean(loss.numpy())))

# 1. train & save model.

# create network
layer = LinearNet()
loss_fn = nn.CrossEntropyLoss()
adam = opt.Adam(learning_rate=0.001, parameters=layer.parameters())

# create data loader
dataset = RandomDataset(BATCH_NUM * BATCH_SIZE)
loader = paddle.io.DataLoader(dataset,
    batch_size=BATCH_SIZE,
    shuffle=True,
    drop_last=True,
    num_workers=2)

# train
train(layer, loader, loss_fn, adam)

# save
model_path = "linear.example.model"
paddle.jit.save(layer, model_path)

# 2. load model as TranslatedLayer

# load
translated_layer = paddle.jit.load(model_path)

# inference
translated_layer.eval()
x = paddle.randn([1, IMAGE_SIZE], 'float32')
pred = translated_layer(x)

# fine-tune
translated_layer.train()
adam = opt.Adam(learning_rate=0.001, parameters=translated_layer.parameters())
train(translated_layer, loader, loss_fn, adam)
import paddle

class MyLayer(paddle.nn.Layer):
    def __init__(self):
        super(MyLayer, self).__init__()
        self._linear = paddle.nn.Linear(1, 1)
        self._dropout = paddle.nn.Dropout(p=0.5)

    def forward(self, input):
        temp = self._linear(input)
        temp = self._dropout(temp)
        return temp

x = paddle.randn([10, 1], 'float32')
mylayer = MyLayer()
mylayer.eval()  # set mylayer._dropout to eval mode
out = mylayer(x)
mylayer.train()  # set mylayer._dropout to train mode
out = mylayer(x)
import paddle

class MyLayer(paddle.nn.Layer):
    def __init__(self):
        super(MyLayer, self).__init__()
        self._linear = paddle.nn.Linear(1, 1)
        self._dropout = paddle.nn.Dropout(p=0.5)

    def forward(self, input):
        temp = self._linear(input)
        temp = self._dropout(temp)
        return temp

x = paddle.randn([10, 1], 'float32')
mylayer = MyLayer()
mylayer.eval()  # set mylayer._dropout to eval mode
out = mylayer(x)
print(out)
import numpy as np
import paddle
import paddle.nn as nn
import paddle.optimizer as opt

BATCH_SIZE = 16
BATCH_NUM = 4
EPOCH_NUM = 4

IMAGE_SIZE = 784
CLASS_NUM = 10

# define a random dataset
class RandomDataset(paddle.io.Dataset):
    def __init__(self, num_samples):
        self.num_samples = num_samples

    def __getitem__(self, idx):
        image = np.random.random([IMAGE_SIZE]).astype('float32')
        label = np.random.randint(0, CLASS_NUM - 1, (1, )).astype('int64')
        return image, label

    def __len__(self):
        return self.num_samples

class LinearNet(nn.Layer):
    def __init__(self):
        super(LinearNet, self).__init__()
        self._linear = nn.Linear(IMAGE_SIZE, CLASS_NUM)

    @paddle.jit.to_static
    def forward(self, x):
        return self._linear(x)

def train(layer, loader, loss_fn, opt):
    for epoch_id in range(EPOCH_NUM):
        for batch_id, (image, label) in enumerate(loader()):
            out = layer(image)
            loss = loss_fn(out, label)
            loss.backward()
            opt.step()
            opt.clear_grad()
            print("Epoch {} batch {}: loss = {}".format(
                epoch_id, batch_id, np.mean(loss.numpy())))

# create network
layer = LinearNet()
loss_fn = nn.CrossEntropyLoss()
adam = opt.Adam(learning_rate=0.001, parameters=layer.parameters())

# create data loader
dataset = RandomDataset(BATCH_NUM * BATCH_SIZE)
loader = paddle.io.DataLoader(dataset,
    batch_size=BATCH_SIZE,
    shuffle=True,
    drop_last=True,
    num_workers=2)

# train
train(layer, loader, loss_fn, adam)

# save
model_path = "linear.example.model"
paddle.jit.save(layer, model_path)

# load
translated_layer = paddle.jit.load(model_path)

# get program
program = translated_layer.program()
import paddle

class MyLayer(paddle.nn.Layer):
    def __init__(self):
        super(MyLayer, self).__init__()
        self._linear = paddle.nn.Linear(1, 1)
        w_tmp = self.create_parameter([1,1])
        self.add_parameter("w_tmp", w_tmp)

    def forward(self, input):
        return self._linear(input)

mylayer = MyLayer()
for name, param in mylayer.named_parameters():
    print(name, param)      # will print w_tmp,_linear.weight,_linear.bias
import paddle

class MySequential(paddle.nn.Layer):
    def __init__(self, *layers):
        super(MySequential, self).__init__()
        if len(layers) > 0 and isinstance(layers[0], tuple):
            for name, layer in layers:
                self.add_sublayer(name, layer)
        else:
            for idx, layer in enumerate(layers):
                self.add_sublayer(str(idx), layer)

    def forward(self, input):
        for layer in self._sub_layers.values():
            input = layer(input)
        return input

fc1 = paddle.nn.Linear(10, 3)
fc2 = paddle.nn.Linear(3, 10, bias_attr=False)
model = MySequential(fc1, fc2)
for prefix, layer in model.named_sublayers():
    print(prefix, layer)
import paddle
import paddle.nn as nn

net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2))

def init_weights(layer):
    if type(layer) == nn.Linear:
        print('before init weight:', layer.weight.numpy())
        new_weight = paddle.full(shape=layer.weight.shape, dtype=layer.weight.dtype, fill_value=0.9)
        layer.weight.set_value(new_weight)
        print('after init weight:', layer.weight.numpy())

net.apply(init_weights)

print(net.state_dict())
import numpy as np
import paddle

linear = paddle.nn.Linear(10, 3)
value = np.array([0]).astype("float32")
buffer = paddle.to_tensor(value)
linear.register_buffer("buf_name", buffer, persistable=True)

print(linear.buffers())     # == print([linear.buf_name])
import paddle

linear1 = paddle.nn.Linear(10, 3)
linear2 = paddle.nn.Linear(3, 10, bias_attr=False)
model = paddle.nn.Sequential(linear1, linear2)

layer_list = list(model.children())

print(layer_list)   # [<paddle.nn.layer.common.Linear object at 0x7f7b8113f830>, <paddle.nn.layer.common.Linear object at 0x7f7b8113f950>]
import paddle
import numpy as np

value = np.arange(26).reshape(2, 13).astype("float32")
a = paddle.to_tensor(value)
linear = paddle.nn.Linear(13, 5)
adam = paddle.optimizer.Adam(learning_rate=0.01,
                            parameters=linear.parameters())
out = linear(a)
out.backward()
adam.step()
linear.clear_gradients()
import paddle

class MyLayer(paddle.nn.Layer):
    def __init__(self):
        super(MyLayer, self).__init__()
        self._linear = paddle.nn.Linear(1, 1)
        w_tmp = self.create_parameter([1,1])
        self.add_parameter("w_tmp", w_tmp)

    def forward(self, input):
        return self._linear(input)

mylayer = MyLayer()
for name, param in mylayer.named_parameters():
    print(name, param)      # will print w_tmp,_linear.weight,_linear.bias
import paddle

class MyLinear(paddle.nn.Layer):
    def __init__(self,
                in_features,
                out_features):
        super(MyLinear, self).__init__()
        self.linear = paddle.nn.Linear( 10, 10)

        self.back_var = self.create_tensor(name = "linear_tmp_0", dtype=self._dtype)

    def forward(self, input):
        out = self.linear(input)
        paddle.assign( out, self.back_var)

        return out
import paddle

class MyLinear(paddle.nn.Layer):
    def __init__(self,
                in_features,
                out_features):
        super(MyLinear, self).__init__()
        self.linear = paddle.nn.Linear( 10, 10)

        self.back_var = self.create_variable(name = "linear_tmp_0", dtype=self._dtype)

    def forward(self, input):
        out = self.linear(input)
        paddle.assign( out, self.back_var)

        return out
import paddle

class LinearNet(paddle.nn.Layer):
    def __init__(self):
        super(LinearNet, self).__init__(name_scope = "demo_linear_net")
        self._linear = paddle.nn.Linear(1, 1)

    def forward(self, x):
        return self._linear(x)

linear_net = LinearNet()
print(linear_net.full_name())   # demo_linear_net_0
import paddle

emb = paddle.nn.Embedding(10, 10)

state_dict = emb.state_dict()
paddle.save(state_dict, "paddle_dy.pdparams")
para_state_dict = paddle.load("paddle_dy.pdparams")
emb.set_state_dict(para_state_dict)
import numpy as np
import paddle

fc1 = paddle.nn.Linear(10, 3)
buffer1 = paddle.to_tensor(np.array([0]).astype("float32"))
# register a tensor as buffer by specific `persistable`
fc1.register_buffer("buf_name_1", buffer1, persistable=True)

fc2 = paddle.nn.Linear(3, 10)
buffer2 = paddle.to_tensor(np.array([1]).astype("float32"))
# register a buffer by assigning an attribute with Tensor.
# The `persistable` can only be False by this way.
fc2.buf_name_2 = buffer2

model = paddle.nn.Sequential(fc1, fc2)

# get all named buffers
for name, buffer in model.named_buffers():
    print(name, buffer)
import paddle

linear1 = paddle.nn.Linear(10, 3)
linear2 = paddle.nn.Linear(3, 10, bias_attr=False)
model = paddle.nn.Sequential(linear1, linear2)
for prefix, layer in model.named_children():
    print(prefix, layer)
    # ('0', <paddle.nn.layer.common.Linear object at 0x7fb61ed85830>)
    # ('1', <paddle.nn.layer.common.Linear object at 0x7fb61ed85950>)
import paddle

fc1 = paddle.nn.Linear(10, 3)
fc2 = paddle.nn.Linear(3, 10, bias_attr=False)
model = paddle.nn.Sequential(fc1, fc2)
for name, param in model.named_parameters():
    print(name, param)
import paddle

fc1 = paddle.nn.Linear(10, 3)
fc2 = paddle.nn.Linear(3, 10, bias_attr=False)
model = paddle.nn.Sequential(fc1, fc2)
for prefix, layer in model.named_sublayers():
    print(prefix, layer)

import numpy as np
import paddle

linear = paddle.nn.Linear(10, 3)
value = np.array([0]).astype("float32")
buffer = paddle.to_tensor(value)
linear.register_buffer("buf_name", buffer, persistable=True)

# get the buffer by attribute.
print(linear.buf_name)
import paddle
import numpy as np

# the forward_post_hook change the output of the layer: output = output * 2
def forward_post_hook(layer, input, output):
    # user can use layer, input and output for information statistis tasks

    # change the output
    return output * 2

linear = paddle.nn.Linear(13, 5)

# register the hook
forward_post_hook_handle = linear.register_forward_post_hook(forward_post_hook)

value1 = np.arange(26).reshape(2, 13).astype("float32")
in1 = paddle.to_tensor(value1)

out0 = linear(in1)

# remove the hook
forward_post_hook_handle.remove()

out1 = linear(in1)

# hook change the linear's output to output * 2, so out0 is equal to out1 * 2.
assert (out0.numpy() == (out1.numpy()) * 2).any()
import paddle
import numpy as np

# the forward_post_hook change the input of the layer: input = input * 2
def forward_pre_hook(layer, input):
    # user can use layer and input for information statistis tasks

    # change the input
    input_return = (input[0] * 2)
    return input_return

linear = paddle.nn.Linear(13, 5)

# register the hook
forward_pre_hook_handle = linear.register_forward_pre_hook(forward_pre_hook)

value0 = np.arange(26).reshape(2, 13).astype("float32")
in0 = paddle.to_tensor(value0)
out0 = linear(in0)

# remove the hook
forward_pre_hook_handle.remove()

value1 = value0 * 2
in1 = paddle.to_tensor(value1)
out1 = linear(in1)

# hook change the linear's input to input * 2, so out0 is equal to out1.
assert (out0.numpy() == out1.numpy()).any()
import paddle

emb = paddle.nn.Embedding(10, 10)

state_dict = emb.state_dict()
paddle.save(state_dict, "paddle_dy.pdparams")
para_state_dict = paddle.load("paddle_dy.pdparams")
emb.set_state_dict(para_state_dict)
import paddle

emb = paddle.nn.Embedding(10, 10)

state_dict = emb.state_dict()
paddle.save(state_dict, "paddle_dy.pdparams")
para_state_dict = paddle.load("paddle_dy.pdparams")
emb.set_state_dict(para_state_dict)
import paddle

emb = paddle.nn.Embedding(10, 10)

state_dict = emb.state_dict()
paddle.save( state_dict, "paddle_dy.pdparams")
import paddle

class MyLayer(paddle.nn.Layer):
    def __init__(self):
        super(MyLayer, self).__init__()
        self._linear = paddle.nn.Linear(1, 1)
        self._dropout = paddle.nn.Dropout(p=0.5)

    def forward(self, input):
        temp = self._linear(input)
        temp = self._dropout(temp)
        return temp

mylayer = MyLayer()
print(mylayer.sublayers())  # [<paddle.nn.layer.common.Linear object at 0x7f44b58977d0>, <paddle.nn.layer.common.Dropout object at 0x7f44b58978f0>]
# required: skip
import paddle

linear=paddle.nn.Linear(2, 2)
linear.weight
#Parameter containing:
#Tensor(shape=[2, 2], dtype=float32, place=CUDAPlace(0), stop_gradient=False,
#       [[-0.32770029,  0.38653070],
#        [ 0.46030545,  0.08158520]])

linear.to(dtype='float64')
linear.weight
#Tenor(shape=[2, 2], dtype=float64, place=CUDAPlace(0), stop_gradient=False,
#       [[-0.32770029,  0.38653070],
#        [ 0.46030545,  0.08158520]])

linear.to(device='cpu')
linear.weight
#Tensor(shape=[2, 2], dtype=float64, place=CPUPlace, stop_gradient=False,
#       [[-0.32770029,  0.38653070],
#        [ 0.46030545,  0.08158520]])
linear.to(device=paddle.CUDAPinnedPlace(), blocking=False)
linear.weight
#Tensor(shape=[2, 2], dtype=float64, place=CUDAPinnedPlace, stop_gradient=False,
#       [[-0.04989364, -0.56889004],
#        [ 0.33960250,  0.96878713]])
import paddle

emb = paddle.nn.Embedding(10, 10)

state_dict = emb.to_static_state_dict()
paddle.save( state_dict, "paddle_dy.pdparams")
import paddle
import numpy as np

x = paddle.to_tensor([[1., 0, -1], [0, 1, 0], [1, 0, 1]])

# compute conditional number when p is None
out = paddle.linalg.cond(x)
# out.numpy() [1.4142135]

# compute conditional number when order of the norm is 'fro'
out_fro = paddle.linalg.cond(x, p='fro')
# out_fro.numpy() [3.1622777]

# compute conditional number when order of the norm is 'nuc'
out_nuc = paddle.linalg.cond(x, p='nuc')
# out_nuc.numpy() [9.2426405]

# compute conditional number when order of the norm is 1
out_1 = paddle.linalg.cond(x, p=1)
# out_1.numpy() [2.]

# compute conditional number when order of the norm is -1
out_minus_1 = paddle.linalg.cond(x, p=-1)
# out_minus_1.numpy() [1.]

# compute conditional number when order of the norm is 2
out_2 = paddle.linalg.cond(x, p=2)
# out_2.numpy() [1.4142135]

# compute conditional number when order of the norm is -1
out_minus_2 = paddle.linalg.cond(x, p=-2)
# out_minus_2.numpy() [0.70710677]

# compute conditional number when order of the norm is inf
out_inf = paddle.linalg.cond(x, p=np.inf)
# out_inf.numpy() [2.]

# compute conditional number when order of the norm is -inf
out_minus_inf = paddle.linalg.cond(x, p=-np.inf)
# out_minus_inf.numpy() [1.]

a = paddle.to_tensor(np.random.randn(2, 4, 4).astype('float32'))
# a.numpy()
# [[[ 0.14063153 -0.996288    0.7996131  -0.02571543]
#   [-0.16303636  1.5534962  -0.49919784 -0.04402903]
#   [-1.1341571  -0.6022629   0.5445269   0.29154757]
#   [-0.16816919 -0.30972657  1.7521842  -0.5402487 ]]
#  [[-0.58081484  0.12402827  0.7229862  -0.55046535]
#   [-0.15178485 -1.1604939   0.75810957  0.30971205]
#   [-0.9669573   1.0940945  -0.27363303 -0.35416734]
#   [-1.216529    2.0018666  -0.7773689  -0.17556527]]]
a_cond_fro = paddle.linalg.cond(a, p='fro')
# a_cond_fro.numpy()  [31.572273 28.120834]

b = paddle.to_tensor(np.random.randn(2, 3, 4).astype('float64'))
# b.numpy()
# [[[ 1.61707487  0.46829144  0.38130416  0.82546736]
#   [-1.72710298  0.08866375 -0.62518804  0.16128892]
#   [-0.02822879 -1.67764516  0.11141444  0.3220113 ]]
#  [[ 0.22524372  0.62474921 -0.85503233 -1.03960523]
#   [-0.76620689  0.56673047  0.85064753 -0.45158196]
#   [ 1.47595418  2.23646462  1.5701758   0.10497519]]]
b_cond_2 = paddle.linalg.cond(b, p=2)
# b_cond_2.numpy()  [3.30064451 2.51976252]

import paddle
import numpy as np

paddle.device.set_device("cpu")

x_data = np.array([[1.6707249, 7.2249975, 6.5045543],
                   [9.956216,  8.749598,  6.066444 ],
                   [4.4251957, 1.7983172, 0.370647 ]]).astype("float32")
x = paddle.to_tensor(x_data)
w, v = paddle.linalg.eig(x)
print(w)
# Tensor(shape=[3, 3], dtype=complex128, place=CPUPlace, stop_gradient=False,
#       [[(-0.5061363550800655+0j) , (-0.7971760990842826+0j) ,
#         (0.18518077798279986+0j)],
#        [(-0.8308237755993192+0j) ,  (0.3463813401919749+0j) ,
#         (-0.6837005269141947+0j) ],
#        [(-0.23142567697893396+0j),  (0.4944999840400175+0j) ,
#         (0.7058765252952796+0j) ]])

print(v)
# Tensor(shape=[3], dtype=complex128, place=CPUPlace, stop_gradient=False,
#       [ (16.50471283351188+0j)  , (-5.5034820550763515+0j) ,
#         (-0.21026087843552282+0j)])
import numpy as np
import paddle

x_data = np.array([[1, -2j], [2j, 5]])
x = paddle.to_tensor(x_data)
out_value, out_vector = paddle.linalg.eigh(x, UPLO='L')
print(out_value)
#[0.17157288, 5.82842712]
print(out_vector)
#[(-0.9238795325112867+0j), (-0.3826834323650898+0j)],
#[ 0.3826834323650898j    , -0.9238795325112867j    ]]
import paddle

paddle.set_device("cpu")
paddle.seed(1234)

x = paddle.rand(shape=[3, 3], dtype='float64')
# [[0.02773777, 0.93004224, 0.06911496],
#  [0.24831591, 0.45733623, 0.07717843],
#  [0.48016702, 0.14235102, 0.42620817]])

print(paddle.linalg.eigvals(x))
# [(-0.27078833542132674+0j), (0.29962280156230725+0j), (0.8824477020120244+0j)] #complex128
import numpy as np
import paddle

x_data = np.array([[1, -2j], [2j, 5]])
x = paddle.to_tensor(x_data)
out_value = paddle.eigvalsh(x, UPLO='L')
print(out_value)
#[0.17157288, 5.82842712]
import paddle

x = paddle.to_tensor([[1, 2, 3],
                      [1, 4, 9],
                      [1, 8, 27]], dtype='float64')
print(paddle.linalg.matrix_power(x, 2))
# [[6.  , 34. , 102.],
#  [14. , 90. , 282.],
#  [36. , 250., 804.]]

print(paddle.linalg.matrix_power(x, 0))
# [[1., 0., 0.],
#  [0., 1., 0.],
#  [0., 0., 1.]]

print(paddle.linalg.matrix_power(x, -2))
# [[ 12.91666667, -12.75000000,  2.83333333 ],
#  [-7.66666667 ,  8.         , -1.83333333 ],
#  [ 1.80555556 , -1.91666667 ,  0.44444444 ]]
import paddle

a = paddle.eye(10)
b = paddle.linalg.matrix_rank(a)
print(b)
# b = [10]

c = paddle.ones(shape=[3, 4, 5, 5])
d = paddle.linalg.matrix_rank(c, tol=0.01, hermitian=True)
print(d)
# d = [[1, 1, 1, 1],
#      [1, 1, 1, 1],
#      [1, 1, 1, 1]]
import paddle
import numpy as np

# A * B
A_data = np.random.random([3, 4]).astype(np.float32)
B_data = np.random.random([4, 5]).astype(np.float32)
A = paddle.to_tensor(A_data)
B = paddle.to_tensor(B_data)
out = paddle.linalg.multi_dot([A, B])
print(out.numpy().shape)
# [3, 5]

# A * B * C
A_data = np.random.random([10, 5]).astype(np.float32)
B_data = np.random.random([5, 8]).astype(np.float32)
C_data = np.random.random([8, 7]).astype(np.float32)
A = paddle.to_tensor(A_data)
B = paddle.to_tensor(B_data)
C = paddle.to_tensor(C_data)
out = paddle.linalg.multi_dot([A, B, C])
print(out.numpy().shape)
# [10, 7]
import paddle

x = paddle.arange(15).reshape((3, 5)).astype('float64')
input = paddle.to_tensor(x)
out = paddle.linalg.pinv(input)
print(input)
print(out)

# input:
# [[0. , 1. , 2. , 3. , 4. ],
# [5. , 6. , 7. , 8. , 9. ],
# [10., 11., 12., 13., 14.]]

# out:
# [[-0.22666667, -0.06666667,  0.09333333],
# [-0.12333333, -0.03333333,  0.05666667],
# [-0.02000000,  0.00000000,  0.02000000],
# [ 0.08333333,  0.03333333, -0.01666667],
# [ 0.18666667,  0.06666667, -0.05333333]]

# one can verify : x * out * x = x ;
# or              out * x * out = x ;
import paddle

x = paddle.to_tensor([[1.0, 2.0], [3.0, 4.0], [5.0, 6.0]]).astype('float64')
q, r = paddle.linalg.qr(x)
print (q)
print (r)

# Q = [[-0.16903085,  0.89708523],
#      [-0.50709255,  0.27602622],
#      [-0.84515425, -0.34503278]])

# R = [[-5.91607978, -7.43735744],
#      [ 0.        ,  0.82807867]])

# one can verify : X = Q * R ;
import paddle

x = paddle.to_tensor([[1.0, 2.0], [1.0, 3.0], [4.0, 6.0]]).astype('float64')
x = x.reshape([3, 2])
u, s, vh = paddle.linalg.svd(x)
print (u)
#U = [[ 0.27364809, -0.21695147  ],
#      [ 0.37892198, -0.87112408 ],
#      [ 0.8840446 ,  0.44053933 ]]

print (s)
#S = [8.14753743, 0.78589688]
print (vh)
#VT= [[ 0.51411221,  0.85772294],
#     [ 0.85772294, -0.51411221]]

# one can verify : U * S * VT == X
#                  U * UH == I
#                  V * VH == I
import numpy as np
import paddle

x = paddle.to_tensor(np.array([
    [0.1, 0.2, 0.3, 0.4],
    [0.1, 0.4, 0.3, 0.2],
    [0.1, 0.2, 0.4, 0.3],
    [0.1, 0.2, 0.3, 0.4]]))
y = paddle.to_tensor(np.array([[0], [1], [2], [3]]))

m = paddle.metric.Accuracy()
correct = m.compute(x, y)
m.update(correct)
res = m.accumulate()
print(res) # 0.75
import paddle
from paddle.static import InputSpec
import paddle.vision.transforms as T
from paddle.vision.datasets import MNIST

input = InputSpec([None, 1, 28, 28], 'float32', 'image')
label = InputSpec([None, 1], 'int64', 'label')
transform = T.Compose([T.Transpose(), T.Normalize([127.5], [127.5])])
train_dataset = MNIST(mode='train', transform=transform)

model = paddle.Model(paddle.vision.models.LeNet(), input, label)
optim = paddle.optimizer.Adam(
    learning_rate=0.001, parameters=model.parameters())
model.prepare(
    optim,
    loss=paddle.nn.CrossEntropyLoss(),
    metrics=paddle.metric.Accuracy())

model.fit(train_dataset, batch_size=64)
import paddle

predictions = paddle.to_tensor([[0.2, 0.1, 0.4, 0.1, 0.1], [0.2, 0.3, 0.1, 0.15, 0.25]], dtype='float32')
label = paddle.to_tensor([[2], [0]], dtype="int64")
result = paddle.metric.accuracy(input=predictions, label=label, k=1)
# [0.5]
import numpy as np
import paddle

m = paddle.metric.Auc()

n = 8
class0_preds = np.random.random(size = (n, 1))
class1_preds = 1 - class0_preds

preds = np.concatenate((class0_preds, class1_preds), axis=1)
labels = np.random.randint(2, size = (n, 1))

m.update(preds=preds, labels=labels)
res = m.accumulate()
import numpy as np
import paddle
import paddle.nn as nn

class Data(paddle.io.Dataset):
    def __init__(self):
        super(Data, self).__init__()
        self.n = 1024
        self.x = np.random.randn(self.n, 10).astype('float32')
        self.y = np.random.randint(2, size=(self.n, 1)).astype('int64')

    def __getitem__(self, idx):
        return self.x[idx], self.y[idx]

    def __len__(self):
        return self.n

model = paddle.Model(nn.Sequential(
    nn.Linear(10, 2), nn.Softmax())
)
optim = paddle.optimizer.Adam(
    learning_rate=0.001, parameters=model.parameters())

def loss(x, y):
    return nn.functional.nll_loss(paddle.log(x), y)

model.prepare(
    optim,
    loss=loss,
    metrics=paddle.metric.Auc())
data = Data()
model.fit(data, batch_size=16)
import numpy as np
import paddle

x = np.array([0.1, 0.5, 0.6, 0.7])
y = np.array([0, 1, 1, 1])

m = paddle.metric.Precision()
m.update(x, y)
res = m.accumulate()
print(res) # 1.0
import numpy as np

import paddle
import paddle.nn as nn

class Data(paddle.io.Dataset):
    def __init__(self):
        super(Data, self).__init__()
        self.n = 1024
        self.x = np.random.randn(self.n, 10).astype('float32')
        self.y = np.random.randint(2, size=(self.n, 1)).astype('float32')

    def __getitem__(self, idx):
        return self.x[idx], self.y[idx]

    def __len__(self):
        return self.n

model = paddle.Model(nn.Sequential(
    nn.Linear(10, 1),
    nn.Sigmoid()
))
optim = paddle.optimizer.Adam(
    learning_rate=0.001, parameters=model.parameters())
model.prepare(
    optim,
    loss=nn.BCELoss(),
    metrics=paddle.metric.Precision())

data = Data()
model.fit(data, batch_size=16)
import numpy as np
import paddle

x = np.array([0.1, 0.5, 0.6, 0.7])
y = np.array([1, 0, 1, 1])

m = paddle.metric.Recall()
m.update(x, y)
res = m.accumulate()
print(res) # 2.0 / 3.0
import numpy as np

import paddle
import paddle.nn as nn

class Data(paddle.io.Dataset):
    def __init__(self):
        super(Data, self).__init__()
        self.n = 1024
        self.x = np.random.randn(self.n, 10).astype('float32')
        self.y = np.random.randint(2, size=(self.n, 1)).astype('float32')

    def __getitem__(self, idx):
        return self.x[idx], self.y[idx]

    def __len__(self):
        return self.n

model = paddle.Model(nn.Sequential(
    nn.Linear(10, 1),
    nn.Sigmoid()
))
optim = paddle.optimizer.Adam(
    learning_rate=0.001, parameters=model.parameters())
model.prepare(
    optim,
    loss=nn.BCELoss(),
    metrics=[paddle.metric.Precision(), paddle.metric.Recall()])

data = Data()
model.fit(data, batch_size=16)
# average adaptive pool1d
# suppose input data in shape of [N, C, L], `output_size` is m or [m],
# output shape is [N, C, m], adaptive pool divide L dimension
# of input data into m grids averagely and performs poolings in each
# grid to get output.
# adaptive max pool performs calculations as follow:
#
#     for i in range(m):
#         lstart = floor(i * L / m)
#         lend = ceil((i + 1) * L / m)
#         output[:, :, i] = sum(input[:, :, lstart: lend])/(lend - lstart)
#
import paddle
import paddle.nn as nn
import numpy as np

data = paddle.to_tensor(np.random.uniform(-1, 1, [1, 3, 32]).astype(np.float32))
AdaptiveAvgPool1D = nn.AdaptiveAvgPool1D(output_size=16)
pool_out = AdaptiveAvgPool1D(data)
# pool_out shape: [1, 3, 16]
# adaptive avg pool2d
# suppose input data in shape of [N, C, H, W], `output_size` is [m, n],
# output shape is [N, C, m, n], adaptive pool divide H and W dimensions
# of input data into m * n grids averagely and performs poolings in each
# grid to get output.
# adaptive avg pool performs calculations as follow:
#
#     for i in range(m):
#         for j in range(n):
#             hstart = floor(i * H / m)
#             hend = ceil((i + 1) * H / m)
#             wstart = floor(i * W / n)
#             wend = ceil((i + 1) * W / n)
#             output[:, :, i, j] = avg(input[:, :, hstart: hend, wstart: wend])
#
import paddle
import numpy as np

input_data = np.random.rand(2, 3, 32, 32)
x = paddle.to_tensor(input_data)
# x.shape is [2, 3, 32, 32]
adaptive_avg_pool = paddle.nn.AdaptiveAvgPool2D(output_size=3)
pool_out = adaptive_avg_pool(x = x)
# pool_out.shape is [2, 3, 3, 3]
# adaptive avg pool3d
# suppose input data in shape of [N, C, D, H, W], `output_size` is [l, m, n],
# output shape is [N, C, l, m, n], adaptive pool divide D, H and W dimensions
# of input data into l * m * n grids averagely and performs poolings in each
# grid to get output.
# adaptive avg pool performs calculations as follow:
#
#     for i in range(l):
#         for j in range(m):
#             for k in range(n):
#                 dstart = floor(i * D / l)
#                 dend = ceil((i + 1) * D / l)
#                 hstart = floor(j * H / m)
#                 hend = ceil((j + 1) * H / m)
#                 wstart = floor(k * W / n)
#                 wend = ceil((k + 1) * W / n)
#                 output[:, :, i, j, k] =
#                     avg(input[:, :, dstart:dend, hstart: hend, wstart: wend])
import paddle
import numpy as np

input_data = np.random.rand(2, 3, 8, 32, 32)
x = paddle.to_tensor(input_data)
# x.shape is [2, 3, 8, 32, 32]
adaptive_avg_pool = paddle.nn.AdaptiveAvgPool3D(output_size=3)
pool_out = adaptive_avg_pool(x = x)
# pool_out = [2, 3, 3, 3, 3]
# max adaptive pool1d
# suppose input data in shape of [N, C, L], `output_size` is m or [m],
# output shape is [N, C, m], adaptive pool divide L dimension
# of input data into m grids averagely and performs poolings in each
# grid to get output.
# adaptive max pool performs calculations as follow:
#
#     for i in range(m):
#         lstart = floor(i * L / m)
#         lend = ceil((i + 1) * L / m)
#         output[:, :, i] = max(input[:, :, lstart: lend])
#
import paddle
import paddle.nn as nn
import numpy as np

data = paddle.to_tensor(np.random.uniform(-1, 1, [1, 3, 32]).astype(np.float32))
AdaptiveMaxPool1D = nn.AdaptiveMaxPool1D(output_size=16)
pool_out = AdaptiveMaxPool1D(data)
# pool_out shape: [1, 3, 16]

# for return_mask = true
AdaptiveMaxPool1D = nn.AdaptiveMaxPool1D(output_size=16, return_mask=True)
pool_out, indices = AdaptiveMaxPool1D(data)
# pool_out shape: [1, 3, 16], indices shape: [1, 3, 16]
# adaptive max pool2d
# suppose input data in shape of [N, C, H, W], `output_size` is [m, n],
# output shape is [N, C, m, n], adaptive pool divide H and W dimensions
# of input data into m * n grids averagely and performs poolings in each
# grid to get output.
# adaptive max pool performs calculations as follow:
#
#     for i in range(m):
#         for j in range(n):
#             hstart = floor(i * H / m)
#             hend = ceil((i + 1) * H / m)
#             wstart = floor(i * W / n)
#             wend = ceil((i + 1) * W / n)
#             output[:, :, i, j] = max(input[:, :, hstart: hend, wstart: wend])
#
import paddle
import numpy as np

input_data = np.random.rand(2, 3, 32, 32)
x = paddle.to_tensor(input_data)
adaptive_max_pool = paddle.nn.AdaptiveMaxPool2D(output_size=3, return_mask=True)
pool_out, indices = adaptive_max_pool(x = x)
# adaptive max pool3d
# suppose input data in shape of [N, C, D, H, W], `output_size` is [l, m, n],
# output shape is [N, C, l, m, n], adaptive pool divide D, H and W dimensions
# of input data into l * m * n grids averagely and performs poolings in each
# grid to get output.
# adaptive max pool performs calculations as follow:
#
#     for i in range(l):
#         for j in range(m):
#             for k in range(n):
#                 dstart = floor(i * D / l)
#                 dend = ceil((i + 1) * D / l)
#                 hstart = floor(j * H / m)
#                 hend = ceil((j + 1) * H / m)
#                 wstart = floor(k * W / n)
#                 wend = ceil((k + 1) * W / n)
#                 output[:, :, i, j, k] =
#                     max(input[:, :, dstart:dend, hstart: hend, wstart: wend])
import paddle
import numpy as np

input_data = np.random.rand(2, 3, 8, 32, 32)
x = paddle.to_tensor(input_data)
pool = paddle.nn.AdaptiveMaxPool3D(output_size=4)
out = pool(x)
# out shape: [2, 3, 4, 4, 4]
pool = paddle.nn.AdaptiveMaxPool3D(output_size=3, return_mask=True)
out, indices = pool(x)
# out shape: [2, 3, 4, 4, 4], indices shape: [2, 3, 4, 4, 4]
import paddle
import numpy as np

x = np.array([[-1, 1], [-1, 1]]).astype('float32')
x = paddle.to_tensor(x)
m = paddle.nn.AlphaDropout(p=0.5)
y_train = m(x)
m.eval()  # switch the model to test phase
y_test = m(x)
print(x)
print(y_train)
# [[-0.10721093, 1.6655989 ], [-0.7791938, -0.7791938]] (randomly)
print(y_test)
import paddle
import paddle.nn as nn
import numpy as np

data = paddle.to_tensor(np.random.uniform(-1, 1, [1, 3, 32]).astype(np.float32))
AvgPool1D = nn.AvgPool1D(kernel_size=2, stride=2, padding=0)
pool_out = AvgPool1D(data)
# pool_out shape: [1, 3, 16]
import paddle
import paddle.nn as nn
import numpy as np

# max pool2d
input = paddle.to_tensor(np.random.uniform(-1, 1, [1, 3, 32, 32]).astype(np.float32))
AvgPool2D = nn.AvgPool2D(kernel_size=2,
                    stride=2, padding=0)
output = AvgPool2D(input)
# output.shape [1, 3, 16, 16]
import paddle
import paddle.nn as nn
import numpy as np

# avg pool3d
input = paddle.to_tensor(np.random.uniform(-1, 1, [1, 2, 3, 32, 32]).astype(np.float32))
AvgPool3D = nn.AvgPool3D(kernel_size=2,
                       stride=2, padding=0)
output = AvgPool3D(input)
# output.shape [1, 2, 3, 16, 16]
import paddle.fluid as fluid
from paddle.fluid.dygraph.base import to_variable
import numpy as np

x = np.random.random(size=(3, 10, 3, 7)).astype('float32')
with fluid.dygraph.guard():
    x = to_variable(x)
    batch_norm = fluid.BatchNorm(10)
    hidden1 = batch_norm(x)
import paddle
import numpy as np

np.random.seed(123)
x_data = np.random.random(size=(2, 1, 3)).astype('float32')
x = paddle.to_tensor(x_data)
batch_norm = paddle.nn.BatchNorm1D(1)
batch_norm_out = batch_norm(x)

print(batch_norm_out)
import paddle
import numpy as np

np.random.seed(123)
x_data = np.random.random(size=(2, 1, 2, 3)).astype('float32')
x = paddle.to_tensor(x_data)
batch_norm = paddle.nn.BatchNorm2D(1)
batch_norm_out = batch_norm(x)

print(batch_norm_out)
import paddle
import numpy as np

np.random.seed(123)
x_data = np.random.random(size=(2, 1, 2, 2, 3)).astype('float32')
x = paddle.to_tensor(x_data)
batch_norm = paddle.nn.BatchNorm3D(1)
batch_norm_out = batch_norm(x)

print(batch_norm_out)
import numpy as np
import paddle
input_data = np.array([0.5, 0.6, 0.7]).astype("float32")
label_data = np.array([1.0, 0.0, 1.0]).astype("float32")

input = paddle.to_tensor(input_data)
label = paddle.to_tensor(label_data)
bce_loss = paddle.nn.BCELoss()
output = bce_loss(input, label)
print(output)  # [0.65537095]
.. code-block:: python
    import paddle
    logit = paddle.to_tensor([5.0, 1.0, 3.0], dtype="float32")
    label = paddle.to_tensor([1.0, 0.0, 1.0], dtype="float32")
    bce_logit_loss = paddle.nn.BCEWithLogitsLoss()
    output = bce_logit_loss(logit, label)
    print(output.numpy())  # [0.45618808]
import numpy as np
import paddle
from paddle.nn import BeamSearchDecoder, dynamic_decode
from paddle.nn import GRUCell, Linear, Embedding
trg_embeder = Embedding(100, 32)
output_layer = Linear(32, 32)
decoder_cell = GRUCell(input_size=32, hidden_size=32)
decoder = BeamSearchDecoder(decoder_cell,
                            start_token=0,
                            end_token=1,
                            beam_size=4,
                            embedding_fn=trg_embeder,
                            output_fn=output_layer)
import paddle
import numpy

layer1 = numpy.random.random((5, 5)).astype('float32')
layer2 = numpy.random.random((5, 4)).astype('float32')
bilinear = paddle.nn.Bilinear(
    in1_features=5, in2_features=4, out_features=1000)
result = bilinear(paddle.to_tensor(layer1),
                paddle.to_tensor(layer2))     # result shape [5, 1000]
import paddle

cell_fw = paddle.nn.LSTMCell(16, 32)
cell_bw = paddle.nn.LSTMCell(16, 32)
rnn = paddle.nn.BiRNN(cell_fw, cell_bw)

inputs = paddle.rand((2, 23, 16))
outputs, final_states = rnn(inputs)

print(outputs.shape)
print(final_states[0][0].shape,len(final_states),len(final_states[0]))

#[4,23,64]
#[2,32] 2 2
import paddle

x = paddle.uniform([10, 10], min=-1.0, max=1.0, dtype='float32')
linear = paddle.nn.Linear(in_features=10, out_features=10,
                          weight_attr=paddle.ParamAttr(need_clip=True),
                          bias_attr=paddle.ParamAttr(need_clip=False))
out = linear(x)
loss = paddle.mean(out)
loss.backward()

clip = paddle.nn.ClipGradByGlobalNorm(clip_norm=1.0)
sdg = paddle.optimizer.SGD(learning_rate=0.1, parameters=linear.parameters(), grad_clip=clip)
sdg.step()
import paddle

x = paddle.uniform([10, 10], min=-1.0, max=1.0, dtype='float32')
linear = paddle.nn.Linear(in_features=10, out_features=10,
                          weight_attr=paddle.ParamAttr(need_clip=True),
                          bias_attr=paddle.ParamAttr(need_clip=False))
out = linear(x)
loss = paddle.mean(out)
loss.backward()

clip = paddle.nn.ClipGradByNorm(clip_norm=1.0)
sdg = paddle.optimizer.SGD(learning_rate=0.1, parameters=linear.parameters(), grad_clip=clip)
sdg.step()
import paddle

x = paddle.uniform([10, 10], min=-1.0, max=1.0, dtype='float32')
linear = paddle.nn.Linear(in_features=10, out_features=10,
                          weight_attr=paddle.ParamAttr(need_clip=True),
                          bias_attr=paddle.ParamAttr(need_clip=False))
out = linear(x)
loss = paddle.mean(out)
loss.backward()

clip = paddle.nn.ClipGradByValue(min=-1, max=1)
sdg = paddle.optimizer.SGD(learning_rate=0.1, parameters=linear.parameters(), grad_clip=clip)
sdg.step()
import paddle
from paddle.nn import Conv1D
import numpy as np
x = np.array([[[4, 8, 1, 9],
  [7, 2, 0, 9],
  [6, 9, 2, 6]]]).astype(np.float32)
w=np.array(
[[[9, 3, 4],
  [0, 0, 7],
  [2, 5, 6]],
 [[0, 3, 4],
  [2, 9, 7],
  [5, 6, 8]]]).astype(np.float32)
x_t = paddle.to_tensor(x)
conv = Conv1D(3, 2, 3)
conv.weight.set_value(w)
y_t = conv(x_t)
print(y_t)
# [[[133. 238.]
#   [160. 211.]]]
import paddle
from paddle.nn import Conv1DTranspose
import numpy as np

# shape: (1, 2, 4)
x=np.array([[[4, 0, 9, 7],
             [8, 0, 9, 2]]]).astype(np.float32)
# shape: (2, 1, 2)
y=np.array([[[7, 0]],
            [[4, 2]]]).astype(np.float32)
x_t = paddle.to_tensor(x)
conv = Conv1DTranspose(2, 1, 2)
conv.weight.set_value(y)
y_t = conv(x_t)
print(y_t)

# [[[60. 16. 99. 75.  4.]]]
import paddle
import paddle.nn as nn

paddle.disable_static()

x_var = paddle.uniform((2, 4, 8, 8), dtype='float32', min=-1., max=1.)

conv = nn.Conv2D(4, 6, (3, 3))
y_var = conv(x_var)
y_np = y_var.numpy()
print(y_np.shape)
# (2, 6, 6, 6)
import paddle
import paddle.nn as nn

paddle.disable_static()

x_var = paddle.uniform((2, 4, 8, 8), dtype='float32', min=-1., max=1.)

conv = nn.Conv2DTranspose(4, 6, (3, 3))
y_var = conv(x_var)
y_np = y_var.numpy()
print(y_np.shape)
# (2, 6, 10, 10)
import paddle
import paddle.nn as nn

paddle.disable_static()

x_var = paddle.uniform((2, 4, 8, 8, 8), dtype='float32', min=-1., max=1.)

conv = nn.Conv3D(4, 6, (3, 3, 3))
y_var = conv(x_var)
y_np = y_var.numpy()
print(y_np.shape)
# (2, 6, 6, 6, 6)
import paddle
import paddle.nn as nn

paddle.disable_static()

x_var = paddle.uniform((2, 4, 8, 8, 8), dtype='float32', min=-1., max=1.)

conv = nn.Conv3DTranspose(4, 6, (3, 3, 3))
y_var = conv(x_var)
y_np = y_var.numpy()
print(y_np.shape)
# (2, 6, 10, 10, 10)
import paddle
import paddle.nn as nn
import numpy as np

np.random.seed(0)
x1 = np.random.rand(2,3)
x2 = np.random.rand(2,3)
x1 = paddle.to_tensor(x1)
x2 = paddle.to_tensor(x2)

cos_sim_func = nn.CosineSimilarity(axis=0)
result = cos_sim_func(x1, x2)
print(result)
# [0.99806249 0.9817672  0.94987036]
import paddle
paddle.seed(99999)
N=100
C=200
reduction='mean'
input =  paddle.rand([N, C], dtype='float64')
label =  paddle.randint(0, C, shape=[N], dtype='int64')
weight = paddle.rand([C], dtype='float64')

cross_entropy_loss = paddle.nn.loss.CrossEntropyLoss(
    weight=weight, reduction=reduction)
dy_ret = cross_entropy_loss(
                           input,
                           label)
print(dy_ret.numpy()) #[5.41993642]
import paddle
paddle.seed(99999)
axis = -1
ignore_index = -100
N = 4
C = 3
shape = [N, C]
reduction='mean'
weight = None
logits = paddle.uniform(shape, dtype='float64', min=0.1, max=1.0)
labels = paddle.uniform(shape, dtype='float64', min=0.1, max=1.0)
labels /= paddle.sum(labels, axis=axis, keepdim=True)
paddle_loss_mean = paddle.nn.functional.cross_entropy(
                                                      logits,
                                                      labels,
                                                      soft_label=True,
                                                      axis=axis,
                                                      weight=weight,
                                                      reduction=reduction)
print(paddle_loss_mean.numpy()) #[1.12908343]
# declarative mode
import numpy as np
import paddle

# length of the longest logit sequence
max_seq_length = 4
#length of the longest label sequence
max_label_length = 3
# number of logit sequences
batch_size = 2
# class num
class_num = 3

np.random.seed(1)
log_probs = np.array([[[4.17021990e-01, 7.20324516e-01, 1.14374816e-04],
                        [3.02332580e-01, 1.46755889e-01, 9.23385918e-02]],

                        [[1.86260208e-01, 3.45560730e-01, 3.96767467e-01],
                        [5.38816750e-01, 4.19194520e-01, 6.85219526e-01]],

                        [[2.04452246e-01, 8.78117442e-01, 2.73875929e-02],
                        [6.70467496e-01, 4.17304814e-01, 5.58689833e-01]],

                        [[1.40386939e-01, 1.98101491e-01, 8.00744593e-01],
                        [9.68261600e-01, 3.13424170e-01, 6.92322612e-01]],

                        [[8.76389146e-01, 8.94606650e-01, 8.50442126e-02],
                        [3.90547849e-02, 1.69830427e-01, 8.78142476e-01]]]).astype("float32")
labels = np.array([[1, 2, 2],
                [1, 2, 2]]).astype("int32")
input_lengths = np.array([5, 5]).astype("int64")
label_lengths = np.array([3, 3]).astype("int64")

log_probs = paddle.to_tensor(log_probs)
labels = paddle.to_tensor(labels)
input_lengths = paddle.to_tensor(input_lengths)
label_lengths = paddle.to_tensor(label_lengths)

loss = paddle.nn.CTCLoss(blank=0, reduction='none')(log_probs, labels,
    input_lengths,
    label_lengths)
print(loss)  #[3.9179852 2.9076521]

loss = paddle.nn.CTCLoss(blank=0, reduction='mean')(log_probs, labels,
    input_lengths,
    label_lengths)
print(loss)  #[1.1376063]
import paddle
import numpy as np

x = np.array([[1,2,3], [4,5,6]]).astype('float32')
x = paddle.to_tensor(x)
m = paddle.nn.Dropout(p=0.5)
y_train = m(x)
m.eval()  # switch the model to test phase
y_test = m(x)
print(x)
print(y_train)
print(y_test)
import paddle
import numpy as np

x = np.random.random(size=(2, 3, 4, 5)).astype('float32')
x = paddle.to_tensor(x)
m = paddle.nn.Dropout2D(p=0.5)
y_train = m(x)
m.eval()  # switch the model to test phase
y_test = m(x)
print(x)
print(y_train)
print(y_test)
import paddle
import numpy as np

x = np.random.random(size=(2, 3, 4, 5, 6)).astype('float32')
x = paddle.to_tensor(x)
m = paddle.nn.Dropout3D(p=0.5)
y_train = m(x)
m.eval()  # switch the model to test phase
y_test = m(x)
print(x)
print(y_train)
print(y_test)
import numpy as np
import paddle
from paddle.nn import BeamSearchDecoder, dynamic_decode
from paddle.nn import GRUCell, Linear, Embedding
trg_embeder = Embedding(100, 32)
output_layer = Linear(32, 32)
decoder_cell = GRUCell(input_size=32, hidden_size=32)
decoder = BeamSearchDecoder(decoder_cell,
                            start_token=0,
                            end_token=1,
                            beam_size=4,
                            embedding_fn=trg_embeder,
                            output_fn=output_layer)
encoder_output = paddle.ones((4, 8, 32), dtype=paddle.get_default_dtype())
outputs = dynamic_decode(decoder=decoder,
                        inits=decoder_cell.get_initial_states(encoder_output),
                        max_step_num=10)
import paddle

x = paddle.to_tensor([[-1. ,6.], [1., 15.6]])
m = paddle.nn.ELU(0.2)
out = m(x)
# [[-0.12642411  6.        ]
#  [ 1.          15.6      ]]
import paddle
import numpy as np

x_data = np.arange(3, 6).reshape((3, 1)).astype(np.int64)
y_data = np.arange(6, 12).reshape((3, 2)).astype(np.float32)

x = paddle.to_tensor(x_data, stop_gradient=False)
y = paddle.to_tensor(y_data, stop_gradient=False)

embedding = paddle.nn.Embedding(10, 3, sparse=True)

w0=np.full(shape=(10, 3), fill_value=2).astype(np.float32)
embedding.weight.set_value(w0)

adam = paddle.optimizer.Adam(parameters=[embedding.weight], learning_rate=0.01)
adam.clear_grad()

# weight.shape = [10, 3]

# x.data = [[3],[4],[5]]
# x.shape = [3, 1]

# out.data = [[2,2,2], [2,2,2], [2,2,2]]
# out.shape = [3, 1, 3]
out=embedding(x)
out.backward()
adam.step()
import paddle
import numpy as np

inp_np = np.ones([5, 2, 3, 4]).astype('float32')
inp_np = paddle.to_tensor(inp_np)
flatten = paddle.nn.Flatten(start_axis=1, stop_axis=2)
flatten_res = flatten(inp_np)
import paddle
import numpy as np

x = paddle.to_tensor(np.array([[-1, 0.5],[1, 1.5]]))

m = paddle.nn.GELU()
out = m(x) # [-0.158655 0.345731 0.841345 1.39979]

m = paddle.nn.GELU(True)
out = m(x) # [-0.158808 0.345714 0.841192 1.39957]
import paddle
import numpy as np

paddle.disable_static()
np.random.seed(123)
x_data = np.random.random(size=(2, 6, 2, 2)).astype('float32')
x = paddle.to_tensor(x_data)
group_norm = paddle.nn.GroupNorm(num_channels=6, num_groups=6)
group_norm_out = group_norm(x)

print(group_norm_out.numpy())
import paddle

rnn = paddle.nn.GRU(16, 32, 2)

x = paddle.randn((4, 23, 16))
prev_h = paddle.randn((2, 4, 32))
y, h = rnn(x, prev_h)

print(y.shape)
print(h.shape)

#[4,23,32]
#[2,4,32]
import paddle

x = paddle.randn((4, 16))
prev_h = paddle.randn((4, 32))

cell = paddle.nn.GRUCell(16, 32)
y, h = cell(x, prev_h)

print(y.shape)
print(h.shape)

#[4,32]
#[4,32]
import paddle

x = paddle.to_tensor([-1, 0.3, 2.5])
m = paddle.nn.Hardshrink()
out = m(x) # [-1., 0., 2.5]
import paddle

m = paddle.nn.Hardsigmoid()
x = paddle.to_tensor([-4., 5., 1.])
out = m(x) # [0., 1, 0.666667]
import paddle

x = paddle.to_tensor([-4., 5., 1.])
m = paddle.nn.Hardswish()
out = m(x) # [0., 5., 0.666667]
import paddle

x = paddle.to_tensor([-1.5, 0.3, 2.5])
m = paddle.nn.Hardtanh()
out = m(x) # [-1., 0.3, 1.]
import paddle
paddle.set_device('cpu')

input = paddle.uniform([2, 3])
# [[-0.2820413   0.9528898  -0.81638825] # random
#  [-0.6733154  -0.33866507  0.25770962]] # random
label = paddle.to_tensor([0, 1, 4, 5])
m = paddle.nn.HSigmoidLoss(3, 5)
out = m(input, label)
# [[2.4543471]
#  [1.9359267]]
import paddle
import numpy as np

np.random.seed(123)
x_data = np.random.random(size=(2, 2, 3)).astype('float32')
x = paddle.to_tensor(x_data)
instance_norm = paddle.nn.InstanceNorm1D(2)
instance_norm_out = instance_norm(x)

print(instance_norm_out)
import paddle
import numpy as np

np.random.seed(123)
x_data = np.random.random(size=(2, 2, 2, 3)).astype('float32')
x = paddle.to_tensor(x_data)
instance_norm = paddle.nn.InstanceNorm2D(2)
instance_norm_out = instance_norm(x)

print(instance_norm_out)
import paddle
import numpy as np

np.random.seed(123)
x_data = np.random.random(size=(2, 2, 2, 2, 3)).astype('float32')
x = paddle.to_tensor(x_data)
instance_norm = paddle.nn.InstanceNorm3D(2)
instance_norm_out = instance_norm(x)

print(instance_norm_out.numpy)
import paddle
import numpy as np
import paddle.nn as nn

shape = (5, 20)
x = np.random.uniform(-10, 10, shape).astype('float32')
target = np.random.uniform(-10, 10, shape).astype('float32')

# 'batchmean' reduction, loss shape will be [1]
kldiv_criterion = nn.KLDivLoss(reduction='batchmean')
pred_loss = kldiv_criterion(paddle.to_tensor(x),
                            paddle.to_tensor(target))
# shape=[1]

# 'mean' reduction, loss shape will be [1]
kldiv_criterion = nn.KLDivLoss(reduction='mean')
pred_loss = kldiv_criterion(paddle.to_tensor(x),
                            paddle.to_tensor(target))
# shape=[1]

# 'sum' reduction, loss shape will be [1]
kldiv_criterion = nn.KLDivLoss(reduction='sum')
pred_loss = kldiv_criterion(paddle.to_tensor(x),
                            paddle.to_tensor(target))
# shape=[1]

# 'none' reduction, loss shape is same with X shape
kldiv_criterion = nn.KLDivLoss(reduction='none')
pred_loss = kldiv_criterion(paddle.to_tensor(x),
                            paddle.to_tensor(target))
# shape=[5, 20]
import paddle
import numpy as np

input_data = np.array([[1.5, 0.8], [0.2, 1.3]]).astype("float32")
label_data = np.array([[1.7, 1], [0.4, 0.5]]).astype("float32")
input = paddle.to_tensor(input_data)
label = paddle.to_tensor(label_data)

l1_loss = paddle.nn.L1Loss()
output = l1_loss(input, label)
print(output.numpy())
# [0.35]

l1_loss = paddle.nn.L1Loss(reduction='sum')
output = l1_loss(input, label)
print(output.numpy())
# [1.4]

l1_loss = paddle.nn.L1Loss(reduction='none')
output = l1_loss(input, label)
print(output)
# [[0.20000005 0.19999999]
# [0.2        0.79999995]]
import paddle

class MyLayer(paddle.nn.Layer):
    def __init__(self):
        super(MyLayer, self).__init__()
        self._linear = paddle.nn.Linear(1, 1)
        self._dropout = paddle.nn.Dropout(p=0.5)

    def forward(self, input):
        temp = self._linear(input)
        temp = self._dropout(temp)
        return temp

x = paddle.randn([10, 1], 'float32')
mylayer = MyLayer()
mylayer.eval()  # set mylayer._dropout to eval mode
out = mylayer(x)
mylayer.train()  # set mylayer._dropout to train mode
out = mylayer(x)
import paddle

class MyLayer(paddle.nn.Layer):
    def __init__(self):
        super(MyLayer, self).__init__()
        self._linear = paddle.nn.Linear(1, 1)
        self._dropout = paddle.nn.Dropout(p=0.5)

    def forward(self, input):
        temp = self._linear(input)
        temp = self._dropout(temp)
        return temp

x = paddle.randn([10, 1], 'float32')
mylayer = MyLayer()
mylayer.eval()  # set mylayer._dropout to eval mode
out = mylayer(x)
print(out)
import paddle
import paddle.nn as nn

net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2))

def init_weights(layer):
    if type(layer) == nn.Linear:
        print('before init weight:', layer.weight.numpy())
        new_weight = paddle.full(shape=layer.weight.shape, dtype=layer.weight.dtype, fill_value=0.9)
        layer.weight.set_value(new_weight)
        print('after init weight:', layer.weight.numpy())

net.apply(init_weights)

print(net.state_dict())
import paddle

class LinearNet(paddle.nn.Layer):
    def __init__(self):
        super(LinearNet, self).__init__(name_scope = "demo_linear_net")
        self._linear = paddle.nn.Linear(1, 1)

    def forward(self, x):
        return self._linear(x)

linear_net = LinearNet()
print(linear_net.full_name())   # demo_linear_net_0
import paddle
import numpy as np

# the forward_post_hook change the output of the layer: output = output * 2
def forward_post_hook(layer, input, output):
    # user can use layer, input and output for information statistis tasks

    # change the output
    return output * 2

linear = paddle.nn.Linear(13, 5)

# register the hook
forward_post_hook_handle = linear.register_forward_post_hook(forward_post_hook)

value1 = np.arange(26).reshape(2, 13).astype("float32")
in1 = paddle.to_tensor(value1)

out0 = linear(in1)

# remove the hook
forward_post_hook_handle.remove()

out1 = linear(in1)

# hook change the linear's output to output * 2, so out0 is equal to out1 * 2.
assert (out0.numpy() == (out1.numpy()) * 2).any()
import paddle
import numpy as np

# the forward_post_hook change the input of the layer: input = input * 2
def forward_pre_hook(layer, input):
    # user can use layer and input for information statistis tasks

    # change the input
    input_return = (input[0] * 2)
    return input_return

linear = paddle.nn.Linear(13, 5)

# register the hook
forward_pre_hook_handle = linear.register_forward_pre_hook(forward_pre_hook)

value0 = np.arange(26).reshape(2, 13).astype("float32")
in0 = paddle.to_tensor(value0)
out0 = linear(in0)

# remove the hook
forward_pre_hook_handle.remove()

value1 = value0 * 2
in1 = paddle.to_tensor(value1)
out1 = linear(in1)

# hook change the linear's input to input * 2, so out0 is equal to out1.
assert (out0.numpy() == out1.numpy()).any()
import paddle

class MyLayer(paddle.nn.Layer):
    def __init__(self):
        super(MyLayer, self).__init__()
        self._linear = paddle.nn.Linear(1, 1)
        w_tmp = self.create_parameter([1,1])
        self.add_parameter("w_tmp", w_tmp)

    def forward(self, input):
        return self._linear(input)

mylayer = MyLayer()
for name, param in mylayer.named_parameters():
    print(name, param)      # will print w_tmp,_linear.weight,_linear.bias
import paddle

class MyLinear(paddle.nn.Layer):
    def __init__(self,
                in_features,
                out_features):
        super(MyLinear, self).__init__()
        self.linear = paddle.nn.Linear( 10, 10)

        self.back_var = self.create_variable(name = "linear_tmp_0", dtype=self._dtype)

    def forward(self, input):
        out = self.linear(input)
        paddle.assign( out, self.back_var)

        return out
import paddle

class MyLinear(paddle.nn.Layer):
    def __init__(self,
                in_features,
                out_features):
        super(MyLinear, self).__init__()
        self.linear = paddle.nn.Linear( 10, 10)

        self.back_var = self.create_tensor(name = "linear_tmp_0", dtype=self._dtype)

    def forward(self, input):
        out = self.linear(input)
        paddle.assign( out, self.back_var)

        return out

import paddle

linear1 = paddle.nn.Linear(10, 3)
linear2 = paddle.nn.Linear(3, 10, bias_attr=False)
model = paddle.nn.Sequential(linear1, linear2)

layer_list = list(model.children())

print(layer_list)   # [<paddle.nn.layer.common.Linear object at 0x7f7b8113f830>, <paddle.nn.layer.common.Linear object at 0x7f7b8113f950>]
import paddle

linear1 = paddle.nn.Linear(10, 3)
linear2 = paddle.nn.Linear(3, 10, bias_attr=False)
model = paddle.nn.Sequential(linear1, linear2)
for prefix, layer in model.named_children():
    print(prefix, layer)
    # ('0', <paddle.nn.layer.common.Linear object at 0x7fb61ed85830>)
    # ('1', <paddle.nn.layer.common.Linear object at 0x7fb61ed85950>)
import paddle

class MyLayer(paddle.nn.Layer):
    def __init__(self):
        super(MyLayer, self).__init__()
        self._linear = paddle.nn.Linear(1, 1)
        self._dropout = paddle.nn.Dropout(p=0.5)

    def forward(self, input):
        temp = self._linear(input)
        temp = self._dropout(temp)
        return temp

mylayer = MyLayer()
print(mylayer.sublayers())  # [<paddle.nn.layer.common.Linear object at 0x7f44b58977d0>, <paddle.nn.layer.common.Dropout object at 0x7f44b58978f0>]
import paddle

fc1 = paddle.nn.Linear(10, 3)
fc2 = paddle.nn.Linear(3, 10, bias_attr=False)
model = paddle.nn.Sequential(fc1, fc2)
for name, param in model.named_parameters():
    print(name, param)
import paddle

fc1 = paddle.nn.Linear(10, 3)
fc2 = paddle.nn.Linear(3, 10, bias_attr=False)
model = paddle.nn.Sequential(fc1, fc2)
for prefix, layer in model.named_sublayers():
    print(prefix, layer)
import numpy as np
import paddle

linear = paddle.nn.Linear(10, 3)
value = np.array([0]).astype("float32")
buffer = paddle.to_tensor(value)
linear.register_buffer("buf_name", buffer, persistable=True)

# get the buffer by attribute.
print(linear.buf_name)
import numpy as np
import paddle

linear = paddle.nn.Linear(10, 3)
value = np.array([0]).astype("float32")
buffer = paddle.to_tensor(value)
linear.register_buffer("buf_name", buffer, persistable=True)

print(linear.buffers())     # == print([linear.buf_name])
import numpy as np
import paddle

fc1 = paddle.nn.Linear(10, 3)
buffer1 = paddle.to_tensor(np.array([0]).astype("float32"))
# register a tensor as buffer by specific `persistable`
fc1.register_buffer("buf_name_1", buffer1, persistable=True)

fc2 = paddle.nn.Linear(3, 10)
buffer2 = paddle.to_tensor(np.array([1]).astype("float32"))
# register a buffer by assigning an attribute with Tensor.
# The `persistable` can only be False by this way.
fc2.buf_name_2 = buffer2

model = paddle.nn.Sequential(fc1, fc2)

# get all named buffers
for name, buffer in model.named_buffers():
    print(name, buffer)
import paddle
import numpy as np

value = np.arange(26).reshape(2, 13).astype("float32")
a = paddle.to_tensor(value)
linear = paddle.nn.Linear(13, 5)
adam = paddle.optimizer.Adam(learning_rate=0.01,
                            parameters=linear.parameters())
out = linear(a)
out.backward()
adam.step()
linear.clear_gradients()
import paddle

class MySequential(paddle.nn.Layer):
    def __init__(self, *layers):
        super(MySequential, self).__init__()
        if len(layers) > 0 and isinstance(layers[0], tuple):
            for name, layer in layers:
                self.add_sublayer(name, layer)
        else:
            for idx, layer in enumerate(layers):
                self.add_sublayer(str(idx), layer)

    def forward(self, input):
        for layer in self._sub_layers.values():
            input = layer(input)
        return input

fc1 = paddle.nn.Linear(10, 3)
fc2 = paddle.nn.Linear(3, 10, bias_attr=False)
model = MySequential(fc1, fc2)
for prefix, layer in model.named_sublayers():
    print(prefix, layer)
import paddle

class MyLayer(paddle.nn.Layer):
    def __init__(self):
        super(MyLayer, self).__init__()
        self._linear = paddle.nn.Linear(1, 1)
        w_tmp = self.create_parameter([1,1])
        self.add_parameter("w_tmp", w_tmp)

    def forward(self, input):
        return self._linear(input)

mylayer = MyLayer()
for name, param in mylayer.named_parameters():
    print(name, param)      # will print w_tmp,_linear.weight,_linear.bias
import paddle

emb = paddle.nn.Embedding(10, 10)

state_dict = emb.to_static_state_dict()
paddle.save( state_dict, "paddle_dy.pdparams")
import paddle

emb = paddle.nn.Embedding(10, 10)

state_dict = emb.state_dict()
paddle.save( state_dict, "paddle_dy.pdparams")
import paddle

emb = paddle.nn.Embedding(10, 10)

state_dict = emb.state_dict()
paddle.save(state_dict, "paddle_dy.pdparams")
para_state_dict = paddle.load("paddle_dy.pdparams")
emb.set_state_dict(para_state_dict)
# required: skip
import paddle

linear=paddle.nn.Linear(2, 2)
linear.weight
#Parameter containing:
#Tensor(shape=[2, 2], dtype=float32, place=CUDAPlace(0), stop_gradient=False,
#       [[-0.32770029,  0.38653070],
#        [ 0.46030545,  0.08158520]])

linear.to(dtype='float64')
linear.weight
#Tenor(shape=[2, 2], dtype=float64, place=CUDAPlace(0), stop_gradient=False,
#       [[-0.32770029,  0.38653070],
#        [ 0.46030545,  0.08158520]])

linear.to(device='cpu')
linear.weight
#Tensor(shape=[2, 2], dtype=float64, place=CPUPlace, stop_gradient=False,
#       [[-0.32770029,  0.38653070],
#        [ 0.46030545,  0.08158520]])
linear.to(device=paddle.CUDAPinnedPlace(), blocking=False)
linear.weight
#Tensor(shape=[2, 2], dtype=float64, place=CUDAPinnedPlace, stop_gradient=False,
#       [[-0.04989364, -0.56889004],
#        [ 0.33960250,  0.96878713]])
import paddle

emb = paddle.nn.Embedding(10, 10)

state_dict = emb.state_dict()
paddle.save(state_dict, "paddle_dy.pdparams")
para_state_dict = paddle.load("paddle_dy.pdparams")
emb.set_state_dict(para_state_dict)
import paddle

emb = paddle.nn.Embedding(10, 10)

state_dict = emb.state_dict()
paddle.save(state_dict, "paddle_dy.pdparams")
para_state_dict = paddle.load("paddle_dy.pdparams")
emb.set_state_dict(para_state_dict)
import paddle
import numpy as np
from collections import OrderedDict

sublayers = OrderedDict([
    ('conv1d', paddle.nn.Conv1D(3, 2, 3)),
    ('conv2d', paddle.nn.Conv2D(3, 2, 3)),
    ('conv3d', paddle.nn.Conv3D(4, 6, (3, 3, 3))),
])

layers_dict = paddle.nn.LayerDict(sublayers=sublayers)

l = layers_dict['conv1d']

for k in layers_dict:
    l = layers_dict[k]

len(layers_dict)
#3

del layers_dict['conv2d']
len(layers_dict)
#2

conv1d = layers_dict.pop('conv1d')
len(layers_dict)
#1

layers_dict.clear()
len(layers_dict)
#0
import paddle
from collections import OrderedDict

sublayers = OrderedDict([
    ('conv1d', paddle.nn.Conv1D(3, 2, 3)),
    ('conv2d', paddle.nn.Conv2D(3, 2, 3)),
    ('conv3d', paddle.nn.Conv3D(4, 6, (3, 3, 3))),
])

layer_dict = paddle.nn.LayerDict(sublayers=sublayers)
len(layer_dict)
#3

layer_dict.clear()
len(layer_dict)
#0
import paddle
from collections import OrderedDict

sublayers = OrderedDict([
    ('conv1d', paddle.nn.Conv1D(3, 2, 3)),
    ('conv2d', paddle.nn.Conv2D(3, 2, 3)),
    ('conv3d', paddle.nn.Conv3D(4, 6, (3, 3, 3))),
])

layer_dict = paddle.nn.LayerDict(sublayers=sublayers)
len(layer_dict)
#3

layer_dict.pop('conv2d')
len(layer_dict)
#2
import paddle
from collections import OrderedDict

sublayers = OrderedDict([
    ('conv1d', paddle.nn.Conv1D(3, 2, 3)),
    ('conv2d', paddle.nn.Conv2D(3, 2, 3)),
    ('conv3d', paddle.nn.Conv3D(4, 6, (3, 3, 3))),
])

layer_dict = paddle.nn.LayerDict(sublayers=sublayers)
for k in layer_dict.keys():
    print(k)

#conv1d
#conv2d
#conv3d
import paddle
from collections import OrderedDict

sublayers = OrderedDict([
    ('conv1d', paddle.nn.Conv1D(3, 2, 3)),
    ('conv2d', paddle.nn.Conv2D(3, 2, 3)),
    ('conv3d', paddle.nn.Conv3D(4, 6, (3, 3, 3))),
])

layer_dict = paddle.nn.LayerDict(sublayers=sublayers)
for k, v in layer_dict.items():
    print(k, ":", v)

#conv1d : Conv1D(3, 2, kernel_size=[3], data_format=NCL)
#conv2d : Conv2D(3, 2, kernel_size=[3, 3], data_format=NCHW)
#conv3d : Conv3D(4, 6, kernel_size=[3, 3, 3], data_format=NCDHW)
import paddle
from collections import OrderedDict

sublayers = OrderedDict([
    ('conv1d', paddle.nn.Conv1D(3, 2, 3)),
    ('conv2d', paddle.nn.Conv2D(3, 2, 3)),
    ('conv3d', paddle.nn.Conv3D(4, 6, (3, 3, 3))),
])

layer_dict = paddle.nn.LayerDict(sublayers=sublayers)
for v in layer_dict.values():
    print(v)

#Conv1D(3, 2, kernel_size=[3], data_format=NCL)
#Conv2D(3, 2, kernel_size=[3, 3], data_format=NCHW)
#Conv3D(4, 6, kernel_size=[3, 3, 3], data_format=NCDHW)
import paddle
from collections import OrderedDict

sublayers = OrderedDict([
    ('conv1d', paddle.nn.Conv1D(3, 2, 3)),
    ('conv2d', paddle.nn.Conv2D(3, 2, 3)),
    ('conv3d', paddle.nn.Conv3D(4, 6, (3, 3, 3))),
])

new_sublayers = OrderedDict([
    ('relu', paddle.nn.ReLU()),
    ('conv2d', paddle.nn.Conv2D(4, 2, 4)),
])
layer_dict = paddle.nn.LayerDict(sublayers=sublayers)

layer_dict.update(new_sublayers)

for k, v in layer_dict.items():
    print(k, ":", v)
#conv1d : Conv1D(3, 2, kernel_size=[3], data_format=NCL)
#conv2d : Conv2D(4, 2, kernel_size=[4, 4], data_format=NCHW)
#conv3d : Conv3D(4, 6, kernel_size=[3, 3, 3], data_format=NCDHW)
#relu : ReLU()
import paddle
import numpy as np

class MyLayer(paddle.nn.Layer):
    def __init__(self):
        super(MyLayer, self).__init__()
        self.linears = paddle.nn.LayerList(
            [paddle.nn.Linear(10, 10) for i in range(10)])

    def forward(self, x):
        # LayerList can act as an iterable, or be indexed using ints
        for i, l in enumerate(self.linears):
            x = self.linears[i // 2](x) + l(x)
        return x
import paddle

linears = paddle.nn.LayerList([paddle.nn.Linear(10, 10) for i in range(10)])
another = paddle.nn.Linear(10, 10)
linears.append(another)
print(len(linears))  # 11
import paddle

linears = paddle.nn.LayerList([paddle.nn.Linear(10, 10) for i in range(10)])
another = paddle.nn.Linear(10, 10)
linears.insert(3, another)
print(linears[3] is another)  # True
another = paddle.nn.Linear(10, 10)
linears.insert(-1, another)
print(linears[-2] is another) # True
import paddle

linears = paddle.nn.LayerList([paddle.nn.Linear(10, 10) for i in range(10)])
another_list = paddle.nn.LayerList([paddle.nn.Linear(10, 10) for i in range(5)])
linears.extend(another_list)
print(len(linears))  # 15
print(another_list[0] is linears[10])  # True
import paddle
import numpy as np

np.random.seed(123)
x_data = np.random.random(size=(2, 2, 2, 3)).astype('float32')
x = paddle.to_tensor(x_data)
layer_norm = paddle.nn.LayerNorm(x_data.shape[1:])
layer_norm_out = layer_norm(x)

print(layer_norm_out)
import paddle
import numpy as np

m = paddle.nn.LeakyReLU()
x = paddle.to_tensor(np.array([-2, 0, 1], 'float32'))
out = m(x)  # [-0.02, 0., 1.]
import paddle

# Define the linear layer.
weight_attr = paddle.ParamAttr(
    name="weight",
    initializer=paddle.nn.initializer.Constant(value=0.5))
bias_attr = paddle.ParamAttr(
    name="bias",
    initializer=paddle.nn.initializer.Constant(value=1.0))
linear = paddle.nn.Linear(2, 4, weight_attr=weight_attr, bias_attr=bias_attr)
# linear.weight: [[0.5 0.5 0.5 0.5]
#                 [0.5 0.5 0.5 0.5]]
# linear.bias: [1. 1. 1. 1.]

x = paddle.randn((3, 2), dtype="float32")
# x: [[-0.32342386 -1.200079  ]
#     [ 0.7979031  -0.90978354]
#     [ 0.40597573  1.8095392 ]]
y = linear(x)
# y: [[0.23824859 0.23824859 0.23824859 0.23824859]
#     [0.9440598  0.9440598  0.9440598  0.9440598 ]
#     [2.1077576  2.1077576  2.1077576  2.1077576 ]]
import paddle

x = paddle.rand(shape=(3, 3, 112, 112), dtype="float32")
m = paddle.nn.LocalResponseNorm(size=5)
y = m(x)
print(y.shape)  # [3, 3, 112, 112]
import paddle

x = paddle.to_tensor([1.0, 2.0, 3.0, 4.0])
m = paddle.nn.LogSigmoid()
out = m(x) # [-0.313262 -0.126928 -0.0485874 -0.0181499]
import paddle

x = [[[-2.0, 3.0, -4.0, 5.0],
      [3.0, -4.0, 5.0, -6.0],
      [-7.0, -8.0, 8.0, 9.0]],
     [[1.0, -2.0, -3.0, 4.0],
      [-5.0, 6.0, 7.0, -8.0],
      [6.0, 7.0, 8.0, 9.0]]]
m = paddle.nn.LogSoftmax()
x = paddle.to_tensor(x)
out = m(x)
# [[[ -7.1278396   -2.1278396   -9.127839    -0.12783948]
#   [ -2.1270514   -9.127051    -0.12705144 -11.127051  ]
#   [-16.313261   -17.313261    -1.3132617   -0.31326184]]
#  [[ -3.0518122   -6.051812    -7.051812    -0.051812  ]
#   [-12.313267    -1.3132664   -0.3132665  -15.313267  ]
#   [ -3.4401896   -2.4401896   -1.4401896   -0.44018966]]]
import paddle

rnn = paddle.nn.LSTM(16, 32, 2)

x = paddle.randn((4, 23, 16))
prev_h = paddle.randn((2, 4, 32))
prev_c = paddle.randn((2, 4, 32))
y, (h, c) = rnn(x, (prev_h, prev_c))

print(y.shape)
print(h.shape)
print(c.shape)

#[4,23,32]
#[2,4,32]
#[2,4,32]
import paddle

x = paddle.randn((4, 16))
prev_h = paddle.randn((4, 32))
prev_c = paddle.randn((4, 32))

cell = paddle.nn.LSTMCell(16, 32)
y, (h, c) = cell(x, (prev_h, prev_c))

print(y.shape)
print(h.shape)
print(c.shape)

#[4,32]
#[4,32]
#[4,32]
import paddle

input = paddle.to_tensor([[1, 2], [3, 4]], dtype="float32")
other = paddle.to_tensor([[2, 1], [2, 4]], dtype="float32")
label = paddle.to_tensor([[1, -1], [-1, -1]], dtype="float32")
margin_rank_loss = paddle.nn.MarginRankingLoss()
loss = margin_rank_loss(input, other, label)

print(loss)
# [0.75]
import paddle

x = paddle.rand([1, 2, 3, 4])
# [[[[0.5002636  0.22272532 0.17402348 0.2874594 ]
#    [0.95313174 0.6228939  0.7129065  0.7087491 ]
#    [0.02879342 0.88725346 0.61093384 0.38833922]]
#   [[0.5231306  0.03807496 0.91661984 0.15602879]
#    [0.666127   0.616567   0.30741522 0.24044901]
#    [0.7142536  0.7351477  0.31588817 0.23782359]]]]
m = paddle.nn.Maxout(groups=2)
out = m(x)
# [[[[0.5231306  0.22272532 0.91661984 0.2874594 ]
#    [0.95313174 0.6228939  0.7129065  0.7087491 ]
#    [0.7142536  0.88725346 0.61093384 0.38833922]]]]
import paddle
import paddle.nn as nn
import numpy as np

data = paddle.to_tensor(np.random.uniform(-1, 1, [1, 3, 32]).astype(np.float32))
MaxPool1D = nn.MaxPool1D(kernel_size=2, stride=2, padding=0)
pool_out = MaxPool1D(data)
# pool_out shape: [1, 3, 16]

MaxPool1D = nn.MaxPool1D(kernel_size=2, stride=2, padding=0, return_mask=True)
pool_out, indices = MaxPool1D(data)
# pool_out shape: [1, 3, 16], indices shape: [1, 3, 16]
import paddle
import paddle.nn as nn
import numpy as np

# max pool2d
input = paddle.to_tensor(np.random.uniform(-1, 1, [1, 3, 32, 32]).astype(np.float32))
MaxPool2D = nn.MaxPool2D(kernel_size=2,
                       stride=2, padding=0)
output = MaxPool2D(input)
# output.shape [1, 3, 16, 16]

# for return_mask=True
MaxPool2D = nn.MaxPool2D(kernel_size=2, stride=2, padding=0, return_mask=True)
output, max_indices = MaxPool2D(input)
# output.shape [1, 3, 16, 16], max_indices.shape [1, 3, 16, 16],
import paddle
import paddle.nn as nn
import numpy as np

# max pool3d
input = paddle.to_tensor(np.random.uniform(-1, 1, [1, 2, 3, 32, 32]).astype(np.float32))
MaxPool3D = nn.MaxPool3D(kernel_size=2,
                       stride=2, padding=0)
output = MaxPool3D(input)
# output.shape [1, 2, 3, 16, 16]

# for return_mask=True
MaxPool3D = nn.MaxPool3D(kernel_size=2, stride=2, padding=0, return_mask=True)
output, max_indices = MaxPool3D(input)
# output.shape [1, 2, 3, 16, 16], max_indices.shape [1, 2, 3, 16, 16],
import paddle

x = paddle.to_tensor([-5., 0., 5.])
m = paddle.nn.Mish()
out = m(x) # [-0.03357624, 0., 4.99955208]
import numpy as np
import paddle

input_data = np.array([1.5]).astype("float32")
label_data = np.array([1.7]).astype("float32")

mse_loss = paddle.nn.loss.MSELoss()
input = paddle.to_tensor(input_data)
label = paddle.to_tensor(label_data)
output = mse_loss(input, label)
print(output)
# [0.04000002]
import paddle

# encoder input: [batch_size, sequence_length, d_model]
query = paddle.rand((2, 4, 128))
# self attention mask: [batch_size, num_heads, query_len, query_len]
attn_mask = paddle.rand((2, 2, 4, 4))
multi_head_attn = paddle.nn.MultiHeadAttention(128, 2)
output = multi_head_attn(query, None, None, attn_mask=attn_mask)  # [2, 4, 128]
import paddle

nll_loss = paddle.nn.loss.NLLLoss()
log_softmax = paddle.nn.LogSoftmax(axis=1)

input = paddle.to_tensor([[0.88103855, 0.9908683 , 0.6226845 ],
                          [0.53331435, 0.07999352, 0.8549948 ],
                          [0.25879037, 0.39530203, 0.698465  ],
                          [0.73427284, 0.63575995, 0.18827209],
                          [0.05689114, 0.0862954 , 0.6325046 ]], "float32")
log_out = log_softmax(input)
label = paddle.to_tensor([0, 2, 1, 1, 0], "int64")
result = nll_loss(log_out, label)
print(result) # Tensor(shape=[1], dtype=float32, place=CPUPlace, stop_gradient=True, [1.07202101])
import paddle
import paddle.nn as nn
import numpy as np

input_shape = (1, 2, 3)
pad = [1, 2]
mode = "constant"
data = paddle.arange(np.prod(input_shape), dtype="float32").reshape(input_shape) + 1
my_pad = nn.Pad1D(padding=pad, mode=mode)
result = my_pad(data)
print(result)
# [[[0. 1. 2. 3. 0. 0.]
#   [0. 4. 5. 6. 0. 0.]]]
import paddle
import paddle.nn as nn
import numpy as np
input_shape = (1, 1, 2, 3)
pad = [1, 0, 1, 2]
mode = "constant"
data = paddle.arange(np.prod(input_shape), dtype="float32").reshape(input_shape) + 1
my_pad = nn.Pad2D(padding=pad, mode=mode)
result = my_pad(data)
print(result)
# [[[[0. 0. 0. 0.]
#    [0. 1. 2. 3.]
#    [0. 4. 5. 6.]
#    [0. 0. 0. 0.]
#    [0. 0. 0. 0.]]]]
import paddle
import paddle.nn as nn
import numpy as np
input_shape = (1, 1, 1, 2, 3)
pad = [1, 0, 1, 2, 0, 0]
mode = "constant"
data = paddle.arange(np.prod(input_shape), dtype="float32").reshape(input_shape) + 1
my_pad = nn.Pad3D(padding=pad, mode=mode)
result = my_pad(data)
print(result)
# [[[[[0. 0. 0. 0.]
#     [0. 1. 2. 3.]
#     [0. 4. 5. 6.]
#     [0. 0. 0. 0.]
#     [0. 0. 0. 0.]]]]]
import paddle
import numpy as np
paddle.disable_static()
x_np = np.array([[1., 3.], [3., 5.]]).astype(np.float64)
y_np = np.array([[5., 6.], [7., 8.]]).astype(np.float64)
x = paddle.to_tensor(x_np)
y = paddle.to_tensor(y_np)
dist = paddle.nn.PairwiseDistance()
distance = dist(x, y)
print(distance.numpy()) # [5. 5.]
import paddle
import numpy as np

class MyLayer(paddle.nn.Layer):
    def __init__(self, num_stacked_param):
        super(MyLayer, self).__init__()
        # create ParameterList with iterable Parameters
        self.params = paddle.nn.ParameterList(
            [paddle.create_parameter(
                shape=[2, 2], dtype='float32')] * num_stacked_param)

    def forward(self, x):
        for i, p in enumerate(self.params):
            tmp = self._helper.create_variable_for_type_inference('float32')
            self._helper.append_op(
                type="mul",
                inputs={"X": x,
                        "Y": p},
                outputs={"Out": tmp},
                attrs={"x_num_col_dims": 1,
                        "y_num_col_dims": 1})
            x = tmp
        return x

data_np = np.random.uniform(-1, 1, [5, 2]).astype('float32')
x = paddle.to_tensor(data_np)
num_stacked_param = 4
model = MyLayer(num_stacked_param)
print(len(model.params))  # 4
res = model(x)
print(res.shape)  # [5, 2]

replaced_param = paddle.create_parameter(shape=[2, 3], dtype='float32')
model.params[num_stacked_param - 1] = replaced_param  # replace last param
res = model(x)
print(res.shape)  # [5, 3]
model.params.append(paddle.create_parameter(shape=[3, 4], dtype='float32'))  # append param
print(len(model.params))  # 5
res = model(x)
print(res.shape)  # [5, 4]
import paddle
import paddle.nn as nn
import numpy as np

x = np.random.randn(2, 9, 4, 4).astype(np.float32)
x_var = paddle.to_tensor(x)
pixel_shuffle = nn.PixelShuffle(3)
out_var = pixel_shuffle(x_var)
out = out_var.numpy()
print(out.shape)
# (2, 1, 12, 12)
import paddle
import numpy as np

paddle.set_default_dtype("float64")

data = np.array([[[[-2.0,  3.0, -4.0,  5.0],
                [ 3.0, -4.0,  5.0, -6.0],
                [-7.0, -8.0,  8.0,  9.0]],
                [[ 1.0, -2.0, -3.0,  4.0],
                [-5.0,  6.0,  7.0, -8.0],
                [ 6.0,  7.0,  8.0,  9.0]]]], 'float64')
x = paddle.to_tensor(data)
m = paddle.nn.PReLU(1, 0.25)
out = m(x)
# [[[[-0.5 ,  3.  , -1.  ,  5.  ],
#    [ 3.  , -1.  ,  5.  , -1.5 ],
#    [-1.75, -2.  ,  8.  ,  9.  ]],
#   [[ 1.  , -0.5 , -0.75,  4.  ],
#    [-1.25,  6.  ,  7.  , -2.  ],
#    [ 6.  ,  7.  ,  8.  ,  9.  ]]]]
import paddle

x = paddle.to_tensor([-2., 0., 1.])
m = paddle.nn.ReLU()
out = m(x) # [0., 0., 1.]
import paddle
import numpy as np

x = paddle.to_tensor(np.array([-1, 0.3, 6.5]))
m = paddle.nn.ReLU6()
out = m(x) # [0, 0.3, 6]
import paddle

inputs = paddle.rand((4, 23, 16))
prev_h = paddle.randn((4, 32))

cell = paddle.nn.SimpleRNNCell(16, 32)
rnn = paddle.nn.RNN(cell)
outputs, final_states = rnn(inputs, prev_h)

print(outputs.shape)
print(final_states.shape)

#[4,23,32]
#[4,32]
import paddle
import numpy as np

x = paddle.to_tensor(np.array([[0.0, 1.0],[2.0, 3.0]]))
m = paddle.nn.SELU()
out = m(x) # [[0, 1.050701],[2.101402, 3.152103]]
import paddle
import numpy as np

data = np.random.uniform(-1, 1, [30, 10]).astype('float32')
data = paddle.to_tensor(data)
# create Sequential with iterable Layers
model1 = paddle.nn.Sequential(
    paddle.nn.Linear(10, 1), paddle.nn.Linear(1, 2)
)
model1[0]  # access the first layer
res1 = model1(data)  # sequential execution

# create Sequential with name Layer pairs
model2 = paddle.nn.Sequential(
    ('l1', paddle.nn.Linear(10, 2)),
    ('l2', paddle.nn.Linear(2, 3))
)
model2['l1']  # access l1 layer
model2.add_sublayer('l3', paddle.nn.Linear(3, 3))  # add sublayer
res2 = model2(data)  # sequential execution
import paddle

m = paddle.nn.Sigmoid()
x = paddle.to_tensor([1.0, 2.0, 3.0, 4.0])
out = m(x) # [0.7310586, 0.880797, 0.95257413, 0.98201376]
Silu(x) =
import paddle

x = paddle.to_tensor([1.0, 2.0, 3.0, 4.0])
m = paddle.nn.Silu()
out = m(x) # [ 0.731059, 1.761594, 2.857722, 3.928055 ]
import paddle

rnn = paddle.nn.SimpleRNN(16, 32, 2)

x = paddle.randn((4, 23, 16))
prev_h = paddle.randn((2, 4, 32))
y, h = rnn(x, prev_h)

print(y.shape)
print(h.shape)

#[4,23,32]
#[2,4,32]
import paddle

x = paddle.randn((4, 16))
prev_h = paddle.randn((4, 32))

cell = paddle.nn.SimpleRNNCell(16, 32)
y, h = cell(x, prev_h)
print(y.shape)

#[4,32]
import paddle
import numpy as np
input_data = np.random.rand(3,3).astype("float32")
label_data = np.random.rand(3,3).astype("float32")
input = paddle.to_tensor(input_data)
label = paddle.to_tensor(label_data)
loss = paddle.nn.SmoothL1Loss()
output = loss(input, label)
print(output)
import paddle
import numpy as np

x = np.array([[[2.0, 3.0, 4.0, 5.0],
            [3.0, 4.0, 5.0, 6.0],
            [7.0, 8.0, 8.0, 9.0]],
            [[1.0, 2.0, 3.0, 4.0],
            [5.0, 6.0, 7.0, 8.0],
            [6.0, 7.0, 8.0, 9.0]]], 'float32')
x = paddle.to_tensor(x)
m = paddle.nn.Softmax()
out = m(x)
# [[[0.0320586 , 0.08714432, 0.23688282, 0.64391426],
#   [0.0320586 , 0.08714432, 0.23688282, 0.64391426],
#   [0.07232949, 0.19661193, 0.19661193, 0.53444665]],
# [[0.0320586 , 0.08714432, 0.23688282, 0.64391426],
#   [0.0320586 , 0.08714432, 0.23688282, 0.64391426],
#   [0.0320586 , 0.08714432, 0.23688282, 0.64391426]]]
import paddle
import numpy as np

x = paddle.to_tensor(np.array([-0.4, -0.2, 0.1, 0.3]))
m = paddle.nn.Softplus()
out = m(x) # [0.513015, 0.598139, 0.744397, 0.854355]
import paddle
import numpy as np

x = paddle.to_tensor(np.array([-0.9, -0.2, 0.1, 0.8]))
m = paddle.nn.Softshrink()
out = m(x) # [-0.4, 0, 0, 0.3]
import paddle
import numpy as np

x = paddle.to_tensor(np.array([-0.4, -0.2, 0.1, 0.3]))
m = paddle.nn.Softsign()
out = m(x) # [-0.285714, -0.166667, 0.0909091, 0.230769]
import paddle
x = paddle.rand((2,8,32,32))

spectral_norm = paddle.nn.SpectralNorm(x.shape, dim=1, power_iters=2)
spectral_norm_out = spectral_norm(x)

print(spectral_norm_out.shape) # [2, 8, 32, 32]
import paddle
import numpy as np

x = paddle.to_tensor(np.array([-2., 0., 1.]))
m = paddle.nn.Swish()
out = m(x) # [-0.238406, 0., 0.731059]
import paddle
import paddle.nn as nn
import numpy as np

x = np.array([[[[0.3, 0.4], [0.3, 0.07]], [[0.83, 0.37], [0.18, 0.93]]]]).astype('float32')
x = paddle.to_tensor(x)

if paddle.is_compiled_with_cuda():
    sync_batch_norm = nn.SyncBatchNorm(2)
    hidden1 = sync_batch_norm(x)
    print(hidden1)
    # [[[[0.26824948, 1.0936325],[0.26824948, -1.6301316]],[[ 0.8095662, -0.665287],[-1.2744656, 1.1301866 ]]]]
.. code-block:: python
    import paddle
    import paddle.nn as nn

    model = nn.Sequential(nn.Conv2D(3, 5, 3), nn.BatchNorm2D(5))
    sync_model = nn.SyncBatchNorm.convert_sync_batchnorm(model)
import paddle
import numpy as np

x = paddle.to_tensor(np.array([-0.4, -0.2, 0.1, 0.3]))
m = paddle.nn.Tanh()
out = m(x)
print(out)
# [-0.37994896 -0.19737532  0.09966799  0.29131261]
import paddle
import numpy as np

x = paddle.to_tensor(np.array([-0.4, -0.2, 0.1, 0.3]))
m = paddle.nn.Tanhshrink()
out = m(x) # [-0.020051, -0.00262468, 0.000332005, 0.00868739]
import paddle
import numpy as np

x = paddle.to_tensor(np.array([2., 0., 1.]))
m = paddle.nn.ThresholdedReLU()
out = m(x) # [2., 0., 0.]
import paddle
from paddle.nn import Transformer

# src: [batch_size, tgt_len, d_model]
enc_input = paddle.rand((2, 4, 128))
# tgt: [batch_size, src_len, d_model]
dec_input = paddle.rand((2, 6, 128))
# src_mask: [batch_size, n_head, src_len, src_len]
enc_self_attn_mask = paddle.rand((2, 2, 4, 4))
# tgt_mask: [batch_size, n_head, tgt_len, tgt_len]
dec_self_attn_mask = paddle.rand((2, 2, 6, 6))
# memory_mask: [batch_size, n_head, tgt_len, src_len]
cross_attn_mask = paddle.rand((2, 2, 6, 4))
transformer = Transformer(128, 2, 4, 4, 512)
output = transformer(enc_input,
                     dec_input,
                     enc_self_attn_mask,
                     dec_self_attn_mask,
                     cross_attn_mask)  # [2, 6, 128]
import paddle
from paddle.nn.layer.transformer import Transformer
length = 5
d_model, n_head, dim_feedforward = 8, 4, 64
transformer_paddle = Transformer(
    d_model, n_head, dim_feedforward=dim_feedforward)
mask = transformer_paddle.generate_square_subsequent_mask(length)
print(mask)

# [[  0. -inf -inf -inf -inf]
# [  0.   0. -inf -inf -inf]
# [  0.   0.   0. -inf -inf]
# [  0.   0.   0.   0. -inf]
# [  0.   0.   0.   0.   0.]]
import paddle
from paddle.nn import TransformerDecoderLayer, TransformerDecoder

# decoder input: [batch_size, tgt_len, d_model]
dec_input = paddle.rand((2, 4, 128))
# encoder output: [batch_size, src_len, d_model]
enc_output = paddle.rand((2, 6, 128))
# self attention mask: [batch_size, n_head, tgt_len, tgt_len]
self_attn_mask = paddle.rand((2, 2, 4, 4))
# cross attention mask: [batch_size, n_head, tgt_len, src_len]
cross_attn_mask = paddle.rand((2, 2, 4, 6))
decoder_layer = TransformerDecoderLayer(128, 2, 512)
decoder = TransformerDecoder(decoder_layer, 2)
output = decoder(dec_input,
                 enc_output,
                 self_attn_mask,
                 cross_attn_mask)  # [2, 4, 128]
import paddle
from paddle.nn import TransformerDecoderLayer

# decoder input: [batch_size, tgt_len, d_model]
dec_input = paddle.rand((2, 4, 128))
# encoder output: [batch_size, src_len, d_model]
enc_output = paddle.rand((2, 6, 128))
# self attention mask: [batch_size, n_head, tgt_len, tgt_len]
self_attn_mask = paddle.rand((2, 2, 4, 4))
# cross attention mask: [batch_size, n_head, tgt_len, src_len]
cross_attn_mask = paddle.rand((2, 2, 4, 6))
decoder_layer = TransformerDecoderLayer(128, 2, 512)
output = decoder_layer(dec_input,
                       enc_output,
                       self_attn_mask,
                       cross_attn_mask)  # [2, 4, 128]
import paddle
from paddle.nn import TransformerEncoderLayer, TransformerEncoder

# encoder input: [batch_size, src_len, d_model]
enc_input = paddle.rand((2, 4, 128))
# self attention mask: [batch_size, n_head, src_len, src_len]
attn_mask = paddle.rand((2, 2, 4, 4))
encoder_layer = TransformerEncoderLayer(128, 2, 512)
encoder = TransformerEncoder(encoder_layer, 2)
enc_output = encoder(enc_input, attn_mask)  # [2, 4, 128]
import paddle
from paddle.nn import TransformerEncoderLayer

# encoder input: [batch_size, src_len, d_model]
enc_input = paddle.rand((2, 4, 128))
# self attention mask: [batch_size, n_head, src_len, src_len]
attn_mask = paddle.rand((2, 2, 4, 4))
encoder_layer = TransformerEncoderLayer(128, 2, 512)
enc_output = encoder_layer(enc_input, attn_mask)  # [2, 4, 128]
import paddle
import paddle.nn as nn

x = paddle.randn((100,3,224,224))
unfold = nn.Unfold(kernel_sizes=[3, 3])
result = unfold(x)
print(result)
import paddle
import paddle.nn as nn
import numpy as np

input_data = np.random.rand(2,3,6,10).astype("float32")
upsample_out  = paddle.nn.Upsample(size=[12,12])

input = paddle.to_tensor(input_data)
output = upsample_out(x=input)
print(output.shape)
# [2L, 3L, 12L, 12L]
import paddle
import paddle.nn as nn

input_data = paddle.rand(shape=(2,3,6,10)).astype("float32")
upsample_out  = paddle.nn.UpsamplingBilinear2D(size=[12,12])
input = paddle.to_tensor(input_data)
output = upsample_out(x=input)
print(output.shape)
# [2L, 3L, 12L, 12L]
import paddle
import paddle.nn as nn

input_data = paddle.rand(shape=(2,3,6,10)).astype("float32")
upsample_out  = paddle.nn.UpsamplingNearest2D(size=[12,12])
input = paddle.to_tensor(input_data)
output = upsample_out(x=input)
print(output.shape)
# [2L, 3L, 12L, 12L]
import paddle
import numpy as np

class LinearNet(paddle.nn.Layer):
    def __init__(self):
        super(LinearNet, self).__init__()
        self._linear = paddle.nn.Linear(128, 10)

    def forward(self, x):
        return self._linear(x)

# Export model with 'InputSpec' to support dynamic input shape.
def export_linear_net():
    model = LinearNet()
    x_spec = paddle.static.InputSpec(shape=[None, 128], dtype='float32')
    paddle.onnx.export(model, 'linear_net', input_spec=[x_spec])

export_linear_net()

class Logic(paddle.nn.Layer):
    def __init__(self):
        super(Logic, self).__init__()

    def forward(self, x, y, z):
        if z:
            return x
        else:
            return y

# Export model with 'Tensor' to support pruned model by set 'output_spec'.
def export_logic():
    model = Logic()
    x = paddle.to_tensor(np.array([1]))
    y = paddle.to_tensor(np.array([2]))
    # Static and run model.
    paddle.jit.to_static(model)
    out = model(x, y, z=True)
    paddle.onnx.export(model, 'pruned', input_spec=[x], output_spec=[out])

export_logic()
import paddle
import numpy as np
inp = np.random.uniform(-0.1, 0.1, [10, 10]).astype("float32")
linear = paddle.nn.Linear(10, 10)
inp = paddle.to_tensor(inp)
out = linear(inp)
loss = paddle.mean(out)
beta1 = paddle.to_tensor([0.9], dtype="float32")
beta2 = paddle.to_tensor([0.99], dtype="float32")
adadelta = paddle.optimizer.Adadelta(learning_rate=0.1, parameters=linear.parameters(), weight_decay=0.01)
back = out.backward()
adadelta.step()
adadelta.clear_grad()

#Note that the learning_rate of linear_2 is 0.01.
linear_1 = paddle.nn.Linear(10, 10)
linear_2 = paddle.nn.Linear(10, 10)
inp = paddle.uniform(shape=[10, 10], min=-0.1, max=0.1)
out = linear_1(inp)
out = linear_2(out)
loss = paddle.mean(out)
adadelta = paddle.optimizer.Adadelta(
    learning_rate=0.1,
    parameters=[{
        'params': linear_1.parameters()
    }, {
        'params': linear_2.parameters(),
        'weight_decay': 0.001,
        'learning_rate': 0.1,
    }],
    weight_decay=0.01)
out.backward()
adadelta.step()
adadelta.clear_grad()
import numpy as np
import paddle

value = np.arange(26).reshape(2, 13).astype("float32")
a = paddle.to_tensor(value)
linear = paddle.nn.Linear(13, 5)
# This can be any optimizer supported by dygraph.
adam = paddle.optimizer.Adam(learning_rate = 0.01,
                            parameters = linear.parameters())
out = linear(a)
out.backward()
adam.step()
adam.clear_grad()
# train on default dynamic graph mode
import paddle
import numpy as np
emb = paddle.nn.Embedding(10, 3)

## example1: LRScheduler is not used, return the same value is all the same
adam = paddle.optimizer.Adam(0.01, parameters = emb.parameters())
for batch in range(10):
    input = paddle.randint(low=0, high=5, shape=[5])
    out = emb(input)
    out.backward()
    print("Learning rate of step{}: {}".format(batch, adam.get_lr())) # 0.01
    adam.step()

## example2: StepDecay is used, return the scheduled learning rate
scheduler = paddle.optimizer.lr.StepDecay(learning_rate=0.5, step_size=2, gamma=0.1)
adam = paddle.optimizer.Adam(scheduler, parameters = emb.parameters())
for batch in range(10):
    input = paddle.randint(low=0, high=5, shape=[5])
    out = emb(input)
    out.backward()
    print("Learning rate of step{}: {}".format(batch, adam.get_lr())) # 0.5->0.05...
    adam.step()
    scheduler.step()

# train on static graph mode
paddle.enable_static()
main_prog = paddle.static.Program()
start_prog = paddle.static.Program()
with paddle.static.program_guard(main_prog, start_prog):
    x = paddle.static.data(name='x', shape=[None, 10])
    z = paddle.static.nn.fc(x, 100)
    loss = paddle.mean(z)
    scheduler = paddle.optimizer.lr.StepDecay(learning_rate=0.5, step_size=2, gamma=0.1)
    adam = paddle.optimizer.Adam(learning_rate=scheduler)
    adam.minimize(loss)

exe = paddle.static.Executor()
exe.run(start_prog)
for batch in range(10):
    print("Learning rate of step{}: {}", adam.get_lr())     # 0.5->0.05->0.005...
    out = exe.run(main_prog, feed={'x': np.random.randn(3, 10).astype('float32')})
    scheduler.step()
import paddle
linear = paddle.nn.Linear(10, 10)
input = paddle.uniform(shape=[10, 10], min=-0.1, max=0.1)
out = linear(input)
loss = paddle.mean(out)

beta1 = paddle.to_tensor([0.9], dtype="float32")
beta2 = paddle.to_tensor([0.99], dtype="float32")

adam = paddle.optimizer.Adam(learning_rate=0.1,
        parameters=linear.parameters(),
        weight_decay=0.01)
out.backward()
adam.minimize(loss)
adam.clear_grad()
import paddle
linear = paddle.nn.Linear(10, 10)

adam = paddle.optimizer.Adam(0.1, parameters=linear.parameters())

# set learning rate manually by python float value
lr_list = [0.2, 0.3, 0.4, 0.5, 0.6]
for i in range(5):
    adam.set_lr(lr_list[i])
    lr = adam.get_lr()
    print("current lr is {}".format(lr))
# Print:
#    current lr is 0.2
#    current lr is 0.3
#    current lr is 0.4
#    current lr is 0.5
#    current lr is 0.6
import paddle

emb = paddle.nn.Embedding(10, 10)

layer_state_dict = emb.state_dict()
paddle.save(layer_state_dict, "emb.pdparams")

scheduler = paddle.optimizer.lr.NoamDecay(
    d_model=0.01, warmup_steps=100, verbose=True)
adam = paddle.optimizer.Adam(
    learning_rate=scheduler,
    parameters=emb.parameters())
opt_state_dict = adam.state_dict()
paddle.save(opt_state_dict, "adam.pdopt")

opti_state_dict = paddle.load("adam.pdopt")
adam.set_state_dict(opti_state_dict)
import paddle
emb = paddle.nn.Embedding(10, 10)

adam = paddle.optimizer.Adam(0.001, parameters=emb.parameters())
state_dict = adam.state_dict()
import paddle
import numpy as np

value = np.arange(26).reshape(2, 13).astype("float32")
a = paddle.to_tensor(value)
linear = paddle.nn.Linear(13, 5)
# This can be any optimizer supported by dygraph.
adam = paddle.optimizer.Adam(learning_rate = 0.01,
                            parameters = linear.parameters())
out = linear(a)
out.backward()
adam.step()
adam.clear_grad()
import paddle
import numpy as np

inp = paddle.rand(shape=[10, 10])
linear = paddle.nn.Linear(10, 10)
out = linear(inp)
loss = paddle.mean(out)
adagrad = paddle.optimizer.Adagrad(learning_rate=0.1,
        parameters=linear.parameters())
out.backward()
adagrad.step()
adagrad.clear_grad()

#Note that the learning_rate of linear_2 is 0.01.
linear_1 = paddle.nn.Linear(10, 10)
linear_2 = paddle.nn.Linear(10, 10)
inp = paddle.uniform(shape=[10, 10], min=-0.1, max=0.1)
out = linear_1(inp)
out = linear_2(out)
loss = paddle.mean(out)
adagrad = paddle.optimizer.Adagrad(
    learning_rate=0.1,
    parameters=[{
        'params': linear_1.parameters()
    }, {
        'params': linear_2.parameters(),
        'weight_decay': 0.001,
        'learning_rate': 0.1,
    }],
    weight_decay=0.01)
out.backward()
adagrad.step()
adagrad.clear_grad()
import numpy as np
import paddle

value = np.arange(26).reshape(2, 13).astype("float32")
a = paddle.to_tensor(value)
linear = paddle.nn.Linear(13, 5)
# This can be any optimizer supported by dygraph.
adam = paddle.optimizer.Adam(learning_rate = 0.01,
                            parameters = linear.parameters())
out = linear(a)
out.backward()
adam.step()
adam.clear_grad()
# train on default dynamic graph mode
import paddle
import numpy as np
emb = paddle.nn.Embedding(10, 3)

## example1: LRScheduler is not used, return the same value is all the same
adam = paddle.optimizer.Adam(0.01, parameters = emb.parameters())
for batch in range(10):
    input = paddle.randint(low=0, high=5, shape=[5])
    out = emb(input)
    out.backward()
    print("Learning rate of step{}: {}".format(batch, adam.get_lr())) # 0.01
    adam.step()

## example2: StepDecay is used, return the scheduled learning rate
scheduler = paddle.optimizer.lr.StepDecay(learning_rate=0.5, step_size=2, gamma=0.1)
adam = paddle.optimizer.Adam(scheduler, parameters = emb.parameters())
for batch in range(10):
    input = paddle.randint(low=0, high=5, shape=[5])
    out = emb(input)
    out.backward()
    print("Learning rate of step{}: {}".format(batch, adam.get_lr())) # 0.5->0.05...
    adam.step()
    scheduler.step()

# train on static graph mode
paddle.enable_static()
main_prog = paddle.static.Program()
start_prog = paddle.static.Program()
with paddle.static.program_guard(main_prog, start_prog):
    x = paddle.static.data(name='x', shape=[None, 10])
    z = paddle.static.nn.fc(x, 100)
    loss = paddle.mean(z)
    scheduler = paddle.optimizer.lr.StepDecay(learning_rate=0.5, step_size=2, gamma=0.1)
    adam = paddle.optimizer.Adam(learning_rate=scheduler)
    adam.minimize(loss)

exe = paddle.static.Executor()
exe.run(start_prog)
for batch in range(10):
    print("Learning rate of step{}: {}", adam.get_lr())     # 0.5->0.05->0.005...
    out = exe.run(main_prog, feed={'x': np.random.randn(3, 10).astype('float32')})
    scheduler.step()
import paddle
linear = paddle.nn.Linear(10, 10)
input = paddle.uniform(shape=[10, 10], min=-0.1, max=0.1)
out = linear(input)
loss = paddle.mean(out)

beta1 = paddle.to_tensor([0.9], dtype="float32")
beta2 = paddle.to_tensor([0.99], dtype="float32")

adam = paddle.optimizer.Adam(learning_rate=0.1,
        parameters=linear.parameters(),
        weight_decay=0.01)
out.backward()
adam.minimize(loss)
adam.clear_grad()
import paddle
linear = paddle.nn.Linear(10, 10)

adam = paddle.optimizer.Adam(0.1, parameters=linear.parameters())

# set learning rate manually by python float value
lr_list = [0.2, 0.3, 0.4, 0.5, 0.6]
for i in range(5):
    adam.set_lr(lr_list[i])
    lr = adam.get_lr()
    print("current lr is {}".format(lr))
# Print:
#    current lr is 0.2
#    current lr is 0.3
#    current lr is 0.4
#    current lr is 0.5
#    current lr is 0.6
import paddle

emb = paddle.nn.Embedding(10, 10)

layer_state_dict = emb.state_dict()
paddle.save(layer_state_dict, "emb.pdparams")

scheduler = paddle.optimizer.lr.NoamDecay(
    d_model=0.01, warmup_steps=100, verbose=True)
adam = paddle.optimizer.Adam(
    learning_rate=scheduler,
    parameters=emb.parameters())
opt_state_dict = adam.state_dict()
paddle.save(opt_state_dict, "adam.pdopt")

opti_state_dict = paddle.load("adam.pdopt")
adam.set_state_dict(opti_state_dict)
import paddle
emb = paddle.nn.Embedding(10, 10)

adam = paddle.optimizer.Adam(0.001, parameters=emb.parameters())
state_dict = adam.state_dict()
import paddle
import numpy as np

value = np.arange(26).reshape(2, 13).astype("float32")
a = paddle.to_tensor(value)
linear = paddle.nn.Linear(13, 5)
# This can be any optimizer supported by dygraph.
adam = paddle.optimizer.Adam(learning_rate = 0.01,
                            parameters = linear.parameters())
out = linear(a)
out.backward()
adam.step()
adam.clear_grad()
import paddle

linear = paddle.nn.Linear(10, 10)
inp = paddle.rand([10,10], dtype="float32")
out = linear(inp)
loss = paddle.mean(out)
adam = paddle.optimizer.Adam(learning_rate=0.1,
        parameters=linear.parameters())
out.backward()
adam.step()
adam.clear_grad()
# Adam with beta1/beta2 as Tensor and weight_decay as float
import paddle

linear = paddle.nn.Linear(10, 10)
inp = paddle.rand([10,10], dtype="float32")
out = linear(inp)
loss = paddle.mean(out)

beta1 = paddle.to_tensor([0.9], dtype="float32")
beta2 = paddle.to_tensor([0.99], dtype="float32")

adam = paddle.optimizer.Adam(learning_rate=0.1,
        parameters=linear.parameters(),
        beta1=beta1,
        beta2=beta2,
        weight_decay=0.01)
out.backward()
adam.step()
adam.clear_grad()

#Note that the learning_rate of linear_2 is 0.01.
linear_1 = paddle.nn.Linear(10, 10)
linear_2 = paddle.nn.Linear(10, 10)
inp = paddle.uniform(shape=[10, 10], min=-0.1, max=0.1)
out = linear_1(inp)
out = linear_2(out)
loss = paddle.mean(out)
adam = paddle.optimizer.Adam(
    learning_rate=0.1,
    parameters=[{
        'params': linear_1.parameters()
    }, {
        'params': linear_2.parameters(),
        'weight_decay': 0.001,
        'learning_rate': 0.1,
        'beta1': 0.8
    }],
    weight_decay=0.01,
    beta1=0.9)
out.backward()
adam.step()
adam.clear_grad()
import paddle

a = paddle.rand([2,13], dtype="float32")
linear = paddle.nn.Linear(13, 5)
# This can be any optimizer supported by dygraph.
adam = paddle.optimizer.Adam(learning_rate = 0.01,
                            parameters = linear.parameters())
out = linear(a)
out.backward()
adam.step()
adam.clear_grad()
import numpy as np
import paddle

value = np.arange(26).reshape(2, 13).astype("float32")
a = paddle.to_tensor(value)
linear = paddle.nn.Linear(13, 5)
# This can be any optimizer supported by dygraph.
adam = paddle.optimizer.Adam(learning_rate = 0.01,
                            parameters = linear.parameters())
out = linear(a)
out.backward()
adam.step()
adam.clear_grad()
# train on default dynamic graph mode
import paddle
import numpy as np
emb = paddle.nn.Embedding(10, 3)

## example1: LRScheduler is not used, return the same value is all the same
adam = paddle.optimizer.Adam(0.01, parameters = emb.parameters())
for batch in range(10):
    input = paddle.randint(low=0, high=5, shape=[5])
    out = emb(input)
    out.backward()
    print("Learning rate of step{}: {}".format(batch, adam.get_lr())) # 0.01
    adam.step()

## example2: StepDecay is used, return the scheduled learning rate
scheduler = paddle.optimizer.lr.StepDecay(learning_rate=0.5, step_size=2, gamma=0.1)
adam = paddle.optimizer.Adam(scheduler, parameters = emb.parameters())
for batch in range(10):
    input = paddle.randint(low=0, high=5, shape=[5])
    out = emb(input)
    out.backward()
    print("Learning rate of step{}: {}".format(batch, adam.get_lr())) # 0.5->0.05...
    adam.step()
    scheduler.step()

# train on static graph mode
paddle.enable_static()
main_prog = paddle.static.Program()
start_prog = paddle.static.Program()
with paddle.static.program_guard(main_prog, start_prog):
    x = paddle.static.data(name='x', shape=[None, 10])
    z = paddle.static.nn.fc(x, 100)
    loss = paddle.mean(z)
    scheduler = paddle.optimizer.lr.StepDecay(learning_rate=0.5, step_size=2, gamma=0.1)
    adam = paddle.optimizer.Adam(learning_rate=scheduler)
    adam.minimize(loss)

exe = paddle.static.Executor()
exe.run(start_prog)
for batch in range(10):
    print("Learning rate of step{}: {}", adam.get_lr())     # 0.5->0.05->0.005...
    out = exe.run(main_prog, feed={'x': np.random.randn(3, 10).astype('float32')})
    scheduler.step()
import paddle
linear = paddle.nn.Linear(10, 10)
input = paddle.uniform(shape=[10, 10], min=-0.1, max=0.1)
out = linear(input)
loss = paddle.mean(out)

beta1 = paddle.to_tensor([0.9], dtype="float32")
beta2 = paddle.to_tensor([0.99], dtype="float32")

adam = paddle.optimizer.Adam(learning_rate=0.1,
        parameters=linear.parameters(),
        weight_decay=0.01)
out.backward()
adam.minimize(loss)
adam.clear_grad()
import paddle
linear = paddle.nn.Linear(10, 10)

adam = paddle.optimizer.Adam(0.1, parameters=linear.parameters())

# set learning rate manually by python float value
lr_list = [0.2, 0.3, 0.4, 0.5, 0.6]
for i in range(5):
    adam.set_lr(lr_list[i])
    lr = adam.get_lr()
    print("current lr is {}".format(lr))
# Print:
#    current lr is 0.2
#    current lr is 0.3
#    current lr is 0.4
#    current lr is 0.5
#    current lr is 0.6
import paddle

emb = paddle.nn.Embedding(10, 10)

layer_state_dict = emb.state_dict()
paddle.save(layer_state_dict, "emb.pdparams")

scheduler = paddle.optimizer.lr.NoamDecay(
    d_model=0.01, warmup_steps=100, verbose=True)
adam = paddle.optimizer.Adam(
    learning_rate=scheduler,
    parameters=emb.parameters())
opt_state_dict = adam.state_dict()
paddle.save(opt_state_dict, "adam.pdopt")

opti_state_dict = paddle.load("adam.pdopt")
adam.set_state_dict(opti_state_dict)
import paddle
emb = paddle.nn.Embedding(10, 10)

adam = paddle.optimizer.Adam(0.001, parameters=emb.parameters())
state_dict = adam.state_dict()
import paddle
import numpy as np

inp = np.random.uniform(-0.1, 0.1, [10, 10]).astype("float32")
linear = paddle.nn.Linear(10, 10)
inp = paddle.to_tensor(inp)
out = linear(inp)
loss = paddle.mean(out)

beta1 = paddle.to_tensor([0.9], dtype="float32")
beta2 = paddle.to_tensor([0.99], dtype="float32")

adam = paddle.optimizer.Adamax(learning_rate=0.1,
        parameters=linear.parameters(),
        beta1=beta1,
        beta2=beta2,
        weight_decay=0.01)
out.backward()
adam.step()
adam.clear_grad()


#Note that the learning_rate of linear_2 is 0.01.
linear_1 = paddle.nn.Linear(10, 10)
linear_2 = paddle.nn.Linear(10, 10)
inp = paddle.uniform(shape=[10, 10], min=-0.1, max=0.1)
out = linear_1(inp)
out = linear_2(out)
loss = paddle.mean(out)
adam = paddle.optimizer.Adamax(
    learning_rate=0.1,
    parameters=[{
        'params': linear_1.parameters()
    }, {
        'params': linear_2.parameters(),
        'weight_decay': 0.001,
        'learning_rate': 0.1,
        'beta1': 0.8
    }],
    weight_decay=0.01,
    beta1=0.9)
out.backward()
adam.step()
adam.clear_grad()
import numpy as np
import paddle

value = np.arange(26).reshape(2, 13).astype("float32")
a = paddle.to_tensor(value)
linear = paddle.nn.Linear(13, 5)
# This can be any optimizer supported by dygraph.
adam = paddle.optimizer.Adam(learning_rate = 0.01,
                            parameters = linear.parameters())
out = linear(a)
out.backward()
adam.step()
adam.clear_grad()
# train on default dynamic graph mode
import paddle
import numpy as np
emb = paddle.nn.Embedding(10, 3)

## example1: LRScheduler is not used, return the same value is all the same
adam = paddle.optimizer.Adam(0.01, parameters = emb.parameters())
for batch in range(10):
    input = paddle.randint(low=0, high=5, shape=[5])
    out = emb(input)
    out.backward()
    print("Learning rate of step{}: {}".format(batch, adam.get_lr())) # 0.01
    adam.step()

## example2: StepDecay is used, return the scheduled learning rate
scheduler = paddle.optimizer.lr.StepDecay(learning_rate=0.5, step_size=2, gamma=0.1)
adam = paddle.optimizer.Adam(scheduler, parameters = emb.parameters())
for batch in range(10):
    input = paddle.randint(low=0, high=5, shape=[5])
    out = emb(input)
    out.backward()
    print("Learning rate of step{}: {}".format(batch, adam.get_lr())) # 0.5->0.05...
    adam.step()
    scheduler.step()

# train on static graph mode
paddle.enable_static()
main_prog = paddle.static.Program()
start_prog = paddle.static.Program()
with paddle.static.program_guard(main_prog, start_prog):
    x = paddle.static.data(name='x', shape=[None, 10])
    z = paddle.static.nn.fc(x, 100)
    loss = paddle.mean(z)
    scheduler = paddle.optimizer.lr.StepDecay(learning_rate=0.5, step_size=2, gamma=0.1)
    adam = paddle.optimizer.Adam(learning_rate=scheduler)
    adam.minimize(loss)

exe = paddle.static.Executor()
exe.run(start_prog)
for batch in range(10):
    print("Learning rate of step{}: {}", adam.get_lr())     # 0.5->0.05->0.005...
    out = exe.run(main_prog, feed={'x': np.random.randn(3, 10).astype('float32')})
    scheduler.step()
import paddle
linear = paddle.nn.Linear(10, 10)
input = paddle.uniform(shape=[10, 10], min=-0.1, max=0.1)
out = linear(input)
loss = paddle.mean(out)

beta1 = paddle.to_tensor([0.9], dtype="float32")
beta2 = paddle.to_tensor([0.99], dtype="float32")

adam = paddle.optimizer.Adam(learning_rate=0.1,
        parameters=linear.parameters(),
        weight_decay=0.01)
out.backward()
adam.minimize(loss)
adam.clear_grad()
import paddle
linear = paddle.nn.Linear(10, 10)

adam = paddle.optimizer.Adam(0.1, parameters=linear.parameters())

# set learning rate manually by python float value
lr_list = [0.2, 0.3, 0.4, 0.5, 0.6]
for i in range(5):
    adam.set_lr(lr_list[i])
    lr = adam.get_lr()
    print("current lr is {}".format(lr))
# Print:
#    current lr is 0.2
#    current lr is 0.3
#    current lr is 0.4
#    current lr is 0.5
#    current lr is 0.6
import paddle

emb = paddle.nn.Embedding(10, 10)

layer_state_dict = emb.state_dict()
paddle.save(layer_state_dict, "emb.pdparams")

scheduler = paddle.optimizer.lr.NoamDecay(
    d_model=0.01, warmup_steps=100, verbose=True)
adam = paddle.optimizer.Adam(
    learning_rate=scheduler,
    parameters=emb.parameters())
opt_state_dict = adam.state_dict()
paddle.save(opt_state_dict, "adam.pdopt")

opti_state_dict = paddle.load("adam.pdopt")
adam.set_state_dict(opti_state_dict)
import paddle
emb = paddle.nn.Embedding(10, 10)

adam = paddle.optimizer.Adam(0.001, parameters=emb.parameters())
state_dict = adam.state_dict()
import paddle
import numpy as np

value = np.arange(26).reshape(2, 13).astype("float32")
a = paddle.to_tensor(value)
linear = paddle.nn.Linear(13, 5)
# This can be any optimizer supported by dygraph.
adam = paddle.optimizer.Adam(learning_rate = 0.01,
                            parameters = linear.parameters())
out = linear(a)
out.backward()
adam.step()
adam.clear_grad()
import paddle

linear = paddle.nn.Linear(10, 10)
inp = paddle.rand([10,10], dtype="float32")
out = linear(inp)
loss = paddle.mean(out)

beta1 = paddle.to_tensor([0.9], dtype="float32")
beta2 = paddle.to_tensor([0.99], dtype="float32")

adam = paddle.optimizer.AdamW(learning_rate=0.1,
        parameters=linear.parameters(),
        beta1=beta1,
        beta2=beta2,
        weight_decay=0.01)
out.backward()
adam.step()
adam.clear_grad()


#Note that the learning_rate of linear_2 is 0.01.
linear_1 = paddle.nn.Linear(10, 10)
linear_2 = paddle.nn.Linear(10, 10)
inp = paddle.uniform(shape=[10, 10], min=-0.1, max=0.1)
out = linear_1(inp)
out = linear_2(out)
loss = paddle.mean(out)
adam = paddle.optimizer.AdamW(
    learning_rate=0.1,
    parameters=[{
        'params': linear_1.parameters()
    }, {
        'params': linear_2.parameters(),
        'weight_decay': 0.001,
        'learning_rate': 0.1,
        'beta1': 0.8
    }],
    weight_decay=0.01,
    beta1=0.9)
out.backward()
adam.step()
adam.clear_grad()
import numpy as np
import paddle

value = np.arange(26).reshape(2, 13).astype("float32")
a = paddle.to_tensor(value)
linear = paddle.nn.Linear(13, 5)
# This can be any optimizer supported by dygraph.
adam = paddle.optimizer.Adam(learning_rate = 0.01,
                            parameters = linear.parameters())
out = linear(a)
out.backward()
adam.step()
adam.clear_grad()
# train on default dynamic graph mode
import paddle
import numpy as np
emb = paddle.nn.Embedding(10, 3)

## example1: LRScheduler is not used, return the same value is all the same
adam = paddle.optimizer.Adam(0.01, parameters = emb.parameters())
for batch in range(10):
    input = paddle.randint(low=0, high=5, shape=[5])
    out = emb(input)
    out.backward()
    print("Learning rate of step{}: {}".format(batch, adam.get_lr())) # 0.01
    adam.step()

## example2: StepDecay is used, return the scheduled learning rate
scheduler = paddle.optimizer.lr.StepDecay(learning_rate=0.5, step_size=2, gamma=0.1)
adam = paddle.optimizer.Adam(scheduler, parameters = emb.parameters())
for batch in range(10):
    input = paddle.randint(low=0, high=5, shape=[5])
    out = emb(input)
    out.backward()
    print("Learning rate of step{}: {}".format(batch, adam.get_lr())) # 0.5->0.05...
    adam.step()
    scheduler.step()

# train on static graph mode
paddle.enable_static()
main_prog = paddle.static.Program()
start_prog = paddle.static.Program()
with paddle.static.program_guard(main_prog, start_prog):
    x = paddle.static.data(name='x', shape=[None, 10])
    z = paddle.static.nn.fc(x, 100)
    loss = paddle.mean(z)
    scheduler = paddle.optimizer.lr.StepDecay(learning_rate=0.5, step_size=2, gamma=0.1)
    adam = paddle.optimizer.Adam(learning_rate=scheduler)
    adam.minimize(loss)

exe = paddle.static.Executor()
exe.run(start_prog)
for batch in range(10):
    print("Learning rate of step{}: {}", adam.get_lr())     # 0.5->0.05->0.005...
    out = exe.run(main_prog, feed={'x': np.random.randn(3, 10).astype('float32')})
    scheduler.step()
import paddle
linear = paddle.nn.Linear(10, 10)
input = paddle.uniform(shape=[10, 10], min=-0.1, max=0.1)
out = linear(input)
loss = paddle.mean(out)

beta1 = paddle.to_tensor([0.9], dtype="float32")
beta2 = paddle.to_tensor([0.99], dtype="float32")

adam = paddle.optimizer.Adam(learning_rate=0.1,
        parameters=linear.parameters(),
        weight_decay=0.01)
out.backward()
adam.minimize(loss)
adam.clear_grad()
import paddle
linear = paddle.nn.Linear(10, 10)

adam = paddle.optimizer.Adam(0.1, parameters=linear.parameters())

# set learning rate manually by python float value
lr_list = [0.2, 0.3, 0.4, 0.5, 0.6]
for i in range(5):
    adam.set_lr(lr_list[i])
    lr = adam.get_lr()
    print("current lr is {}".format(lr))
# Print:
#    current lr is 0.2
#    current lr is 0.3
#    current lr is 0.4
#    current lr is 0.5
#    current lr is 0.6
import paddle

emb = paddle.nn.Embedding(10, 10)

layer_state_dict = emb.state_dict()
paddle.save(layer_state_dict, "emb.pdparams")

scheduler = paddle.optimizer.lr.NoamDecay(
    d_model=0.01, warmup_steps=100, verbose=True)
adam = paddle.optimizer.Adam(
    learning_rate=scheduler,
    parameters=emb.parameters())
opt_state_dict = adam.state_dict()
paddle.save(opt_state_dict, "adam.pdopt")

opti_state_dict = paddle.load("adam.pdopt")
adam.set_state_dict(opti_state_dict)
import paddle
emb = paddle.nn.Embedding(10, 10)

adam = paddle.optimizer.Adam(0.001, parameters=emb.parameters())
state_dict = adam.state_dict()
import paddle

a = paddle.rand([2,13], dtype="float32")
linear = paddle.nn.Linear(13, 5)
# This can be any optimizer supported by dygraph.
adam = paddle.optimizer.Adam(learning_rate = 0.01,
                            parameters = linear.parameters())
out = linear(a)
out.backward()
adam.step()
adam.clear_grad()
import paddle

inp = paddle.uniform(shape=[10, 10], dtype='float32', min=-0.1, max=0.1)
linear = paddle.nn.Linear(10, 10)
out = linear(inp)
loss = paddle.mean(out)
beta1 = paddle.to_tensor([0.9], dtype="float32")
beta2 = paddle.to_tensor([0.85], dtype="float32")
lamb = paddle.optimizer.Lamb(learning_rate=0.002, parameters=linear.parameters(), lamb_weight_decay=0.01)
back = out.backward()
lamb.step()
lamb.clear_grad()
import numpy as np
import paddle

value = np.arange(26).reshape(2, 13).astype("float32")
a = paddle.to_tensor(value)
linear = paddle.nn.Linear(13, 5)
# This can be any optimizer supported by dygraph.
adam = paddle.optimizer.Adam(learning_rate = 0.01,
                            parameters = linear.parameters())
out = linear(a)
out.backward()
adam.step()
adam.clear_grad()
# train on default dynamic graph mode
import paddle
import numpy as np
emb = paddle.nn.Embedding(10, 3)

## example1: LRScheduler is not used, return the same value is all the same
adam = paddle.optimizer.Adam(0.01, parameters = emb.parameters())
for batch in range(10):
    input = paddle.randint(low=0, high=5, shape=[5])
    out = emb(input)
    out.backward()
    print("Learning rate of step{}: {}".format(batch, adam.get_lr())) # 0.01
    adam.step()

## example2: StepDecay is used, return the scheduled learning rate
scheduler = paddle.optimizer.lr.StepDecay(learning_rate=0.5, step_size=2, gamma=0.1)
adam = paddle.optimizer.Adam(scheduler, parameters = emb.parameters())
for batch in range(10):
    input = paddle.randint(low=0, high=5, shape=[5])
    out = emb(input)
    out.backward()
    print("Learning rate of step{}: {}".format(batch, adam.get_lr())) # 0.5->0.05...
    adam.step()
    scheduler.step()

# train on static graph mode
paddle.enable_static()
main_prog = paddle.static.Program()
start_prog = paddle.static.Program()
with paddle.static.program_guard(main_prog, start_prog):
    x = paddle.static.data(name='x', shape=[None, 10])
    z = paddle.static.nn.fc(x, 100)
    loss = paddle.mean(z)
    scheduler = paddle.optimizer.lr.StepDecay(learning_rate=0.5, step_size=2, gamma=0.1)
    adam = paddle.optimizer.Adam(learning_rate=scheduler)
    adam.minimize(loss)

exe = paddle.static.Executor()
exe.run(start_prog)
for batch in range(10):
    print("Learning rate of step{}: {}", adam.get_lr())     # 0.5->0.05->0.005...
    out = exe.run(main_prog, feed={'x': np.random.randn(3, 10).astype('float32')})
    scheduler.step()
import paddle
linear = paddle.nn.Linear(10, 10)
input = paddle.uniform(shape=[10, 10], min=-0.1, max=0.1)
out = linear(input)
loss = paddle.mean(out)

beta1 = paddle.to_tensor([0.9], dtype="float32")
beta2 = paddle.to_tensor([0.99], dtype="float32")

adam = paddle.optimizer.Adam(learning_rate=0.1,
        parameters=linear.parameters(),
        weight_decay=0.01)
out.backward()
adam.minimize(loss)
adam.clear_grad()
import paddle
linear = paddle.nn.Linear(10, 10)

adam = paddle.optimizer.Adam(0.1, parameters=linear.parameters())

# set learning rate manually by python float value
lr_list = [0.2, 0.3, 0.4, 0.5, 0.6]
for i in range(5):
    adam.set_lr(lr_list[i])
    lr = adam.get_lr()
    print("current lr is {}".format(lr))
# Print:
#    current lr is 0.2
#    current lr is 0.3
#    current lr is 0.4
#    current lr is 0.5
#    current lr is 0.6
import paddle

emb = paddle.nn.Embedding(10, 10)

layer_state_dict = emb.state_dict()
paddle.save(layer_state_dict, "emb.pdparams")

scheduler = paddle.optimizer.lr.NoamDecay(
    d_model=0.01, warmup_steps=100, verbose=True)
adam = paddle.optimizer.Adam(
    learning_rate=scheduler,
    parameters=emb.parameters())
opt_state_dict = adam.state_dict()
paddle.save(opt_state_dict, "adam.pdopt")

opti_state_dict = paddle.load("adam.pdopt")
adam.set_state_dict(opti_state_dict)
import paddle
emb = paddle.nn.Embedding(10, 10)

adam = paddle.optimizer.Adam(0.001, parameters=emb.parameters())
state_dict = adam.state_dict()
import paddle
import numpy as np

value = np.arange(26).reshape(2, 13).astype("float32")
a = paddle.to_tensor(value)
linear = paddle.nn.Linear(13, 5)
# This can be any optimizer supported by dygraph.
adam = paddle.optimizer.Adam(learning_rate = 0.01,
                            parameters = linear.parameters())
out = linear(a)
out.backward()
adam.step()
adam.clear_grad()
import paddle
import numpy as np
inp = np.random.uniform(-0.1, 0.1, [10, 10]).astype("float32")
linear = paddle.nn.Linear(10, 10)
inp = paddle.to_tensor(inp)
out = linear(inp)
loss = paddle.mean(out)
beta1 = paddle.to_tensor([0.9], dtype="float32")
beta2 = paddle.to_tensor([0.99], dtype="float32")
momentum = paddle.optimizer.Momentum(learning_rate=0.1, parameters=linear.parameters(), weight_decay=0.01)
back = out.backward()
momentum.step()
momentum.clear_grad()

#Note that the learning_rate of linear_2 is 0.01.
linear_1 = paddle.nn.Linear(10, 10)
linear_2 = paddle.nn.Linear(10, 10)
inp = paddle.uniform(shape=[10, 10], min=-0.1, max=0.1)
out = linear_1(inp)
out = linear_2(out)
loss = paddle.mean(out)
momentum = paddle.optimizer.Momentum(
    learning_rate=0.1,
    parameters=[{
        'params': linear_1.parameters()
    }, {
        'params': linear_2.parameters(),
        'weight_decay': 0.001,
        'learning_rate': 0.1
    }],
    weight_decay=0.01,
    momentum=0.9)
out.backward()
momentum.step()
momentum.clear_grad()
import numpy as np
import paddle

value = np.arange(26).reshape(2, 13).astype("float32")
a = paddle.to_tensor(value)
linear = paddle.nn.Linear(13, 5)
# This can be any optimizer supported by dygraph.
adam = paddle.optimizer.Adam(learning_rate = 0.01,
                            parameters = linear.parameters())
out = linear(a)
out.backward()
adam.step()
adam.clear_grad()
# train on default dynamic graph mode
import paddle
import numpy as np
emb = paddle.nn.Embedding(10, 3)

## example1: LRScheduler is not used, return the same value is all the same
adam = paddle.optimizer.Adam(0.01, parameters = emb.parameters())
for batch in range(10):
    input = paddle.randint(low=0, high=5, shape=[5])
    out = emb(input)
    out.backward()
    print("Learning rate of step{}: {}".format(batch, adam.get_lr())) # 0.01
    adam.step()

## example2: StepDecay is used, return the scheduled learning rate
scheduler = paddle.optimizer.lr.StepDecay(learning_rate=0.5, step_size=2, gamma=0.1)
adam = paddle.optimizer.Adam(scheduler, parameters = emb.parameters())
for batch in range(10):
    input = paddle.randint(low=0, high=5, shape=[5])
    out = emb(input)
    out.backward()
    print("Learning rate of step{}: {}".format(batch, adam.get_lr())) # 0.5->0.05...
    adam.step()
    scheduler.step()

# train on static graph mode
paddle.enable_static()
main_prog = paddle.static.Program()
start_prog = paddle.static.Program()
with paddle.static.program_guard(main_prog, start_prog):
    x = paddle.static.data(name='x', shape=[None, 10])
    z = paddle.static.nn.fc(x, 100)
    loss = paddle.mean(z)
    scheduler = paddle.optimizer.lr.StepDecay(learning_rate=0.5, step_size=2, gamma=0.1)
    adam = paddle.optimizer.Adam(learning_rate=scheduler)
    adam.minimize(loss)

exe = paddle.static.Executor()
exe.run(start_prog)
for batch in range(10):
    print("Learning rate of step{}: {}", adam.get_lr())     # 0.5->0.05->0.005...
    out = exe.run(main_prog, feed={'x': np.random.randn(3, 10).astype('float32')})
    scheduler.step()
import paddle
linear = paddle.nn.Linear(10, 10)
input = paddle.uniform(shape=[10, 10], min=-0.1, max=0.1)
out = linear(input)
loss = paddle.mean(out)

beta1 = paddle.to_tensor([0.9], dtype="float32")
beta2 = paddle.to_tensor([0.99], dtype="float32")

adam = paddle.optimizer.Adam(learning_rate=0.1,
        parameters=linear.parameters(),
        weight_decay=0.01)
out.backward()
adam.minimize(loss)
adam.clear_grad()
import paddle
linear = paddle.nn.Linear(10, 10)

adam = paddle.optimizer.Adam(0.1, parameters=linear.parameters())

# set learning rate manually by python float value
lr_list = [0.2, 0.3, 0.4, 0.5, 0.6]
for i in range(5):
    adam.set_lr(lr_list[i])
    lr = adam.get_lr()
    print("current lr is {}".format(lr))
# Print:
#    current lr is 0.2
#    current lr is 0.3
#    current lr is 0.4
#    current lr is 0.5
#    current lr is 0.6
import paddle

emb = paddle.nn.Embedding(10, 10)

layer_state_dict = emb.state_dict()
paddle.save(layer_state_dict, "emb.pdparams")

scheduler = paddle.optimizer.lr.NoamDecay(
    d_model=0.01, warmup_steps=100, verbose=True)
adam = paddle.optimizer.Adam(
    learning_rate=scheduler,
    parameters=emb.parameters())
opt_state_dict = adam.state_dict()
paddle.save(opt_state_dict, "adam.pdopt")

opti_state_dict = paddle.load("adam.pdopt")
adam.set_state_dict(opti_state_dict)
import paddle
emb = paddle.nn.Embedding(10, 10)

adam = paddle.optimizer.Adam(0.001, parameters=emb.parameters())
state_dict = adam.state_dict()
import paddle
import numpy as np

value = np.arange(26).reshape(2, 13).astype("float32")
a = paddle.to_tensor(value)
linear = paddle.nn.Linear(13, 5)
# This can be any optimizer supported by dygraph.
adam = paddle.optimizer.Adam(learning_rate = 0.01,
                            parameters = linear.parameters())
out = linear(a)
out.backward()
adam.step()
adam.clear_grad()
#Take the subclass adam as an example
import paddle
linear = paddle.nn.Linear(10, 10)
inp = paddle.uniform(shape=[10, 10], min=-0.1, max=0.1)
out = linear(inp)
loss = paddle.mean(out)
adam = paddle.optimizer.Adam(learning_rate=0.1,
        parameters=linear.parameters())
out.backward()
adam.step()
adam.clear_grad()

#Take the subclass sgd as an example
#optimize parameters in linear_1 and linear2 in different options.
#Note that the learning_rate of linear_2 is 0.01.
linear_1 = paddle.nn.Linear(10, 10)
linear_2 = paddle.nn.Linear(10, 10)
inp = paddle.uniform(shape=[10, 10], min=-0.1, max=0.1)
out = linear_1(inp)
out = linear_2(out)
loss = paddle.mean(out)
sgd = paddle.optimizer.SGD(
    learning_rate=0.1,
    parameters=[{
        'params': linear_1.parameters()
    }, {
        'params': linear_2.parameters(),
        'weight_decay': 0.001,
        'learning_rate': 0.1
    }],
    weight_decay=0.01)
out.backward()
sgd.step()
sgd.clear_grad()
import paddle
emb = paddle.nn.Embedding(10, 10)

adam = paddle.optimizer.Adam(0.001, parameters=emb.parameters())
state_dict = adam.state_dict()
import paddle

emb = paddle.nn.Embedding(10, 10)

layer_state_dict = emb.state_dict()
paddle.save(layer_state_dict, "emb.pdparams")

scheduler = paddle.optimizer.lr.NoamDecay(
    d_model=0.01, warmup_steps=100, verbose=True)
adam = paddle.optimizer.Adam(
    learning_rate=scheduler,
    parameters=emb.parameters())
opt_state_dict = adam.state_dict()
paddle.save(opt_state_dict, "adam.pdopt")

opti_state_dict = paddle.load("adam.pdopt")
adam.set_state_dict(opti_state_dict)
import paddle
linear = paddle.nn.Linear(10, 10)

adam = paddle.optimizer.Adam(0.1, parameters=linear.parameters())

# set learning rate manually by python float value
lr_list = [0.2, 0.3, 0.4, 0.5, 0.6]
for i in range(5):
    adam.set_lr(lr_list[i])
    lr = adam.get_lr()
    print("current lr is {}".format(lr))
# Print:
#    current lr is 0.2
#    current lr is 0.3
#    current lr is 0.4
#    current lr is 0.5
#    current lr is 0.6
# train on default dynamic graph mode
import paddle
import numpy as np
emb = paddle.nn.Embedding(10, 3)

## example1: LRScheduler is not used, return the same value is all the same
adam = paddle.optimizer.Adam(0.01, parameters = emb.parameters())
for batch in range(10):
    input = paddle.randint(low=0, high=5, shape=[5])
    out = emb(input)
    out.backward()
    print("Learning rate of step{}: {}".format(batch, adam.get_lr())) # 0.01
    adam.step()

## example2: StepDecay is used, return the scheduled learning rate
scheduler = paddle.optimizer.lr.StepDecay(learning_rate=0.5, step_size=2, gamma=0.1)
adam = paddle.optimizer.Adam(scheduler, parameters = emb.parameters())
for batch in range(10):
    input = paddle.randint(low=0, high=5, shape=[5])
    out = emb(input)
    out.backward()
    print("Learning rate of step{}: {}".format(batch, adam.get_lr())) # 0.5->0.05...
    adam.step()
    scheduler.step()

# train on static graph mode
paddle.enable_static()
main_prog = paddle.static.Program()
start_prog = paddle.static.Program()
with paddle.static.program_guard(main_prog, start_prog):
    x = paddle.static.data(name='x', shape=[None, 10])
    z = paddle.static.nn.fc(x, 100)
    loss = paddle.mean(z)
    scheduler = paddle.optimizer.lr.StepDecay(learning_rate=0.5, step_size=2, gamma=0.1)
    adam = paddle.optimizer.Adam(learning_rate=scheduler)
    adam.minimize(loss)

exe = paddle.static.Executor()
exe.run(start_prog)
for batch in range(10):
    print("Learning rate of step{}: {}", adam.get_lr())     # 0.5->0.05->0.005...
    out = exe.run(main_prog, feed={'x': np.random.randn(3, 10).astype('float32')})
    scheduler.step()
import numpy as np
import paddle

value = np.arange(26).reshape(2, 13).astype("float32")
a = paddle.to_tensor(value)
linear = paddle.nn.Linear(13, 5)
# This can be any optimizer supported by dygraph.
adam = paddle.optimizer.Adam(learning_rate = 0.01,
                            parameters = linear.parameters())
out = linear(a)
out.backward()
adam.step()
adam.clear_grad()
import paddle
linear = paddle.nn.Linear(10, 10)
input = paddle.uniform(shape=[10, 10], min=-0.1, max=0.1)
out = linear(input)
loss = paddle.mean(out)

beta1 = paddle.to_tensor([0.9], dtype="float32")
beta2 = paddle.to_tensor([0.99], dtype="float32")

adam = paddle.optimizer.Adam(learning_rate=0.1,
        parameters=linear.parameters(),
        weight_decay=0.01)
out.backward()
adam.minimize(loss)
adam.clear_grad()
import paddle
import numpy as np

value = np.arange(26).reshape(2, 13).astype("float32")
a = paddle.to_tensor(value)
linear = paddle.nn.Linear(13, 5)
# This can be any optimizer supported by dygraph.
adam = paddle.optimizer.Adam(learning_rate = 0.01,
                            parameters = linear.parameters())
out = linear(a)
out.backward()
adam.step()
adam.clear_grad()
import paddle

inp = paddle.rand([10,10], dtype="float32")
linear = paddle.nn.Linear(10, 10)
out = linear(inp)
loss = paddle.mean(out)

rmsprop = paddle.optimizer.RMSProp(learning_rate=0.1,
                 parameters=linear.parameters(),
                           weight_decay=0.01)
out.backward()
rmsprop.step()
rmsprop.clear_grad()

#Note that the learning_rate of linear_2 is 0.01.
linear_1 = paddle.nn.Linear(10, 10)
linear_2 = paddle.nn.Linear(10, 10)
inp = paddle.uniform(shape=[10, 10], min=-0.1, max=0.1)
out = linear_1(inp)
out = linear_2(out)
loss = paddle.mean(out)
rmsprop = paddle.optimizer.RMSProp(
    learning_rate=0.1,
    parameters=[{
        'params': linear_1.parameters()
    }, {
        'params': linear_2.parameters(),
        'weight_decay': 0.001,
        'learning_rate': 0.1
    }],
    weight_decay=0.01)
out.backward()
rmsprop.step()
rmsprop.clear_grad()
import numpy as np
import paddle

value = np.arange(26).reshape(2, 13).astype("float32")
a = paddle.to_tensor(value)
linear = paddle.nn.Linear(13, 5)
# This can be any optimizer supported by dygraph.
adam = paddle.optimizer.Adam(learning_rate = 0.01,
                            parameters = linear.parameters())
out = linear(a)
out.backward()
adam.step()
adam.clear_grad()
# train on default dynamic graph mode
import paddle
import numpy as np
emb = paddle.nn.Embedding(10, 3)

## example1: LRScheduler is not used, return the same value is all the same
adam = paddle.optimizer.Adam(0.01, parameters = emb.parameters())
for batch in range(10):
    input = paddle.randint(low=0, high=5, shape=[5])
    out = emb(input)
    out.backward()
    print("Learning rate of step{}: {}".format(batch, adam.get_lr())) # 0.01
    adam.step()

## example2: StepDecay is used, return the scheduled learning rate
scheduler = paddle.optimizer.lr.StepDecay(learning_rate=0.5, step_size=2, gamma=0.1)
adam = paddle.optimizer.Adam(scheduler, parameters = emb.parameters())
for batch in range(10):
    input = paddle.randint(low=0, high=5, shape=[5])
    out = emb(input)
    out.backward()
    print("Learning rate of step{}: {}".format(batch, adam.get_lr())) # 0.5->0.05...
    adam.step()
    scheduler.step()

# train on static graph mode
paddle.enable_static()
main_prog = paddle.static.Program()
start_prog = paddle.static.Program()
with paddle.static.program_guard(main_prog, start_prog):
    x = paddle.static.data(name='x', shape=[None, 10])
    z = paddle.static.nn.fc(x, 100)
    loss = paddle.mean(z)
    scheduler = paddle.optimizer.lr.StepDecay(learning_rate=0.5, step_size=2, gamma=0.1)
    adam = paddle.optimizer.Adam(learning_rate=scheduler)
    adam.minimize(loss)

exe = paddle.static.Executor()
exe.run(start_prog)
for batch in range(10):
    print("Learning rate of step{}: {}", adam.get_lr())     # 0.5->0.05->0.005...
    out = exe.run(main_prog, feed={'x': np.random.randn(3, 10).astype('float32')})
    scheduler.step()
import paddle
linear = paddle.nn.Linear(10, 10)
input = paddle.uniform(shape=[10, 10], min=-0.1, max=0.1)
out = linear(input)
loss = paddle.mean(out)

beta1 = paddle.to_tensor([0.9], dtype="float32")
beta2 = paddle.to_tensor([0.99], dtype="float32")

adam = paddle.optimizer.Adam(learning_rate=0.1,
        parameters=linear.parameters(),
        weight_decay=0.01)
out.backward()
adam.minimize(loss)
adam.clear_grad()
import paddle
linear = paddle.nn.Linear(10, 10)

adam = paddle.optimizer.Adam(0.1, parameters=linear.parameters())

# set learning rate manually by python float value
lr_list = [0.2, 0.3, 0.4, 0.5, 0.6]
for i in range(5):
    adam.set_lr(lr_list[i])
    lr = adam.get_lr()
    print("current lr is {}".format(lr))
# Print:
#    current lr is 0.2
#    current lr is 0.3
#    current lr is 0.4
#    current lr is 0.5
#    current lr is 0.6
import paddle

emb = paddle.nn.Embedding(10, 10)

layer_state_dict = emb.state_dict()
paddle.save(layer_state_dict, "emb.pdparams")

scheduler = paddle.optimizer.lr.NoamDecay(
    d_model=0.01, warmup_steps=100, verbose=True)
adam = paddle.optimizer.Adam(
    learning_rate=scheduler,
    parameters=emb.parameters())
opt_state_dict = adam.state_dict()
paddle.save(opt_state_dict, "adam.pdopt")

opti_state_dict = paddle.load("adam.pdopt")
adam.set_state_dict(opti_state_dict)
import paddle
emb = paddle.nn.Embedding(10, 10)

adam = paddle.optimizer.Adam(0.001, parameters=emb.parameters())
state_dict = adam.state_dict()
import paddle
import numpy as np

value = np.arange(26).reshape(2, 13).astype("float32")
a = paddle.to_tensor(value)
linear = paddle.nn.Linear(13, 5)
# This can be any optimizer supported by dygraph.
adam = paddle.optimizer.Adam(learning_rate = 0.01,
                            parameters = linear.parameters())
out = linear(a)
out.backward()
adam.step()
adam.clear_grad()
import paddle
import numpy as np
inp = np.random.uniform(-0.1, 0.1, [10, 10]).astype("float32")
linear = paddle.nn.Linear(10, 10)
inp = paddle.to_tensor(inp)
out = linear(inp)
loss = paddle.mean(out)
beta1 = paddle.to_tensor([0.9], dtype="float32")
beta2 = paddle.to_tensor([0.99], dtype="float32")
sgd = paddle.optimizer.SGD(learning_rate=0.1, parameters=linear.parameters(), weight_decay=0.01)
back = out.backward()
sgd.step()
sgd.clear_grad()
import numpy as np
import paddle

value = np.arange(26).reshape(2, 13).astype("float32")
a = paddle.to_tensor(value)
linear = paddle.nn.Linear(13, 5)
# This can be any optimizer supported by dygraph.
adam = paddle.optimizer.Adam(learning_rate = 0.01,
                            parameters = linear.parameters())
out = linear(a)
out.backward()
adam.step()
adam.clear_grad()
# train on default dynamic graph mode
import paddle
import numpy as np
emb = paddle.nn.Embedding(10, 3)

## example1: LRScheduler is not used, return the same value is all the same
adam = paddle.optimizer.Adam(0.01, parameters = emb.parameters())
for batch in range(10):
    input = paddle.randint(low=0, high=5, shape=[5])
    out = emb(input)
    out.backward()
    print("Learning rate of step{}: {}".format(batch, adam.get_lr())) # 0.01
    adam.step()

## example2: StepDecay is used, return the scheduled learning rate
scheduler = paddle.optimizer.lr.StepDecay(learning_rate=0.5, step_size=2, gamma=0.1)
adam = paddle.optimizer.Adam(scheduler, parameters = emb.parameters())
for batch in range(10):
    input = paddle.randint(low=0, high=5, shape=[5])
    out = emb(input)
    out.backward()
    print("Learning rate of step{}: {}".format(batch, adam.get_lr())) # 0.5->0.05...
    adam.step()
    scheduler.step()

# train on static graph mode
paddle.enable_static()
main_prog = paddle.static.Program()
start_prog = paddle.static.Program()
with paddle.static.program_guard(main_prog, start_prog):
    x = paddle.static.data(name='x', shape=[None, 10])
    z = paddle.static.nn.fc(x, 100)
    loss = paddle.mean(z)
    scheduler = paddle.optimizer.lr.StepDecay(learning_rate=0.5, step_size=2, gamma=0.1)
    adam = paddle.optimizer.Adam(learning_rate=scheduler)
    adam.minimize(loss)

exe = paddle.static.Executor()
exe.run(start_prog)
for batch in range(10):
    print("Learning rate of step{}: {}", adam.get_lr())     # 0.5->0.05->0.005...
    out = exe.run(main_prog, feed={'x': np.random.randn(3, 10).astype('float32')})
    scheduler.step()
import paddle
linear = paddle.nn.Linear(10, 10)
input = paddle.uniform(shape=[10, 10], min=-0.1, max=0.1)
out = linear(input)
loss = paddle.mean(out)

beta1 = paddle.to_tensor([0.9], dtype="float32")
beta2 = paddle.to_tensor([0.99], dtype="float32")

adam = paddle.optimizer.Adam(learning_rate=0.1,
        parameters=linear.parameters(),
        weight_decay=0.01)
out.backward()
adam.minimize(loss)
adam.clear_grad()
import paddle
linear = paddle.nn.Linear(10, 10)

adam = paddle.optimizer.Adam(0.1, parameters=linear.parameters())

# set learning rate manually by python float value
lr_list = [0.2, 0.3, 0.4, 0.5, 0.6]
for i in range(5):
    adam.set_lr(lr_list[i])
    lr = adam.get_lr()
    print("current lr is {}".format(lr))
# Print:
#    current lr is 0.2
#    current lr is 0.3
#    current lr is 0.4
#    current lr is 0.5
#    current lr is 0.6
import paddle

emb = paddle.nn.Embedding(10, 10)

layer_state_dict = emb.state_dict()
paddle.save(layer_state_dict, "emb.pdparams")

scheduler = paddle.optimizer.lr.NoamDecay(
    d_model=0.01, warmup_steps=100, verbose=True)
adam = paddle.optimizer.Adam(
    learning_rate=scheduler,
    parameters=emb.parameters())
opt_state_dict = adam.state_dict()
paddle.save(opt_state_dict, "adam.pdopt")

opti_state_dict = paddle.load("adam.pdopt")
adam.set_state_dict(opti_state_dict)
import paddle
emb = paddle.nn.Embedding(10, 10)

adam = paddle.optimizer.Adam(0.001, parameters=emb.parameters())
state_dict = adam.state_dict()
import paddle
import numpy as np

value = np.arange(26).reshape(2, 13).astype("float32")
a = paddle.to_tensor(value)
linear = paddle.nn.Linear(13, 5)
# This can be any optimizer supported by dygraph.
adam = paddle.optimizer.Adam(learning_rate = 0.01,
                            parameters = linear.parameters())
out = linear(a)
out.backward()
adam.step()
adam.clear_grad()
# Example1: set Regularizer in optimizer
import paddle
from paddle.regularizer import L1Decay
import numpy as np
linear = paddle.nn.Linear(10, 10)
inp = paddle.rand(shape=[10, 10], dtype="float32")
out = linear(inp)
loss = paddle.mean(out)
beta1 = paddle.to_tensor([0.9], dtype="float32")
beta2 = paddle.to_tensor([0.99], dtype="float32")
momentum = paddle.optimizer.Momentum(
    learning_rate=0.1,
    parameters=linear.parameters(),
    weight_decay=L1Decay(0.0001))
back = out.backward()
momentum.step()
momentum.clear_grad()

# Example2: set Regularizer in parameters
# Set L1 regularization in parameters.
# Global regularizer does not take effect on my_conv2d for this case.
from paddle.nn import Conv2D
from paddle import ParamAttr
from paddle.regularizer import L2Decay

my_conv2d = Conv2D(
        in_channels=10,
        out_channels=10,
        kernel_size=1,
        stride=1,
        padding=0,
        weight_attr=ParamAttr(regularizer=L2Decay(coeff=0.01)),
        bias_attr=False)
# Example1: set Regularizer in optimizer
import paddle
from paddle.regularizer import L2Decay
import numpy as np
linear = paddle.nn.Linear(10, 10)
inp = paddle.rand(shape=[10, 10], dtype="float32")
out = linear(inp)
loss = paddle.mean(out)
beta1 = paddle.to_tensor([0.9], dtype="float32")
beta2 = paddle.to_tensor([0.99], dtype="float32")
momentum = paddle.optimizer.Momentum(
    learning_rate=0.1,
    parameters=linear.parameters(),
    weight_decay=L2Decay(0.0001))
back = out.backward()
momentum.step()
momentum.clear_grad()

# Example2: set Regularizer in parameters
# Set L2 regularization in parameters.
# Global regularizer does not take effect on my_conv2d for this case.
from paddle.nn import Conv2D
from paddle import ParamAttr
from paddle.regularizer import L2Decay

my_conv2d = Conv2D(
        in_channels=10,
        out_channels=10,
        kernel_size=1,
        stride=1,
        padding=0,
        weight_attr=ParamAttr(regularizer=L2Decay(coeff=0.01)),
        bias_attr=False)
import numpy as np
import paddle
from paddle.signal import stft, istft

paddle.seed(0)

# STFT
x = paddle.randn([8, 48000], dtype=paddle.float64)
y = stft(x, n_fft=512)  # [8, 257, 376]

# ISTFT
x_ = istft(y, n_fft=512)  # [8, 48000]

np.allclose(x, x_)  # True
import paddle
from paddle.signal import stft

# real-valued input
x = paddle.randn([8, 48000], dtype=paddle.float64)
y1 = stft(x, n_fft=512)  # [8, 257, 376]
y2 = stft(x, n_fft=512, onesided=False)  # [8, 512, 376]

# complex input
x = paddle.randn([8, 48000], dtype=paddle.float64) +                     paddle.randn([8, 48000], dtype=paddle.float64)*1j  # [8, 48000] complex128
y1 = stft(x, n_fft=512, center=False, onesided=False)  # [8, 512, 372]
import numpy as np

import paddle
import paddle.static as static
import paddle.nn.functional as F

paddle.enable_static()
data = static.data(name="input", shape=[-1, 32, 32], dtype="float32")
label = static.data(name="label", shape=[-1,1], dtype="int")
fc_out = static.nn.fc(x=data, size=10)
predict = F.softmax(x=fc_out)
result = static.accuracy(input=predict, label=label, k=5)

place = paddle.CPUPlace()
exe = static.Executor(place)

exe.run(static.default_startup_program())
x = np.random.rand(3, 32, 32).astype("float32")
y = np.array([[1],[0],[1]])
output= exe.run(feed={"input": x,"label": y},
            fetch_list=[result[0]])
print(output)

#[array([0.], dtype=float32)]
import paddle
import paddle.nn.functional as F

paddle.enable_static()

x = paddle.static.data(name='x', shape=[None, 13], dtype='int64')
y = paddle.static.data(name='y', shape=[None, 1], dtype='float32')
x_emb = paddle.static.nn.embedding(x, size=[100, 256])
y_predict = paddle.static.nn.fc(x=x_emb, size=1, activation=None, name='my_fc')
loss = F.square_error_cost(input=y_predict, label=y)
avg_loss = paddle.mean(loss)

# Get all weights in main_program, not include bias.
all_weights = [param for param in paddle.static.default_main_program().block(0).all_parameters() if 'w_' in param.name]
all_weights_name = [w.name for w in all_weights]

# return all param_grads needed to be updated if parameter_list set default None.
p_g_list1 = paddle.static.append_backward(loss=avg_loss)
# output: [(embedding_0.w_0, embedding_0.w_0@GRAD), (my_fc.w_0, my_fc.w_0@GRAD), (my_fc.b_0, my_fc.b_0@GRAD)]

# return the param_grads corresponding to parameter_list that can be list of param (Tensor).
p_g_list2 = paddle.static.append_backward(loss=avg_loss, parameter_list=all_weights)
# output: [(embedding_0.w_0, embedding_0.w_0@GRAD), (my_fc.w_0, my_fc.w_0@GRAD)]

# parameter_list can be list of param.name (str).
p_g_list3 = paddle.static.append_backward(loss=avg_loss, parameter_list=all_weights_name)
# output: [(embedding_0.w_0, embedding_0.w_0@GRAD), (my_fc.w_0, my_fc.w_0@GRAD)]

# no_grad_set can be set of Tensors that means grad will be cut off from these Tensors.
p_g_list4 = paddle.static.append_backward(loss=avg_loss, no_grad_set=set([x_emb]))
# output: [(my_fc.w_0, my_fc.w_0@GRAD), (my_fc.b_0, my_fc.b_0@GRAD)]

# no_grad_set can be set of Tensor.name when the Tensor is created inside layers and can't be specified explicitly.
p_g_list5 = paddle.static.append_backward(loss=avg_loss, no_grad_set=set(['my_fc.b_0']))
# output: [(embedding_0.w_0, embedding_0.w_0@GRAD), (my_fc.w_0, my_fc.w_0@GRAD)]

# return [] because all param_grads are filtered by no_grad_set.
p_g_list6 = paddle.static.append_backward(loss=avg_loss, parameter_list=all_weights, no_grad_set=set(all_weights))
import numpy as np

import paddle
import paddle.static as static
import paddle.nn.functional as F

paddle.enable_static()
data = static.data(name="input", shape=[-1, 32,32], dtype="float32")
label = static.data(name="label", shape=[-1], dtype="int")
fc_out = static.nn.fc(x=data, size=2)
predict = F.softmax(x=fc_out)
result = static.auc(input=predict, label=label)

place = paddle.CPUPlace()
exe = static.Executor(place)

exe.run(static.default_startup_program())
x = np.random.rand(3,32,32).astype("float32")
y = np.array([1,0,1])
output= exe.run(feed={"input": x,"label": y},
            fetch_list=[result[0]])
print(output)
#[array([0.])]
import os
import paddle
import paddle.static as static

paddle.enable_static()

os.environ['CPU_NUM'] = str(2)
places = static.cpu_places()

data = static.data(name="x", shape=[None, 1], dtype="float32")
hidden = static.nn.fc(input=data, size=10)
loss = paddle.mean(hidden)
paddle.optimizer.SGD(learning_rate=0.01).minimize(loss)

build_strategy = static.BuildStrategy()
build_strategy.enable_inplace = True
build_strategy.memory_optimize = True
build_strategy.reduce_strategy = static.BuildStrategy.ReduceStrategy.Reduce
program = static.CompiledProgram(static.default_main_program())
program = program.with_data_parallel(loss_name=loss.name,
                                      build_strategy=build_strategy,
                                      places=places)
import paddle
import paddle.static as static

paddle.enable_static()

build_strategy = static.BuildStrategy()
build_strategy.debug_graphviz_path = "./graph"
import paddle
import paddle.static as static

paddle.enable_static()

build_strategy = static.BuildStrategy()
build_strategy.enable_auto_fusion = True
import paddle
import paddle.static as static

paddle.enable_static()

build_strategy = static.BuildStrategy()
build_strategy.enable_sequential_execution = True
import paddle
import paddle.static as static

paddle.enable_static()

build_strategy = static.BuildStrategy()
build_strategy.fuse_bn_act_ops = True
import paddle
import paddle.static as static

paddle.enable_static()

build_strategy = static.BuildStrategy()
build_strategy.fuse_bn_add_act_ops = True
import paddle
import paddle.static as static

paddle.enable_static()

build_strategy = static.BuildStrategy()
build_strategy.fuse_broadcast_ops = True
import paddle
import paddle.static as static

paddle.enable_static()

build_strategy = static.BuildStrategy()
build_strategy.fuse_elewise_add_act_ops = True
import paddle
import paddle.static as static

paddle.enable_static()

build_strategy = static.BuildStrategy()
build_strategy.fuse_relu_depthwise_conv = True
import numpy
import os
import paddle
import paddle.static as static

paddle.enable_static()

use_cuda = True
place = paddle.CUDAPlace(0) if use_cuda else paddle.CPUPlace()
exe = static.Executor(place)

# NOTE: If you use CPU to run the program, you need
# to specify the CPU_NUM, otherwise, paddle will use
# all the number of the logic core as the CPU_NUM,
# in that case, the batch size of the input should be
# greater than CPU_NUM, if not, the process will be
# failed by an exception.
if not use_cuda:
    os.environ['CPU_NUM'] = str(2)
    places = static.cpu_places()
else:
    places = static.cuda_places()

data = static.data(name='X', shape=[None, 1], dtype='float32')
hidden = static.nn.fc(input=data, size=10)
loss = paddle.mean(hidden)
paddle.optimizer.SGD(learning_rate=0.01).minimize(loss)

exe.run(static.default_startup_program())

build_strategy = static.BuildStrategy()
build_strategy.gradient_scale_strategy = \
          static.BuildStrategy.GradientScaleStrategy.Customized
compiled_prog = static.CompiledProgram(
          static.default_main_program()).with_data_parallel(
                  loss_name=loss.name, build_strategy=build_strategy,
                  places=places)

dev_count =  len(places)
x = numpy.random.random(size=(10, 1)).astype('float32')
loss_grad = numpy.ones((dev_count)).astype("float32") * 0.01
loss_grad_name = loss.name+"@GRAD"
loss_data = exe.run(compiled_prog,
                      feed={"X": x, loss_grad_name : loss_grad},
                      fetch_list=[loss.name, loss_grad_name])
import paddle
import paddle.static as static

paddle.enable_static()

build_strategy = static.BuildStrategy()
build_strategy.memory_optimize = True
import paddle
import paddle.static as static

paddle.enable_static()

build_strategy = static.BuildStrategy()
build_strategy.reduce_strategy = static.BuildStrategy.ReduceStrategy.Reduce
import paddle
import paddle.static as static

paddle.enable_static()

build_strategy = static.BuildStrategy()
build_strategy.remove_unnecessary_lock = True
import paddle
import paddle.static as static

paddle.enable_static()

build_strategy = static.BuildStrategy()
build_strategy.sync_batch_norm = True
import numpy
import paddle
import paddle.static as static

paddle.enable_static()

place = paddle.CUDAPlace(0) # paddle.CPUPlace()
exe = static.Executor(place)

data = static.data(name='X', shape=[None, 1], dtype='float32')
hidden = static.nn.fc(x=data, size=10)
loss = paddle.mean(hidden)
paddle.optimizer.SGD(learning_rate=0.01).minimize(loss)

exe.run(static.default_startup_program())
compiled_prog = static.CompiledProgram(
    static.default_main_program())

x = numpy.random.random(size=(10, 1)).astype('float32')
loss_data, = exe.run(compiled_prog,
                    feed={"X": x},
                    fetch_list=[loss.name])
import numpy
import os
import paddle
import paddle.static as static

paddle.enable_static()

use_cuda = True
place = paddle.CUDAPlace(0) if use_cuda else paddle.CPUPlace()
parallel_places = [paddle.CUDAPlace(0), paddle.CUDAPlace(1)] if use_cuda else [paddle.CPUPlace()] * 2

# NOTE: If you use CPU to run the program, you need
# to specify the CPU_NUM, otherwise, paddle will use
# all the number of the logic core as the CPU_NUM,
# in that case, the batch size of the input should be
# greater than CPU_NUM, if not, the process will be
# failed by an exception.
if not use_cuda:
    os.environ['CPU_NUM'] = str(2)

exe = static.Executor(place)

data = static.data(name='X', shape=[None, 1], dtype='float32')
hidden = static.nn.fc(x=data, size=10)
loss = paddle.mean(hidden)

test_program = static.default_main_program().clone(for_test=True)
paddle.optimizer.SGD(learning_rate=0.01).minimize(loss)

exe.run(static.default_startup_program())
compiled_train_prog = static.CompiledProgram(
    static.default_main_program()).with_data_parallel(
            loss_name=loss.name, places=parallel_places)
# NOTE: if not set share_vars_from=compiled_train_prog,
# the parameters used in test process are different with
# the parameters used by train process
compiled_test_prog = static.CompiledProgram(
    test_program).with_data_parallel(
            share_vars_from=compiled_train_prog,
            places=parallel_places)

train_data = numpy.random.random(size=(10, 1)).astype('float32')
loss_data, = exe.run(compiled_train_prog,
                feed={"X": train_data},
                fetch_list=[loss.name])
test_data = numpy.random.random(size=(10, 1)).astype('float32')
loss_data, = exe.run(compiled_test_prog,
                feed={"X": test_data},
                fetch_list=[loss.name])
import paddle
import paddle.static as static

paddle.enable_static()

cpu_places = static.cpu_places()
import paddle
paddle.enable_static()
var = paddle.static.create_global_var(shape=[2,3], value=1.0, dtype='float32',
                               persistable=True, force_cpu=True, name='new_var')
import paddle
import paddle.static as static

# required: gpu

paddle.enable_static()

cuda_places = static.cuda_places()
import numpy as np
import paddle
paddle.enable_static()

# Creates a variable with fixed size [3, 2, 1]
# User can only feed data of the same shape to x
# the dtype is not set, so it will set "float32" by
# paddle.get_default_dtype(). You can use paddle.get_default_dtype() to
# change the global dtype
x = paddle.static.data(name='x', shape=[3, 2, 1])

# Creates a variable with changeable batch size -1.
# Users can feed data of any batch size into y,
# but size of each data sample has to be [2, 1]
y = paddle.static.data(name='y', shape=[-1, 2, 1], dtype='float32')

z = x + y

# In this example, we will feed x and y with np-ndarray "1"
# and fetch z, like implementing "1 + 1 = 2" in PaddlePaddle
feed_data = np.ones(shape=[3, 2, 1], dtype=np.float32)

exe = paddle.static.Executor(paddle.framework.CPUPlace())
out = exe.run(paddle.static.default_main_program(),
              feed={
                  'x': feed_data,
                  'y': feed_data
              },
              fetch_list=[z.name])

# np-ndarray of shape=[3, 2, 1], dtype=float32, whose elements are 2
print(out)
import paddle

paddle.enable_static()
# Sample Network:
x = paddle.static.data(name='x', shape=[100, 100], dtype='float32')
y = paddle.static.data(name='x', shape=[100, 100], dtype='float32')
out = paddle.add(x, y)

#print the number of blocks in the program, 1 in this case
print(paddle.static.default_main_program().num_blocks) # 1
#print the default_main_program
print(paddle.static.default_main_program())
import paddle

paddle.enable_static()
x = paddle.static.data(name="x", shape=[-1, 784], dtype='float32')
out = paddle.static.nn.fc(name="fc", x=x, size=10, activation="relu")
print("main program is: {}".format(paddle.static.default_main_program()))
print("start up program is: {}".format(paddle.static.default_startup_program()))
import paddle

paddle.enable_static()

path_prefix = "./infer_model"

# User defined network, here a softmax regession example
image = paddle.static.data(name='img', shape=[None, 28, 28], dtype='float32')
label = paddle.static.data(name='label', shape=[None, 1], dtype='int64')
predict = paddle.static.nn.fc(image, 10, activation='softmax')

loss = paddle.nn.functional.cross_entropy(predict, label)

exe = paddle.static.Executor(paddle.CPUPlace())
exe.run(paddle.static.default_startup_program())

# serialize parameters to bytes.
serialized_params = paddle.static.serialize_persistables([image], [predict], exe)

# deserialize bytes to parameters.
main_program = paddle.static.default_main_program()
deserialized_params = paddle.static.deserialize_persistables(main_program, serialized_params, exe)
import paddle

paddle.enable_static()

path_prefix = "./infer_model"

# User defined network, here a softmax regession example
image = paddle.static.data(name='img', shape=[None, 28, 28], dtype='float32')
label = paddle.static.data(name='label', shape=[None, 1], dtype='int64')
predict = paddle.static.nn.fc(image, 10, activation='softmax')

loss = paddle.nn.functional.cross_entropy(predict, label)

exe = paddle.static.Executor(paddle.CPUPlace())
exe.run(paddle.static.default_startup_program())

# serialize the default main program to bytes.
serialized_program = paddle.static.serialize_program([image], [predict])

# deserialize bytes to program
deserialized_program = paddle.static.deserialize_program(serialized_program)
import paddle

paddle.enable_static()
support_gpu = paddle.is_compiled_with_cuda()
place = paddle.CPUPlace()
if support_gpu:
    place = paddle.CUDAPlace(0)

# if GPU is supported, the three OPs below will be automatically assigned to CUDAPlace(0)
data1 = paddle.full(shape=[1, 3, 8, 8], fill_value=0.5, dtype='float32')
data2 = paddle.full(shape=[1, 3, 64], fill_value=0.5, dtype='float32')
shape = paddle.shape(data2)

with paddle.static.device_guard("cpu"):
    # Ops created here will be placed on CPUPlace
    shape = paddle.slice(shape, axes=[0], starts=[0], ends=[4])
with paddle.static.device_guard('gpu'):
    # if GPU is supported, OPs created here will be placed on CUDAPlace(0), otherwise on CPUPlace
    out = paddle.reshape(data1, shape=shape)

exe = paddle.static.Executor(place)
exe.run(paddle.static.default_startup_program())
result = exe.run(fetch_list=[out])
import paddle
import paddle.static as static
import paddle.nn.functional as F

paddle.enable_static()

x = static.data(name='x', shape=[None, 13], dtype='float32')
y = static.data(name='y', shape=[None, 1], dtype='float32')
y_predict = static.nn.fc(input=x, size=1, act=None)

cost = F.square_error_cost(input=y_predict, label=y)
avg_loss = paddle.mean(cost)

sgd_optimizer = paddle.optimizer.SGD(learning_rate=0.001)
sgd_optimizer.minimize(avg_loss)

exec_strategy = static.ExecutionStrategy()
exec_strategy.num_threads = 4

train_exe = static.ParallelExecutor(use_cuda=False,
                                    loss_name=avg_loss.name,
                                    exec_strategy=exec_strategy)
import paddle
import paddle.static as static

paddle.enable_static()

exec_strategy = static.ExecutionStrategy()
exec_strategy.num_iteration_per_drop_scope = 10
import paddle
import paddle.static as static

paddle.enable_static()

exec_strategy = static.ExecutionStrategy()
exec_strategy.num_iteration_per_run = 10
import paddle
import paddle.static as static

paddle.enable_static()

exec_strategy = static.ExecutionStrategy()
exec_strategy.num_threads = 4
import paddle
import numpy
import os

# Executor is only used in static graph mode
paddle.enable_static()

# Set place explicitly.
# use_cuda = True
# place = paddle.CUDAPlace(0) if use_cuda else paddle.CPUPlace()
# exe = paddle.static.Executor(place)

# If you don't set place, PaddlePaddle sets the default device.
exe = paddle.static.Executor()

train_program = paddle.static.Program()
startup_program = paddle.static.Program()
with paddle.static.program_guard(train_program, startup_program):
    data = paddle.static.data(name='X', shape=[None, 1], dtype='float32')
    hidden = paddle.static.nn.fc(data, 10)
    loss = paddle.mean(hidden)
    paddle.optimizer.SGD(learning_rate=0.01).minimize(loss)

# Run the startup program once and only once.
# Not need to optimize/compile the startup program.
exe.run(startup_program)

# Run the main program directly without compile.
x = numpy.random.random(size=(10, 1)).astype('float32')
loss_data, = exe.run(train_program, feed={"X": x}, fetch_list=[loss.name])

# Or, compiled the program and run. See `CompiledProgram`
# for more details.
# NOTE: If you use CPU to run the program or Paddle is
# CPU version, you need to specify the CPU_NUM, otherwise,
# PaddlePaddle will use all the number of the logic core as
# the CPU_NUM, in that case, the batch size of the input
# should be greater than CPU_NUM, if not, the process will be
# failed by an exception.

# Set place explicitly.
# if not use_cuda:
#     os.environ['CPU_NUM'] = str(2)

# If you don't set place and PaddlePaddle is CPU version
os.environ['CPU_NUM'] = str(2)

compiled_prog = paddle.static.CompiledProgram(
    train_program).with_data_parallel(loss_name=loss.name)
loss_data, = exe.run(compiled_prog, feed={"X": x}, fetch_list=[loss.name])
import paddle

cpu = paddle.CPUPlace()
exe = paddle.static.Executor(cpu)
# execute training or testing
exe.close()
import paddle
import numpy

# First create the Executor.
paddle.enable_static()
place = paddle.CPUPlace()  # paddle.CUDAPlace(0)
exe = paddle.static.Executor(place)

data = paddle.static.data(name='X', shape=[None, 1], dtype='float32')
hidden = paddle.static.nn.fc(data, 10)
loss = paddle.mean(hidden)
adam = paddle.optimizer.Adam()
adam.minimize(loss)
i = paddle.zeros(shape=[1], dtype='int64')
array = paddle.fluid.layers.array_write(x=loss, i=i)

# Run the startup program once and only once.
exe.run(paddle.static.default_startup_program())

x = numpy.random.random(size=(10, 1)).astype('float32')
loss_val, array_val = exe.run(feed={'X': x},
                              fetch_list=[loss.name, array.name])
print(array_val)
# [array([0.02153828], dtype=float32)]
import paddle
import numpy as np

# First create the Executor.
paddle.enable_static()
place = paddle.CUDAPlace(0)
exe = paddle.static.Executor(place)

data = paddle.static.data(name='X', shape=[None, 1], dtype='float32')
class_dim = 2
prediction = paddle.static.nn.fc(data, class_dim)
loss = paddle.mean(prediction)
adam = paddle.optimizer.Adam()
adam.minimize(loss)

# Run the startup program once and only once.
exe.run(paddle.static.default_startup_program())
build_strategy = paddle.static.BuildStrategy()
binary = paddle.static.CompiledProgram(
    paddle.static.default_main_program()).with_data_parallel(
        loss_name=loss.name, build_strategy=build_strategy)
batch_size = 6
x = np.random.random(size=(batch_size, 1)).astype('float32')

# Set return_merged as False to fetch unmerged results:
unmerged_prediction, = exe.run(binary,
                               feed={'X': x},
                               fetch_list=[prediction.name],
                               return_merged=False)
# If the user uses two GPU cards to run this python code, the printed result will be
# (2, 3, class_dim). The first dimension value of the printed result is the number of used
# GPU cards, and the second dimension value is the quotient of batch_size and the
# number of used GPU cards.
print("The unmerged prediction shape: {}".format(
    np.array(unmerged_prediction).shape))
print(unmerged_prediction)

# Set return_merged as True to fetch merged results:
merged_prediction, = exe.run(binary,
                             feed={'X': x},
                             fetch_list=[prediction.name],
                             return_merged=True)
# If the user uses two GPU cards to run this python code, the printed result will be
# (6, class_dim). The first dimension value of the printed result is the batch_size.
print("The merged prediction shape: {}".format(
    np.array(merged_prediction).shape))
print(merged_prediction)

# Out:
# The unmerged prediction shape: (2, 3, 2)
# [array([[-0.37620035, -0.19752218],
#        [-0.3561043 , -0.18697084],
#        [-0.24129935, -0.12669306]], dtype=float32), array([[-0.24489994, -0.12858354],
#        [-0.49041364, -0.25748932],
#        [-0.44331917, -0.23276259]], dtype=float32)]
# The merged prediction shape: (6, 2)
# [[-0.37789783 -0.19921964]
#  [-0.3577645  -0.18863106]
#  [-0.24274671 -0.12814042]
#  [-0.24635398 -0.13003758]
#  [-0.49232286 -0.25939852]
#  [-0.44514108 -0.2345845 ]]
import paddle

paddle.enable_static()
place = paddle.CPUPlace()  # you can set place = paddle.CUDAPlace(0) to use gpu
exe = paddle.static.Executor(place)
x = paddle.static.data(name="x", shape=[None, 10, 10], dtype="int64")
y = paddle.static.data(name="y", shape=[None, 1], dtype="int64", lod_level=1)
dataset = paddle.fluid.DatasetFactory().create_dataset()
dataset.set_use_var([x, y])
dataset.set_thread(1)
# you should set your own filelist, e.g. filelist = ["dataA.txt"]
filelist = []
dataset.set_filelist(filelist)
exe.run(paddle.static.default_startup_program())
exe.infer_from_dataset(program=paddle.static.default_main_program(),
                       dataset=dataset)
import paddle

paddle.enable_static()
place = paddle.CPUPlace() # you can set place = paddle.CUDAPlace(0) to use gpu
exe = paddle.static.Executor(place)
x = paddle.static.data(name="x", shape=[None, 10, 10], dtype="int64")
y = paddle.static.data(name="y", shape=[None, 1], dtype="int64", lod_level=1)
dataset = paddle.fluid.DatasetFactory().create_dataset()
dataset.set_use_var([x, y])
dataset.set_thread(1)
# you should set your own filelist, e.g. filelist = ["dataA.txt"]
filelist = []
dataset.set_filelist(filelist)
exe.run(paddle.static.default_startup_program())
exe.train_from_dataset(program=paddle.static.default_main_program(),
                       dataset=dataset)
import numpy
import paddle
import paddle.static as static
from paddle.static import ExponentialMovingAverage

paddle.enable_static()

data = static.data(name='x', shape=[-1, 5], dtype='float32')
hidden = static.nn.fc(x=data, size=10)
cost = paddle.mean(hidden)

test_program = static.default_main_program().clone(for_test=True)
optimizer = paddle.optimizer.Adam(learning_rate=0.001)
optimizer.minimize(cost)

ema = ExponentialMovingAverage(0.999)
ema.update()

place = paddle.CPUPlace()
exe = static.Executor(place)
exe.run(static.default_startup_program())

for pass_id in range(3):
    for batch_id in range(6):
        data = numpy.random.random(size=(10, 5)).astype('float32')
        exe.run(program=static.default_main_program(),
        feed={'x': data},
        fetch_list=[cost.name])

    # usage 1
    with ema.apply(exe):
        data = numpy.random.random(size=(10, 5)).astype('float32')
        exe.run(program=test_program,
            feed={'x': data},
            fetch_list=[hidden.name])

    # usage 2
    with ema.apply(exe, need_restore=False):
        data = numpy.random.random(size=(10, 5)).astype('float32')
        exe.run(program=test_program,
            feed={'x': data},
            fetch_list=[hidden.name])
    ema.restore(exe)
import paddle
import numpy

paddle.static.global_scope().var("data").get_tensor().set(numpy.ones((2, 2)), paddle.CPUPlace())
numpy.array(paddle.static.global_scope().find_var("data").get_tensor())
import paddle
import paddle.nn.functional as F

paddle.enable_static()

x = paddle.static.data(name='x', shape=[None, 2, 8, 8], dtype='float32')
x.stop_gradient=False
y = paddle.static.nn.conv2d(x, 4, 1, bias_attr=False)
y = F.relu(y)
z = paddle.static.gradients([y], x)
print(z) # [var x@GRAD : fluid.VarType.LOD_TENSOR.shape(-1L, 2L, 8L, 8L).astype(VarType.FP32)]
from paddle.static import InputSpec

input = InputSpec([None, 784], 'float32', 'x')
label = InputSpec([None, 1], 'int64', 'label')

print(input)  # InputSpec(shape=(-1, 784), dtype=VarType.FP32, name=x)
print(label)  # InputSpec(shape=(-1, 1), dtype=VarType.INT64, name=label)
import numpy as np
import paddle
from paddle.static import InputSpec

paddle.disable_static()

x = paddle.to_tensor(np.ones([2, 2], np.float32))
x_spec = InputSpec.from_tensor(x, name='x')
print(x_spec)  # InputSpec(shape=(2, 2), dtype=VarType.FP32, name=x)
import numpy as np
from paddle.static import InputSpec

x = np.ones([2, 2], np.float32)
x_spec = InputSpec.from_numpy(x, name='x')
print(x_spec)  # InputSpec(shape=(2, 2), dtype=VarType.FP32, name=x)
from paddle.static import InputSpec

x_spec = InputSpec(shape=[64], dtype='float32', name='x')
x_spec.batch(4)
print(x_spec) # InputSpec(shape=(4, 64), dtype=VarType.FP32, name=x)
from paddle.static import InputSpec

x_spec = InputSpec(shape=[4, 64], dtype='float32', name='x')
x_spec.unbatch()
print(x_spec) # InputSpec(shape=(64,), dtype=VarType.FP32, name=x)
import paddle
import paddle.static as static

paddle.enable_static()

x = static.data(name="x", shape=[10, 10], dtype='float32')
y = static.nn.fc(x, 10)
z = static.nn.fc(y, 10)

place = paddle.CPUPlace()
exe = static.Executor(place)
exe.run(static.default_startup_program())
prog = static.default_main_program()

static.save(prog, "./temp")
static.load(prog, "./temp")
import paddle
import numpy as np

paddle.enable_static()

# Build the model
startup_prog = paddle.static.default_startup_program()
main_prog = paddle.static.default_main_program()
with paddle.static.program_guard(main_prog, startup_prog):
    image = paddle.static.data(name="img", shape=[64, 784])
    w = paddle.create_parameter(shape=[784, 200], dtype='float32')
    b = paddle.create_parameter(shape=[200], dtype='float32')
    hidden_w = paddle.matmul(x=image, y=w)
    hidden_b = paddle.add(hidden_w, b)
exe = paddle.static.Executor(paddle.CPUPlace())
exe.run(startup_prog)

# Save the inference model
path_prefix = "./infer_model"
paddle.static.save_inference_model(path_prefix, [image], [hidden_b], exe)

[inference_program, feed_target_names, fetch_targets] = (
    paddle.static.load_inference_model(path_prefix, exe))
tensor_img = np.array(np.random.random((64, 784)), dtype=np.float32)
results = exe.run(inference_program,
              feed={feed_target_names[0]: tensor_img},
              fetch_list=fetch_targets)

# In this example, the inference program was saved in file
# "./infer_model.pdmodel" and parameters were saved in file
# " ./infer_model.pdiparams".
# By the inference program, feed_target_names and
# fetch_targets, we can use an executor to run the inference
# program to get the inference result.
import paddle
import paddle.static as static

paddle.enable_static()

x = static.data(name="x", shape=[10, 10], dtype='float32')
y = static.nn.fc(x, 10)
z = static.nn.fc(y, 10)

place = paddle.CPUPlace()
exe = static.Executor(place)
exe.run(static.default_startup_program())
prog = static.default_main_program()

static.save(prog, "./temp")
program_state = static.load_program_state("./temp")
import paddle
paddle.enable_static()
with paddle.static.name_scope("s1"):
   a = paddle.static.data(name='data', shape=[None, 1], dtype='int32')
   b = a + 1
   with paddle.static.name_scope("s2"):
      c = b * 1
   with paddle.static.name_scope("s3"):
      d = c / 1
with paddle.static.name_scope("s1"):
      f = paddle.tensor.pow(d, 2.0)
with paddle.static.name_scope("s4"):
      g = f - 1

# Op are created in the default main program.
for op in paddle.static.default_main_program().block(0).ops:
    # elementwise_add is created in /s1/
    if op.type == 'elementwise_add':
        assert op.desc.attr("op_namescope") == '/s1/'
    # elementwise_mul is created in '/s1/s2'
    elif op.type == 'elementwise_mul':
        assert op.desc.attr("op_namescope") == '/s1/s2/'
    # elementwise_div is created in '/s1/s3'
    elif op.type == 'elementwise_div':
        assert op.desc.attr("op_namescope") == '/s1/s3/'
    # elementwise_sum is created in '/s4'
    elif op.type == 'elementwise_sub':
        assert op.desc.attr("op_namescope") == '/s4/'
    # pow is created in /s1_1/
    elif op.type == 'pow':
        assert op.desc.attr("op_namescope") == '/s1_1/'
import paddle

paddle.enable_static()

path_prefix = "./infer_model"

# User defined network, here a softmax regession example
image = paddle.static.data(name='img', shape=[None, 28, 28], dtype='float32')
label = paddle.static.data(name='label', shape=[None, 1], dtype='int64')
predict = paddle.static.nn.fc(image, 10, activation='softmax')

loss = paddle.nn.functional.cross_entropy(predict, label)

exe = paddle.static.Executor(paddle.CPUPlace())
exe.run(paddle.static.default_startup_program())

# normalize main program.
program = paddle.static.default_main_program()
normalized_program = paddle.static.normalize_program(program, [image], [predict])
import paddle
import numpy
import os

use_cuda = True
paddle.enable_static()
place = paddle.CUDAPlace(0) if use_cuda else paddle.CPUPlace()

# NOTE: If you use CPU to run the program, you need
# to specify the CPU_NUM, otherwise, PaddlePaddle will use
# all the number of the logic core as the CPU_NUM,
# in that case, the batch size of the input should be
# greater than CPU_NUM, if not, the process will be
# failed by an exception.
if not use_cuda:
    os.environ['CPU_NUM'] = str(2)

exe = paddle.static.Executor(place)

train_program = paddle.static.Program()
startup_program = paddle.static.Program()
with paddle.static.program_guard(train_program, startup_program):
    data = paddle.static.data(name='X', shape=[None, 1], dtype='float32')
    hidden = paddle.static.nn.fc(data, 10)
    loss = paddle.mean(hidden)
    test_program = paddle.static.default_main_program().clone(for_test=True)
    paddle.optimizer.SGD(learning_rate=0.01).minimize(loss)

exe.run(startup_program)

train_exe = paddle.static.ParallelExecutor(use_cuda=use_cuda,
                                           main_program=train_program,
                                           loss_name=loss.name)
# Note: if share_vars_from is not set here, the test parameter is different to the train one
test_exe = paddle.static.ParallelExecutor(use_cuda=use_cuda,
                                          main_program=test_program,
                                          share_vars_from=train_exe)

x = numpy.random.random(size=(10, 1)).astype('float32')
loss_data, = train_exe.run(feed={"X": x},
                           fetch_list=[loss.name])

loss_data, = test_exe.run(feed={"X": x},
                          fetch_list=[loss.name])
import paddle
import numpy
import os

use_cuda = True
paddle.enable_static()
place = paddle.CUDAPlace(0) if use_cuda else paddle.CPUPlace()

# NOTE: If you use CPU to run the program, you need
# to specify the CPU_NUM, otherwise, PaddlePaddle will use
# all the number of the logic core as the CPU_NUM,
# in that case, the batch size of the input should be
# greater than CPU_NUM, if not, the process will be
# failed by an exception.
if not use_cuda:
    os.environ['CPU_NUM'] = str(2)

exe = paddle.static.Executor(place)

train_program = paddle.static.Program()
startup_program = paddle.static.Program()
with paddle.static.program_guard(train_program, startup_program):
    data = paddle.static.data(name='X', shape=[None, 1], dtype='float32')
    hidden = paddle.static.nn.fc(data, 10)
    loss = paddle.mean(hidden)
    paddle.optimizer.SGD(learning_rate=0.01).minimize(loss)

exe.run(startup_program)

train_exe = paddle.static.ParallelExecutor(use_cuda=use_cuda,
                                           main_program=train_program,
                                           loss_name=loss.name)

# If the feed is a dict:
# the image will be split into devices. If there is two devices
# each device will process an image with shape (5, 1)
x = numpy.random.random(size=(10, 1)).astype('float32')
loss_data, = train_exe.run(feed={"X": x},
                           fetch_list=[loss.name])

# If the feed is a list:
# each device will process each element in the list.
# the 1st device will process an image with shape (10, 1)
# the 2nd device will process an image with shape (9, 1)
#
# you can use exe.device_count to get the device number.
x2 = numpy.random.random(size=(9, 1)).astype('float32')
loss_data, = train_exe.run(feed=[{"X": x}, {"X": x2}],
                           fetch_list=[loss.name])
import paddle
import numpy
import os

use_cuda = True
# NOTE: If you use CPU to run the program, you need
# to specify the CPU_NUM, otherwise, PaddlePaddle will use
# all the number of the logic core as the CPU_NUM,
# in that case, the batch size of the input should be
# greater than CPU_NUM, if not, the process will be
# failed by an exception.
if not use_cuda:
    os.environ['CPU_NUM'] = str(2)

paddle.enable_static()
train_program = paddle.static.Program()
startup_program = paddle.static.Program()
with paddle.static.program_guard(train_program, startup_program):
    data = paddle.static.data(name='X', shape=[None, 1], dtype='float32')
    hidden = paddle.static.nn.fc(data, 10)
    loss = paddle.mean(hidden)

place = paddle.CUDAPlace(0) if use_cuda else paddle.CPUPlace()
exe = paddle.static.Executor(place)
exe.run(startup_program)

parallel_exe = paddle.static.ParallelExecutor(use_cuda=use_cuda,
                                              main_program=train_program,
                                              loss_name=loss.name)

x = numpy.random.random(size=(10, 1)).astype('float32')
loss_data, = parallel_exe.run(feed={"X": x},
                              fetch_list=[loss.name])

parallel_exe.drop_local_exe_scopes()
import paddle

paddle.enable_static()

x = paddle.full(shape=[2, 3], fill_value=3, dtype='int64')
out = paddle.static.Print(x, message="The content of input layer:")

main_program = paddle.static.default_main_program()
exe = paddle.static.Executor(place=paddle.CPUPlace())
res = exe.run(main_program, fetch_list=[out])
# Variable: fill_constant_1.tmp_0
#   - message: The content of input layer:
#   - lod: {}
#   - place: CPUPlace
#   - shape: [2, 3]
#   - layout: NCHW
#   - dtype: long
#   - data: [3 3 3 3 3 3]
import paddle
import paddle.static as static

paddle.enable_static()

main_program = static.Program()
startup_program = static.Program()
with static.program_guard(main_program=main_program, startup_program=startup_program):
    x = static.data(name="x", shape=[-1, 784], dtype='float32')
    y = static.data(name="y", shape=[-1, 1], dtype='int32')
    z = static.nn.fc(name="fc", x=x, size=10, activation="relu")

print("main program is: {}".format(main_program))
print("start up program is: {}".format(startup_program))
import paddle
import paddle.static as static

paddle.enable_static()

prog = static.default_main_program()
print(prog.random_seed)
## 0
## the default random seed is 0

prog.global_seed(102)
prog1 = static.default_main_program()
print(prog1.random_seed)
## 102
## the random seed is 102
import paddle
import paddle.static as static

paddle.enable_static()

prog = static.default_main_program()
x = static.data(name="X", shape=[2,3], dtype="float32")
pred = static.nn.fc(x, size=3)
prog_string = prog.to_string(throw_on_error=True, with_details=False)
prog_string_with_details = prog.to_string(throw_on_error=False, with_details=True)
print("program string without detail: {}".format(prog_string))
print("program string with detail: {}".format(prog_string_with_details))
import paddle
import paddle.static as static

paddle.enable_static()

img = static.data(name='image', shape=[None, 784])
pred = static.nn.fc(x=img, size=10, actvation='relu')
loss = paddle.mean(pred)
# Here we use clone before Momentum
test_program = static.default_main_program().clone(for_test=True)
optimizer = paddle.optimizer.Momentum(learning_rate=0.01, momentum=0.9)
optimizer.minimize(loss)
import six

def print_prog(prog):
    for name, value in sorted(six.iteritems(prog.block(0).vars)):
        print(value)
    for op in prog.block(0).ops:
        print("op type is {}".format(op.type))
        print("op inputs are {}".format(op.input_arg_names))
        print("op outputs are {}".format(op.output_arg_names))
        for key, value in sorted(six.iteritems(op.all_attrs())):
            if key not in ['op_callstack', 'op_role_var']:
                print(" [ attrs: {}:   {} ]".format(key, value))
import six
import paddle
import paddle.static as static
import paddle.utils as utils
import paddle.nn.functional as F

paddle.enable_static()

def print_prog(prog):
    for name, value in sorted(six.iteritems(prog.block(0).vars)):
        print(value)
    for op in prog.block(0).ops:
        print("op type is {}".format(op.type))
        print("op inputs are {}".format(op.input_arg_names))
        print("op outputs are {}".format(op.output_arg_names))
        for key, value in sorted(six.iteritems(op.all_attrs())):
            if key not in ['op_callstack', 'op_role_var']:
                print(" [ attrs: {}:   {} ]".format(key, value))

train_program = static.Program()
startup_program = static.Program()

# startup_program is used to do some parameter init work,
# and main program is used to hold the network
with static.program_guard(train_program, startup_program):
    with utils.unique_name.guard():
        img = static.data(name='image', shape=[None, 784])
        hidden = static.nn.fc(x=img, size=200, activation='relu')
        hidden = F.dropout(hidden, p=0.5)
        loss = F.cross_entropy(
            input=static.nn.fc(x=hidden, size=10, activation='softmax'),
            label=static.data(name='label', shape=[1], dtype='int64'))
        avg_loss = paddle.mean(loss)
        test_program = train_program.clone(for_test=True)
print_prog(test_program)

# Due to parameter sharing usage for train and test, so we need to use startup program of train
# instead of using test startup program, while nothing is in test's startup program

# In Paddle we will share weights by using the same Tensor name. In train and test program
# all parameters will have the same name and this can make train and test program sharing parameters,
# that's why we need to use startup program of train. And for startup program of test, it has nothing,
# since it is a new program.

with static.program_guard(train_program, startup_program):
    with utils.unique_name.guard():
        sgd = paddle.optimizer.SGD(learning_rate=1e-3)
        sgd.minimize(avg_loss)
import six
import paddle
import paddle.static as static
import paddle.utils as utils
import paddle.nn.functional as F

paddle.enable_static()

def print_prog(prog):
    for name, value in sorted(six.iteritems(prog.block(0).vars)):
        print(value)
    for op in prog.block(0).ops:
        print("op type is {}".format(op.type))
        print("op inputs are {}".format(op.input_arg_names))
        print("op outputs are {}".format(op.output_arg_names))
        for key, value in sorted(six.iteritems(op.all_attrs())):
            if key not in ['op_callstack', 'op_role_var']:
                print(" [ attrs: {}:   {} ]".format(key, value))

def network():
    img = static.data(name='image', shape=[None, 784])
    hidden = static.nn.fc(x=img, size=200, activation='relu')
    hidden = F.dropout(hidden, p=0.5)
    loss = F.cross_entropy(
        input=static.nn.fc(x=hidden, size=10, activation='softmax'),
        label=static.data(name='label', shape=[1], dtype='int64'))
    avg_loss = paddle.mean(loss)
    return avg_loss

train_program_2 = static.Program()
startup_program_2 = static.Program()
test_program_2 = static.Program()
with static.program_guard(train_program_2, startup_program_2):
    with utils.unique_name.guard():
        avg_loss = network()
        sgd = paddle.optimizer.SGD(learning_rate=1e-3)
        sgd.minimize(avg_loss)
# the test startup program is not used.
with static.program_guard(test_program_2, startup_program_2):
    with utils.unique_name.guard():
        avg_loss = network()
print_prog(test_program_2)
import paddle
import paddle.static as static

paddle.enable_static()

startup_prog = static.Program()
main_prog = static.Program()
with static.program_guard(startup_prog, main_prog):
    x = static.data(name='X', shape=[1000, 784], dtype='float32')

    y = static.data(name='Y', shape=[784, 100], dtype='float32')

    z = paddle.matmul(x=x, y=y)

    binary_str = static.default_main_program().desc.serialize_to_string()
    prog_restored = static.default_main_program().parse_from_string(binary_str)

    print(static.default_main_program())
    print(prog_restored)
import paddle
import paddle.static as static

paddle.enable_static()

prog = static.default_main_program()
num_blocks = prog.num_blocks
print(num_blocks)

# print result:
# 1
import paddle
import paddle.static as static
import paddle.nn.functional as F

paddle.enable_static()

prog = static.default_main_program()
random_seed = prog.random_seed
x_var = static.data(name="X", shape=[3,3], dtype="float32")
print(random_seed)
## 0
## the default random seed is 0

# Here we need to set random seed before we use paddle.nn.functional.dropout
prog.random_seed = 1
z_var = F.dropout(x_var, 0.7)

print(prog.random_seed)
## 1
## the random seed is change to 1
import paddle
import paddle.static as static

paddle.enable_static()

prog = static.default_main_program()
gb_block = prog.global_block()
print(gb_block)
import paddle
import paddle.static as static

paddle.enable_static()

prog = static.default_main_program()
block_0 = prog.block(0)
print(block_0)
import paddle
import paddle.static as static

paddle.enable_static()

prog = static.default_main_program()
current_blk = prog.current_block()
print(current_blk)
import paddle
import paddle.static as static

paddle.enable_static()

prog = static.default_main_program()
img = static.data(name='img', shape=[None, 1,28,28], dtype='float32')
label = static.data(name='label', shape=[None,1], dtype='int64')
for var in prog.list_vars():
    print(var)

# var img : paddle.VarType.LOD_TENSOR.shape(-1, 1, 28, 28).astype(VarType.FP32)
# var label : paddle.VarType.LOD_TENSOR.shape(-1, 1).astype(VarType.INT64)
import paddle
import paddle.static as static

paddle.enable_static()

program = static.default_main_program()
data = static.data(name='x', shape=[None, 13], dtype='float32')
hidden = static.nn.fc(x=data, size=10)
loss = paddle.mean(hidden)
paddle.optimizer.SGD(learning_rate=0.01).minimize(loss)

for param in program.all_parameters():
    print(param)

# Here will print all parameters in current program, in this example,
# the result is like:
#
# persist trainable param fc_0.w_0 : paddle.VarType.LOD_TENSOR.shape(13, 10).astype(VarType.FP32)
# persist trainable param fc_0.b_0 : paddle.VarType.LOD_TENSOR.shape(10,).astype(VarType.FP32)
#
# Here print(param) will print out all the properties of a parameter,
# including name, type and persistable, you can access to specific
# property of a parameter, such as param.name, param.type
import paddle
import paddle.static as static

paddle.enable_static()

x = static.data(name="x", shape=[10, 10], dtype='float32')
y = static.nn.fc(x, 10)
z = static.nn.fc(y, 10)

place = paddle.CPUPlace()
exe = static.Executor(place)
exe.run(static.default_startup_program())
prog = static.default_main_program()

path = "./temp/model.pdparams"
paddle.save(prog.state_dict(), path)
import paddle
import paddle.static as static

paddle.enable_static()

x = static.data(name="x", shape=[10, 10], dtype='float32')
y = static.nn.fc(x, 10)
z = static.nn.fc(y, 10)

place = paddle.CPUPlace()
exe = static.Executor(place)
exe.run(static.default_startup_program())
prog = static.default_main_program()

path = "./temp/model.pdparams"
paddle.save(prog.state_dict(), path)
state_dict_load = paddle.load(path)
prog.set_state_dict(state_dict_load)
import paddle

paddle.enable_static()
main_program = paddle.static.Program()
startup_program = paddle.static.Program()
with paddle.static.program_guard(main_program, startup_program):
    data = paddle.static.data(name='image', shape=[None, 784, 784], dtype='float32')
    hidden = paddle.static.nn.fc(x=data, size=10, activation='relu')
import paddle

paddle.enable_static()
main_program = paddle.static.Program()
# does not care about startup program. Just pass a temporary value.
with paddle.static.program_guard(main_program, paddle.static.Program()):
    data = paddle.static.data(name='image', shape=[None, 784, 784], dtype='float32')
# example 1:
import paddle
import six
import numpy as np

paddle.enable_static()

# Creates a forward function, Tensor can be input directly without
# being converted into numpy array.
def tanh(x):
    return np.tanh(x)

# Skip x in backward function and return the gradient of x
# Tensor must be actively converted to numpy array, otherwise,
# operations such as +/- can't be used.
def tanh_grad(y, dy):
    return np.array(dy) * (1 - np.square(np.array(y)))

# Creates a forward function for debugging running networks(print value)
def debug_func(x):
    print(x)

def create_tmp_var(name, dtype, shape):
    return paddle.static.default_main_program().current_block().create_var(
        name=name, dtype=dtype, shape=shape)

def simple_net(img, label):
    hidden = img
    for idx in six.moves.range(4):
        hidden = paddle.static.nn.fc(hidden, size=200)
        new_hidden = create_tmp_var(name='hidden_{}'.format(idx),
            dtype=hidden.dtype, shape=hidden.shape)

        # User-defined forward and backward
        hidden = paddle.static.py_func(func=tanh, x=hidden,
            out=new_hidden, backward_func=tanh_grad,
            skip_vars_in_backward_input=hidden)

        # User-defined debug functions that print out the input Tensor
        paddle.static.py_func(func=debug_func, x=hidden, out=None)

    prediction = paddle.static.nn.fc(hidden, size=10, activation='softmax')
    ce_loss = paddle.nn.loss.CrossEntropyLoss()
    return ce_loss(prediction, label)

x = paddle.static.data(name='x', shape=[1,4], dtype='float32')
y = paddle.static.data(name='y', shape=[1,10], dtype='int64')
res = simple_net(x, y)

exe = paddle.static.Executor(paddle.CPUPlace())
exe.run(paddle.static.default_startup_program())
input1 = np.random.random(size=[1,4]).astype('float32')
input2 = np.random.randint(1, 10, size=[1,10], dtype='int64')
out = exe.run(paddle.static.default_main_program(),
              feed={'x':input1, 'y':input2},
              fetch_list=[res.name])
print(out)
# example 2:
# This example shows how to turn Tensor into numpy array and
# use numpy API to register an Python OP
import paddle
import numpy as np

paddle.enable_static()

def element_wise_add(x, y):
    # Tensor must be actively converted to numpy array, otherwise,
    # numpy.shape can't be used.
    x = np.array(x)
    y = np.array(y)

    if x.shape != y.shape:
        raise AssertionError("the shape of inputs must be the same!")

    result = np.zeros(x.shape, dtype='int32')
    for i in range(len(x)):
        for j in range(len(x[0])):
            result[i][j] = x[i][j] + y[i][j]

    return result

def create_tmp_var(name, dtype, shape):
    return paddle.static.default_main_program().current_block().create_var(
                name=name, dtype=dtype, shape=shape)

def py_func_demo():
    start_program = paddle.static.default_startup_program()
    main_program = paddle.static.default_main_program()

    # Input of the forward function
    x = paddle.static.data(name='x', shape=[2,3], dtype='int32')
    y = paddle.static.data(name='y', shape=[2,3], dtype='int32')

    # Output of the forward function, name/dtype/shape must be specified
    output = create_tmp_var('output','int32', [3,1])

    # Multiple Variable should be passed in the form of tuple(Variale) or list[Variale]
    paddle.static.py_func(func=element_wise_add, x=[x,y], out=output)

    exe=paddle.static.Executor(paddle.CPUPlace())
    exe.run(start_program)

    # Feed numpy array to main_program
    input1 = np.random.randint(1, 10, size=[2,3], dtype='int32')
    input2 = np.random.randint(1, 10, size=[2,3], dtype='int32')
    out = exe.run(main_program,
                feed={'x':input1, 'y':input2},
                fetch_list=[output.name])
    print("{0} + {1} = {2}".format(input1, input2, out))

py_func_demo()

# Reference output:
# [[5, 9, 9]   + [[7, 8, 4]  =  [array([[12, 17, 13]
#  [7, 5, 2]]     [1, 3, 3]]            [8, 8, 5]], dtype=int32)]
import paddle
import paddle.static as static

paddle.enable_static()

x = static.data(name="x", shape=[10, 10], dtype='float32')
y = static.nn.fc(x, 10)
z = static.nn.fc(y, 10)

place = paddle.CPUPlace()
exe = static.Executor(place)
exe.run(static.default_startup_program())
prog = static.default_main_program()

static.save(prog, "./temp")
import paddle

paddle.enable_static()

path_prefix = "./infer_model"

# User defined network, here a softmax regession example
image = paddle.static.data(name='img', shape=[None, 28, 28], dtype='float32')
label = paddle.static.data(name='label', shape=[None, 1], dtype='int64')
predict = paddle.static.nn.fc(image, 10, activation='softmax')

loss = paddle.nn.functional.cross_entropy(predict, label)

exe = paddle.static.Executor(paddle.CPUPlace())
exe.run(paddle.static.default_startup_program())

# Feed data and train process

# Save inference model. Note we don't save label and loss in this example
paddle.static.save_inference_model(path_prefix, [image], [predict], exe)

# In this example, the save_inference_mode inference will prune the default
# main program according to the network's input node (img) and output node(predict).
# The pruned inference program is going to be saved in file "./infer_model.pdmodel"
# and parameters are going to be saved in file "./infer_model.pdiparams".
import paddle
import numpy
paddle.enable_static()

new_scope = paddle.static.Scope()
with paddle.static.scope_guard(new_scope):
     paddle.static.global_scope().var("data").get_tensor().set(numpy.ones((2, 2)), paddle.CPUPlace())
numpy.array(new_scope.find_var("data").get_tensor())
import paddle

paddle.enable_static()

path_prefix = "./infer_model"

# User defined network, here a softmax regession example
image = paddle.static.data(name='img', shape=[None, 28, 28], dtype='float32')
label = paddle.static.data(name='label', shape=[None, 1], dtype='int64')
predict = paddle.static.nn.fc(image, 10, activation='softmax')

loss = paddle.nn.functional.cross_entropy(predict, label)

exe = paddle.static.Executor(paddle.CPUPlace())
exe.run(paddle.static.default_startup_program())

# serialize parameters to bytes.
serialized_params = paddle.static.serialize_persistables([image], [predict], exe)

# deserialize bytes to parameters.
main_program = paddle.static.default_main_program()
deserialized_params = paddle.static.deserialize_persistables(main_program, serialized_params, exe)
import paddle

paddle.enable_static()

path_prefix = "./infer_model"

# User defined network, here a softmax regession example
image = paddle.static.data(name='img', shape=[None, 28, 28], dtype='float32')
label = paddle.static.data(name='label', shape=[None, 1], dtype='int64')
predict = paddle.static.nn.fc(image, 10, activation='softmax')

loss = paddle.nn.functional.cross_entropy(predict, label)

exe = paddle.static.Executor(paddle.CPUPlace())
exe.run(paddle.static.default_startup_program())

# serialize the default main program to bytes.
serialized_program = paddle.static.serialize_program([image], [predict])

# deserialize bytes to program
deserialized_program = paddle.static.deserialize_program(serialized_program)
import paddle
import paddle.static as static

paddle.enable_static()

x = static.data(name="x", shape=[10, 10], dtype='float32')
y = static.nn.fc(x, 10)
z = static.nn.fc(y, 10)

place = paddle.CPUPlace()
exe = static.Executor(place)
exe.run(static.default_startup_program())
prog = static.default_main_program()

static.save(prog, "./temp")
program_state = static.load_program_state("./temp")

static.set_program_state(prog, program_state)
import paddle.fluid as fluid
cur_program = fluid.Program()
cur_block = cur_program.current_block()
new_variable = cur_block.create_var(name="X",
                                    shape=[-1, 23, 48],
                                    dtype='float32')
import paddle.fluid as fluid
import numpy as np

with fluid.dygraph.guard():
    new_variable = fluid.dygraph.to_variable(np.arange(10))
import paddle

paddle.enable_static()

# create a static Variable
x = paddle.static.data(name='x', shape=[3, 2, 1])

# create a detached Variable
y = x.detach()
import paddle.fluid as fluid
from paddle.fluid.dygraph.base import to_variable
from paddle.fluid.dygraph import Linear
import numpy as np

data = np.random.uniform(-1, 1, [30, 10, 32]).astype('float32')
with fluid.dygraph.guard():
    linear = Linear(32, 64)
    data = to_variable(data)
    x = linear(data)
    print(x.numpy())
import numpy as np
import paddle
paddle.disable_static()

x = np.ones([2, 2], np.float32)
inputs = []
for _ in range(10):
    tmp = paddle.to_tensor(x)
    # if we don't set tmp's stop_gradient as False then, all path to loss will has no gradient since
    # there is no one need gradient on it.
    tmp.stop_gradient=False
    inputs.append(tmp)
ret = paddle.add_n(inputs)
loss = paddle.sum(ret)
loss.backward()
import paddle.fluid as fluid
import numpy as np

# example1: return ndarray
x = np.ones([2, 2], np.float32)
with fluid.dygraph.guard():
    inputs2 = []
    for _ in range(10):
        tmp = fluid.dygraph.base.to_variable(x)
        tmp.stop_gradient=False
        inputs2.append(tmp)
    ret2 = fluid.layers.sums(inputs2)
    loss2 = fluid.layers.reduce_sum(ret2)
    loss2.backward()
    print(loss2.gradient())

# example2: return tuple of ndarray
with fluid.dygraph.guard():
    embedding = fluid.dygraph.Embedding(
        size=[20, 32],
        param_attr='emb.w',
        is_sparse=True)
    x_data = np.arange(12).reshape(4, 3).astype('int64')
    x_data = x_data.reshape((-1, 3, 1))
    x = fluid.dygraph.base.to_variable(x_data)
    out = embedding(x)
    out.backward()
    print(embedding.weight.gradient())
import paddle.fluid as fluid
import numpy as np

x = np.ones([2, 2], np.float32)
with fluid.dygraph.guard():
    inputs2 = []
    for _ in range(10):
        tmp = fluid.dygraph.base.to_variable(x)
        tmp.stop_gradient=False
        inputs2.append(tmp)
    ret2 = fluid.layers.sums(inputs2)
    loss2 = fluid.layers.reduce_sum(ret2)
    loss2.backward()
    print(loss2.gradient())
    loss2.clear_gradient()
    print("After clear {}".format(loss2.gradient()))
import paddle.fluid as fluid
import paddle

paddle.enable_static()
cur_program = fluid.Program()
cur_block = cur_program.current_block()
new_variable = cur_block.create_var(name="X",
                                    shape=[-1, 23, 48],
                                    dtype='float32')
print(new_variable.to_string(True))
print("=============with detail===============")
print(new_variable.to_string(True, True))
import paddle.fluid as fluid
import numpy as np

with fluid.dygraph.guard():
    value0 = np.arange(26).reshape(2, 13).astype("float32")
    value1 = np.arange(6).reshape(2, 3).astype("float32")
    value2 = np.arange(10).reshape(2, 5).astype("float32")
    linear = fluid.Linear(13, 5, dtype="float32")
    linear2 = fluid.Linear(3, 3, dtype="float32")
    a = fluid.dygraph.to_variable(value0)
    b = fluid.dygraph.to_variable(value1)
    c = fluid.dygraph.to_variable(value2)
    out1 = linear(a)
    out2 = linear2(b)
    out1.stop_gradient = True
    out = fluid.layers.concat(input=[out1, out2, c], axis=1)
    out.backward()

    assert linear.weight.gradient() is None
    assert (out1.gradient() == 0).all()
import paddle.fluid as fluid
cur_program = fluid.Program()
cur_block = cur_program.current_block()
new_variable = cur_block.create_var(name="X",
                                    shape=[-1, 23, 48],
                                    dtype='float32')
print("persistable of current Var is: {}".format(new_variable.persistable))
import paddle
new_parameter = paddle.static.create_parameter(name="X",
                                    shape=[10, 23, 48],
                                    dtype='float32')
if new_parameter.is_parameter:
    print("Current var is a Parameter")
else:
    print("Current var is not a Parameter")

# Current var is a Parameter

import paddle.fluid as fluid
cur_program = fluid.Program()
cur_block = cur_program.current_block()
new_variable = cur_block.create_var(name="X",
                                    shape=[-1, 23, 48],
                                    dtype='float32')
print("name of current Var is: {}".format(new_variable.name))
import paddle.fluid as fluid
cur_program = fluid.Program()
cur_block = cur_program.current_block()
new_variable = cur_block.create_var(name="X",
                                    shape=[-1, 23, 48],
                                    dtype='float32')
print("shape of current Var is: {}".format(new_variable.shape))
import paddle.fluid as fluid
cur_program = fluid.Program()
cur_block = cur_program.current_block()
new_variable = cur_block.create_var(name="X",
                                    shape=[-1, 23, 48],
                                    dtype='float32')
print("Dtype of current Var is: {}".format(new_variable.dtype))
import paddle.fluid as fluid
cur_program = fluid.Program()
cur_block = cur_program.current_block()
new_variable = cur_block.create_var(name="X",
                                    shape=[-1, 23, 48],
                                    dtype='float32')
print("LoD Level of current Var is: {}".format(new_variable.lod_level))
import paddle.fluid as fluid
cur_program = fluid.Program()
cur_block = cur_program.current_block()
new_variable = cur_block.create_var(name="X",
                                    shape=[-1, 23, 48],
                                    dtype='float32')
print("Type of current Var is: {}".format(new_variable.type))
import paddle
paddle.enable_static()

x = paddle.ones(shape=[2, 3, 5])
x_T = x.T

exe = paddle.static.Executor()
x_T_np = exe.run(paddle.static.default_main_program(), fetch_list=[x_T])[0]
print(x_T_np.shape)
# (5, 3, 2)
import paddle

paddle.enable_static()

# create a static Variable
x = paddle.static.data(name='x', shape=[3, 2, 1])
# create a cloned Variable
y = x.clone()
import paddle
import paddle.static as static
import numpy as np

paddle.enable_static()

x = static.data(name="x", shape=[10, 10], dtype='float32')

y = static.nn.fc(x, 10, name='fc')
place = paddle.CPUPlace()
exe = static.Executor(place)
prog = paddle.static.default_main_program()
exe.run(static.default_startup_program())
inputs = np.ones((10, 10), dtype='float32')
exe.run(prog, feed={'x': inputs}, fetch_list=[y, ])
path = 'temp/tensor_'
for var in prog.list_vars():
    if var.persistable:
        t = var.get_value()
        paddle.save(t, path+var.name+'.pdtensor')

for var in prog.list_vars():
    if var.persistable:
        t_load = paddle.load(path+var.name+'.pdtensor')
        var.set_value(t_load)
import paddle
import paddle.static as static
import numpy as np

paddle.enable_static()

x = static.data(name="x", shape=[10, 10], dtype='float32')

y = static.nn.fc(x, 10, name='fc')
place = paddle.CPUPlace()
exe = static.Executor(place)
prog = paddle.static.default_main_program()
exe.run(static.default_startup_program())
inputs = np.ones((10, 10), dtype='float32')
exe.run(prog, feed={'x': inputs}, fetch_list=[y, ])
path = 'temp/tensor_'
for var in prog.list_vars():
    if var.persistable:
        t = var.get_value()
        paddle.save(t, path+var.name+'.pdtensor')

for var in prog.list_vars():
    if var.persistable:
        t_load = paddle.load(path+var.name+'.pdtensor')
        var.set_value(t_load)
import paddle

paddle.enable_static()

# create a static Variable
x = paddle.static.data(name='x', shape=[3, 2, 1])

# get the number of elements of the Variable
y = x.size()
import paddle

x = paddle.to_tensor([-0.4, -0.2, 0.1, 0.3])
out = paddle.abs(x)
print(out)
# [0.4 0.2 0.1 0.3]
import paddle

x = paddle.to_tensor([-0.4, -0.2, 0.1, 0.3])
out = paddle.acos(x)
print(out)
# [1.98231317 1.77215425 1.47062891 1.26610367]
import paddle
x = paddle.to_tensor([2, 3, 4], 'float64')
y = paddle.to_tensor([1, 5, 2], 'float64')
z = paddle.add(x, y)
print(z)  # [3., 8., 6. ]
import paddle

input0 = paddle.to_tensor([[1, 2, 3], [4, 5, 6]], dtype='float32')
input1 = paddle.to_tensor([[7, 8, 9], [10, 11, 12]], dtype='float32')
output = paddle.add_n([input0, input1])
# [[8., 10., 12.],
#  [14., 16., 18.]]
import paddle

x = paddle.ones([2,2])
y = paddle.ones([2,2])
input = paddle.ones([2,2])

out = paddle.addmm( input=input, x=x, y=y, beta=0.5, alpha=5.0 )

print(out)
# [[10.5 10.5]
# [10.5 10.5]]
import paddle
import numpy as np

# x is a bool Tensor with following elements:
#    [[True, False]
#     [True, True]]
x = paddle.assign(np.array([[1, 0], [1, 1]], dtype='int32'))
print(x)
x = paddle.cast(x, 'bool')

# out1 should be [False]
out1 = paddle.all(x)  # [False]
print(out1)

# out2 should be [True, False]
out2 = paddle.all(x, axis=0)  # [True, False]
print(out2)

# keep_dim=False, out3 should be [False, True], out.shape should be (2,)
out3 = paddle.all(x, axis=-1)  # [False, True]
print(out3)

# keep_dim=True, out4 should be [[False], [True]], out.shape should be (2,1)
out4 = paddle.all(x, axis=1, keepdim=True)
out4 = paddle.cast(out4, 'int32')  # [[False], [True]]
print(out4)
import paddle

x = paddle.to_tensor([10000., 1e-07])
y = paddle.to_tensor([10000.1, 1e-08])
result1 = paddle.allclose(x, y, rtol=1e-05, atol=1e-08,
                        equal_nan=False, name="ignore_nan")
np_result1 = result1.numpy()
# [False]
result2 = paddle.allclose(x, y, rtol=1e-05, atol=1e-08,
                            equal_nan=True, name="equal_nan")
np_result2 = result2.numpy()
# [False]

x = paddle.to_tensor([1.0, float('nan')])
y = paddle.to_tensor([1.0, float('nan')])
result1 = paddle.allclose(x, y, rtol=1e-05, atol=1e-08,
                        equal_nan=False, name="ignore_nan")
np_result1 = result1.numpy()
# [False]
result2 = paddle.allclose(x, y, rtol=1e-05, atol=1e-08,
                            equal_nan=True, name="equal_nan")
np_result2 = result2.numpy()
# [True]
import paddle
import numpy as np

# x is a bool Tensor with following elements:
#    [[True, False]
#     [False, False]]
x = paddle.assign(np.array([[1, 0], [1, 1]], dtype='int32'))
print(x)
x = paddle.cast(x, 'bool')

# out1 should be [True]
out1 = paddle.any(x)  # [True]
print(out1)

# out2 should be [True, True]
out2 = paddle.any(x, axis=0)  # [True, True]
print(out2)

# keep_dim=False, out3 should be [True, True], out.shape should be (2,)
out3 = paddle.any(x, axis=-1)  # [True, True]
print(out3)

# keep_dim=True, result should be [[True], [True]], out.shape should be (2,1)
out4 = paddle.any(x, axis=1, keepdim=True)
out4 = paddle.cast(out4, 'int32')  # [[True], [True]]
print(out4)
import paddle

x =  paddle.to_tensor([[5,8,9,5],
                         [0,0,1,7],
                         [6,9,2,4]])
out1 = paddle.argmax(x)
print(out1) # 2
out2 = paddle.argmax(x, axis=1)
print(out2)
# [2 3 1]
out3 = paddle.argmax(x, axis=-1)
print(out3)
# [2 3 1]
import paddle

x =  paddle.to_tensor([[5,8,9,5],
                         [0,0,1,7],
                         [6,9,2,4]])
out1 = paddle.argmin(x)
print(out1) # 4
out2 = paddle.argmin(x, axis=1)
print(out2)
# [0 0 2]
out3 = paddle.argmin(x, axis=-1)
print(out3)
# [0 0 2]
import paddle

x = paddle.to_tensor([[[5,8,9,5],
                       [0,0,1,7],
                       [6,9,2,4]],
                      [[5,2,4,2],
                       [4,7,7,9],
                       [1,7,0,6]]],
                    dtype='float32')
out1 = paddle.argsort(x=x, axis=-1)
out2 = paddle.argsort(x=x, axis=0)
out3 = paddle.argsort(x=x, axis=1)
print(out1)
#[[[0 3 1 2]
#  [0 1 2 3]
#  [2 3 0 1]]
# [[1 3 2 0]
#  [0 1 2 3]
#  [2 0 3 1]]]
print(out2)
#[[[0 1 1 1]
#  [0 0 0 0]
#  [1 1 1 0]]
# [[1 0 0 0]
#  [1 1 1 1]
#  [0 0 0 1]]]
print(out3)
#[[[1 1 1 2]
#  [0 0 2 0]
#  [2 2 0 1]]
# [[2 0 2 0]
#  [1 1 0 2]
#  [0 2 1 1]]]
import paddle

x = paddle.to_tensor([-0.4, -0.2, 0.1, 0.3])
out = paddle.asin(x)
print(out)
# [-0.41151685 -0.20135792  0.10016742  0.30469265]
import paddle.fluid as fluid

startup_prog = fluid.Program()
main_prog = fluid.Program()
with fluid.program_guard(startup_prog, main_prog):
    original_variable = fluid.data(name = "new_variable", shape=[2,2], dtype='float32')
    new_variable = original_variable.astype('int64')
    print("new var's dtype is: {}".format(new_variable.dtype))
import paddle.fluid as fluid
import numpy as np

x = np.ones([2, 2], np.float32)
with fluid.dygraph.guard():
    original_variable = fluid.dygraph.to_variable(x)
    print("original var's dtype is: {}, numpy dtype is {}".format(original_variable.dtype, original_variable.numpy().dtype))
    new_variable = original_variable.astype('int64')
    print("new var's dtype is: {}, numpy dtype is {}".format(new_variable.dtype, new_variable.numpy().dtype))
import paddle

x = paddle.to_tensor([-0.4, -0.2, 0.1, 0.3])
out = paddle.atan(x)
print(out)
# [-0.38050638 -0.19739556  0.09966865  0.29145679]
import paddle

x = paddle.to_tensor([1, 2, 1, 4, 5])
result1 = paddle.bincount(x)
print(result1) # [0, 2, 1, 0, 1, 1]

w = paddle.to_tensor([2.1, 0.4, 0.1, 0.5, 0.5])
result2 = paddle.bincount(x, weights=w)
print(result2) # [0., 2.19999981, 0.40000001, 0., 0.50000000, 0.50000000]
import paddle
x = paddle.to_tensor([-5, -1, 1])
y = paddle.to_tensor([4,  2, -3])
res = paddle.bitwise_and(x, y)
print(res)  # [0, 2, 1]
import paddle
x = paddle.to_tensor([-5, -1, 1])
res = paddle.bitwise_not(x)
print(res) # [4, 0, -2]
import paddle
x = paddle.to_tensor([-5, -1, 1])
y = paddle.to_tensor([4,  2, -3])
res = paddle.bitwise_or(x, y)
print(res)  # [-1, -1, -3]
import paddle
x = paddle.to_tensor([-5, -1, 1])
y = paddle.to_tensor([4,  2, -3])
res = paddle.bitwise_xor(x, y)
print(res) # [-1, -3, -4]
import paddle

# In imperative mode:
# size x: (2, 2, 3) and y: (2, 3, 2)
x = paddle.to_tensor([[[1.0, 1.0, 1.0],
                    [2.0, 2.0, 2.0]],
                    [[3.0, 3.0, 3.0],
                    [4.0, 4.0, 4.0]]])
y = paddle.to_tensor([[[1.0, 1.0],[2.0, 2.0],[3.0, 3.0]],
                    [[4.0, 4.0],[5.0, 5.0],[6.0, 6.0]]])
out = paddle.bmm(x, y)
#output size: (2, 2, 2)
#output value:
#[[[6.0, 6.0],[12.0, 12.0]],[[45.0, 45.0],[60.0, 60.0]]]
out_np = out.numpy()
import paddle

shape = paddle.broadcast_shape([2, 1, 3], [1, 3, 1])
# [2, 3, 3]

# shape = paddle.broadcast_shape([2, 1, 3], [3, 3, 1])
# ValueError (terminated with error message).
import paddle
x1 = paddle.rand([1, 2, 3, 4]).astype('float32')
x2 = paddle.rand([1, 2, 1, 4]).astype('float32')
x3 = paddle.rand([1, 1, 3, 1]).astype('float32')
out1, out2, out3 = paddle.broadcast_tensors(input=[x1, x2, x3])
# out1, out2, out3: tensors broadcasted from x1, x2, x3 with shape [1,2,3,4]
import paddle

data = paddle.to_tensor([1, 2, 3], dtype='int32')
out = paddle.broadcast_to(data, shape=[2, 3])
print(out)
# [[1, 2, 3], [1, 2, 3]]
import paddle

x = paddle.to_tensor([2, 3, 4], 'float64')
y = paddle.cast(x, 'uint8')
import paddle

x = paddle.to_tensor([-0.4, -0.2, 0.1, 0.3])
out = paddle.ceil(x)
print(out)
# [-0. -0.  1.  1.]
import paddle
import numpy as np

a = np.random.rand(3, 3)
a_t = np.transpose(a, [1, 0])
x_data = np.matmul(a, a_t) + 1e-03
x = paddle.to_tensor(x_data)
out = paddle.cholesky(x, upper=False)
print(out)
# [[1.190523   0.         0.        ]
#  [0.9906703  0.27676893 0.        ]
#  [1.25450498 0.05600871 0.06400121]]
import numpy as np
import paddle

# x is a Tensor which shape is [3, 9, 5]
x_np = np.random.random([3, 9, 5]).astype("int32")
x = paddle.to_tensor(x_np)

out0, out1, out2 = paddle.chunk(x, chunks=3, axis=1)
# out0.shape [3, 3, 5]
# out1.shape [3, 3, 5]
# out2.shape [3, 3, 5]


# axis is negative, the real axis is (rank(x) + axis) which real
# value is 1.
out0, out1, out2 = paddle.chunk(x, chunks=3, axis=-2)
# out0.shape [3, 3, 5]
# out1.shape [3, 3, 5]
# out2.shape [3, 3, 5]
import paddle

x1 = paddle.to_tensor([[1.2, 3.5], [4.5, 6.4]], 'float32')
out1 = paddle.clip(x1, min=3.5, max=5.0)
out2 = paddle.clip(x1, min=2.5)
print(out1)
# [[3.5, 3.5]
# [4.5, 5.0]]
print(out2)
# [[2.5, 3.5]
# [[4.5, 6.4]
import paddle

x1 = paddle.to_tensor([[1, 2, 3],
                       [4, 5, 6]])
x2 = paddle.to_tensor([[11, 12, 13],
                       [14, 15, 16]])
x3 = paddle.to_tensor([[21, 22],
                       [23, 24]])
zero = paddle.full(shape=[1], dtype='int32', fill_value=0)
# When the axis is negative, the real axis is (axis + Rank(x))
# As follow, axis is -1, Rank(x) is 2, the real axis is 1
out1 = paddle.concat(x=[x1, x2, x3], axis=-1)
out2 = paddle.concat(x=[x1, x2], axis=0)
out3 = paddle.concat(x=[x1, x2], axis=zero)
# out1
# [[ 1  2  3 11 12 13 21 22]
#  [ 4  5  6 14 15 16 23 24]]
# out2 out3
# [[ 1  2  3]
#  [ 4  5  6]
#  [11 12 13]
#  [14 15 16]]
import paddle
import numpy as np

x = paddle.to_tensor([[1., 0, -1], [0, 1, 0], [1, 0, 1]])

# compute conditional number when p is None
out = paddle.linalg.cond(x)
# out.numpy() [1.4142135]

# compute conditional number when order of the norm is 'fro'
out_fro = paddle.linalg.cond(x, p='fro')
# out_fro.numpy() [3.1622777]

# compute conditional number when order of the norm is 'nuc'
out_nuc = paddle.linalg.cond(x, p='nuc')
# out_nuc.numpy() [9.2426405]

# compute conditional number when order of the norm is 1
out_1 = paddle.linalg.cond(x, p=1)
# out_1.numpy() [2.]

# compute conditional number when order of the norm is -1
out_minus_1 = paddle.linalg.cond(x, p=-1)
# out_minus_1.numpy() [1.]

# compute conditional number when order of the norm is 2
out_2 = paddle.linalg.cond(x, p=2)
# out_2.numpy() [1.4142135]

# compute conditional number when order of the norm is -1
out_minus_2 = paddle.linalg.cond(x, p=-2)
# out_minus_2.numpy() [0.70710677]

# compute conditional number when order of the norm is inf
out_inf = paddle.linalg.cond(x, p=np.inf)
# out_inf.numpy() [2.]

# compute conditional number when order of the norm is -inf
out_minus_inf = paddle.linalg.cond(x, p=-np.inf)
# out_minus_inf.numpy() [1.]

a = paddle.to_tensor(np.random.randn(2, 4, 4).astype('float32'))
# a.numpy()
# [[[ 0.14063153 -0.996288    0.7996131  -0.02571543]
#   [-0.16303636  1.5534962  -0.49919784 -0.04402903]
#   [-1.1341571  -0.6022629   0.5445269   0.29154757]
#   [-0.16816919 -0.30972657  1.7521842  -0.5402487 ]]
#  [[-0.58081484  0.12402827  0.7229862  -0.55046535]
#   [-0.15178485 -1.1604939   0.75810957  0.30971205]
#   [-0.9669573   1.0940945  -0.27363303 -0.35416734]
#   [-1.216529    2.0018666  -0.7773689  -0.17556527]]]
a_cond_fro = paddle.linalg.cond(a, p='fro')
# a_cond_fro.numpy()  [31.572273 28.120834]

b = paddle.to_tensor(np.random.randn(2, 3, 4).astype('float64'))
# b.numpy()
# [[[ 1.61707487  0.46829144  0.38130416  0.82546736]
#   [-1.72710298  0.08866375 -0.62518804  0.16128892]
#   [-0.02822879 -1.67764516  0.11141444  0.3220113 ]]
#  [[ 0.22524372  0.62474921 -0.85503233 -1.03960523]
#   [-0.76620689  0.56673047  0.85064753 -0.45158196]
#   [ 1.47595418  2.23646462  1.5701758   0.10497519]]]
b_cond_2 = paddle.linalg.cond(b, p=2)
# b_cond_2.numpy()  [3.30064451 2.51976252]
import paddle
data=paddle.to_tensor([[1+1j, 2+2j, 3+3j], [4+4j, 5+5j, 6+6j]])
#Tensor(shape=[2, 3], dtype=complex64, place=CUDAPlace(0), stop_gradient=True,
#       [[(1+1j), (2+2j), (3+3j)],
#        [(4+4j), (5+5j), (6+6j)]])

conj_data=paddle.conj(data)
#Tensor(shape=[2, 3], dtype=complex64, place=CUDAPlace(0), stop_gradient=True,
#       [[(1-1j), (2-2j), (3-3j)],
#        [(4-4j), (5-5j), (6-6j)]])
import paddle

x = paddle.to_tensor([-0.4, -0.2, 0.1, 0.3])
out = paddle.cos(x)
print(out)
# [0.92106099 0.98006658 0.99500417 0.95533649]
import paddle

x = paddle.to_tensor([-0.4, -0.2, 0.1, 0.3])
out = paddle.cosh(x)
print(out)
# [1.08107237 1.02006676 1.00500417 1.04533851]
import paddle

x = paddle.to_tensor([[1.0, 1.0, 1.0],
                      [2.0, 2.0, 2.0],
                      [3.0, 3.0, 3.0]])
y = paddle.to_tensor([[1.0, 1.0, 1.0],
                      [1.0, 1.0, 1.0],
                      [1.0, 1.0, 1.0]])

z1 = paddle.cross(x, y)
# [[-1. -1. -1.]
#  [ 2.  2.  2.]
#  [-1. -1. -1.]]

z2 = paddle.cross(x, y, axis=1)
# [[0. 0. 0.]
#  [0. 0. 0.]
#  [0. 0. 0.]]
import paddle

data = paddle.arange(12)
data = paddle.reshape(data, (3, 4))
# [[ 0  1  2  3 ]
#  [ 4  5  6  7 ]
#  [ 8  9  10 11]]

y = paddle.cumprod(data, dim=0)
# [[ 0  1   2   3]
#  [ 0  5  12  21]
#  [ 0 45 120 231]]

y = paddle.cumprod(data, dim=-1)
# [[ 0   0   0    0]
#  [ 4  20 120  840]
#  [ 8  72 720 7920]]

y = paddle.cumprod(data, dim=1, dtype='float64')
# [[ 0.   0.   0.    0.]
#  [ 4.  20. 120.  840.]
#  [ 8.  72. 720. 7920.]]

print(y.dtype)
# paddle.float64
import paddle

data = paddle.arange(12)
data = paddle.reshape(data, (3, 4))

y = paddle.cumsum(data)
# [ 0  1  3  6 10 15 21 28 36 45 55 66]

y = paddle.cumsum(data, axis=0)
# [[ 0  1  2  3]
#  [ 4  6  8 10]
#  [12 15 18 21]]

y = paddle.cumsum(data, axis=-1)
# [[ 0  1  3  6]
#  [ 4  9 15 22]
#  [ 8 17 27 38]]

y = paddle.cumsum(data, dtype='float64')
print(y.dtype)
# VarType.FP64
import paddle

x = paddle.rand([2,2,3],'float32')
print(x)
# Tensor(shape=[2, 2, 3], dtype=float32, place=CUDAPlace(0), stop_gradient=True,
#        [[[0.45661032, 0.03751532, 0.90191704],
#          [0.43760979, 0.86177313, 0.65221709]],

#         [[0.17020577, 0.00259554, 0.28954273],
#          [0.51795638, 0.27325270, 0.18117726]]])

out1 = paddle.diagonal(x)
print(out1)
#Tensor(shape=[3, 2], dtype=float32, place=CUDAPlace(0), stop_gradient=True,
#       [[0.45661032, 0.51795638],
#        [0.03751532, 0.27325270],
#        [0.90191704, 0.18117726]])

out2 = paddle.diagonal(x, offset=0, axis1=2, axis2=1)
print(out2)
#Tensor(shape=[2, 2], dtype=float32, place=CUDAPlace(0), stop_gradient=True,
#       [[0.45661032, 0.86177313],
#        [0.17020577, 0.27325270]])

out3 = paddle.diagonal(x, offset=1, axis1=0, axis2=1)
print(out3)
#Tensor(shape=[3, 1], dtype=float32, place=CUDAPlace(0), stop_gradient=True,
#       [[0.43760979],
#        [0.86177313],
#        [0.65221709]])

out4 = paddle.diagonal(x, offset=0, axis1=1, axis2=2)
print(out4)
#Tensor(shape=[2, 2], dtype=float32, place=CUDAPlace(0), stop_gradient=True,
#       [[0.45661032, 0.86177313],
#        [0.17020577, 0.27325270]])
import paddle

data = paddle.to_tensor([[1, 1.5], [0, -2.2]], dtype='float32')
res = paddle.digamma(data)
print(res)
# Tensor(shape=[2, 2], dtype=float32, place=CUDAPlace(0), stop_gradient=True,
#       [[-0.57721591,  0.03648996],
#        [ nan       ,  5.32286835]])
import paddle
import numpy as np

x = paddle.to_tensor(np.array([[3, 3],[3, 3]]), "float32")
y = paddle.to_tensor(np.array([[3, 3],[3, 1]]), "float32")
out = paddle.dist(x, y, 0)
print(out) # out = [1.]

out = paddle.dist(x, y, 2)
print(out) # out = [2.]

out = paddle.dist(x, y, float("inf"))
print(out) # out = [2.]

out = paddle.dist(x, y, float("-inf"))
print(out) # out = [0.]
import paddle

x = paddle.to_tensor([2, 3, 4], dtype='float64')
y = paddle.to_tensor([1, 5, 2], dtype='float64')
z = paddle.divide(x, y)
print(z)  # [2., 0.6, 2.]
import paddle
import numpy as np

x_data = np.random.uniform(0.1, 1, [10]).astype(np.float32)
y_data = np.random.uniform(1, 3, [10]).astype(np.float32)
x = paddle.to_tensor(x_data)
y = paddle.to_tensor(y_data)
z = paddle.dot(x, y)
print(z)
import paddle
import numpy as np

paddle.device.set_device("cpu")

x_data = np.array([[1.6707249, 7.2249975, 6.5045543],
                   [9.956216,  8.749598,  6.066444 ],
                   [4.4251957, 1.7983172, 0.370647 ]]).astype("float32")
x = paddle.to_tensor(x_data)
w, v = paddle.linalg.eig(x)
print(w)
# Tensor(shape=[3, 3], dtype=complex128, place=CPUPlace, stop_gradient=False,
#       [[(-0.5061363550800655+0j) , (-0.7971760990842826+0j) ,
#         (0.18518077798279986+0j)],
#        [(-0.8308237755993192+0j) ,  (0.3463813401919749+0j) ,
#         (-0.6837005269141947+0j) ],
#        [(-0.23142567697893396+0j),  (0.4944999840400175+0j) ,
#         (0.7058765252952796+0j) ]])

print(v)
# Tensor(shape=[3], dtype=complex128, place=CPUPlace, stop_gradient=False,
#       [ (16.50471283351188+0j)  , (-5.5034820550763515+0j) ,
#         (-0.21026087843552282+0j)])
import paddle

paddle.set_device("cpu")
paddle.seed(1234)

x = paddle.rand(shape=[3, 3], dtype='float64')
# [[0.02773777, 0.93004224, 0.06911496],
#  [0.24831591, 0.45733623, 0.07717843],
#  [0.48016702, 0.14235102, 0.42620817]])

print(paddle.linalg.eigvals(x))
# [(-0.27078833542132674+0j), (0.29962280156230725+0j), (0.8824477020120244+0j)] #complex128
import numpy as np
import paddle

x_data = np.array([[1, -2j], [2j, 5]])
x = paddle.to_tensor(x_data)
out_value = paddle.eigvalsh(x, UPLO='L')
print(out_value)
#[0.17157288, 5.82842712]
import paddle

x = paddle.to_tensor([1, 2, 3])
y = paddle.to_tensor([1, 3, 2])
result1 = paddle.equal(x, y)
print(result1)  # result1 = [True False False]
import paddle

x = paddle.to_tensor([1, 2, 3])
y = paddle.to_tensor([1, 2, 3])
z = paddle.to_tensor([1, 4, 3])
result1 = paddle.equal_all(x, y)
print(result1) # result1 = [True ]
result2 = paddle.equal_all(x, z)
print(result2) # result2 = [False ]
import paddle
x = paddle.to_tensor([-0.4, -0.2, 0.1, 0.3])
out = paddle.erf(x)
print(out)
# [-0.42839236 -0.22270259  0.11246292  0.32862676]
import paddle

x = paddle.to_tensor([-0.4, -0.2, 0.1, 0.3])
out = paddle.exp(x)
print(out)
# [0.67032005 0.81873075 1.10517092 1.34985881]
import paddle

data = paddle.to_tensor([1, 2, 3], dtype='int32')
out = paddle.expand(data, shape=[2, 3])
print(out)
# [[1, 2, 3], [1, 2, 3]]
import paddle

data_x = paddle.to_tensor([1, 2, 3], 'int32')
data_y = paddle.to_tensor([[1, 2, 3], [4, 5, 6]], 'int32')
out = paddle.expand_as(data_x, data_y)
np_out = out.numpy()
# [[1, 2, 3], [1, 2, 3]]
import paddle

image_shape=(2, 3, 4, 4)

x = paddle.arange(end=image_shape[0] * image_shape[1] * image_shape[2] * image_shape[3])
img = paddle.reshape(x, image_shape)

out = paddle.flatten(img, start_axis=1, stop_axis=2)
# out shape is [2, 12, 4]

# out shares data with img in dygraph mode
img[0, 0, 0, 0] = -1
print(out[0, 0, 0]) # [-1]
import paddle
import numpy as np

image_shape=(3, 2, 2)
x = np.arange(image_shape[0] * image_shape[1] * image_shape[2]).reshape(image_shape)
x = x.astype('float32')
img = paddle.to_tensor(x)
tmp = paddle.flip(img, [0,1])
print(tmp) # [[[10,11],[8, 9]], [[6, 7],[4, 5]], [[2, 3],[0, 1]]]

out = paddle.flip(tmp,-1)
print(out) # [[[11,10],[9, 8]], [[7, 6],[5, 4]], [[3, 2],[1, 0]]]
import paddle

x = paddle.to_tensor([-0.4, -0.2, 0.1, 0.3])
out = paddle.floor(x)
print(out)
# [-1. -1.  0.  0.]
import paddle

x = paddle.to_tensor([2, 3, 8, 7])
y = paddle.to_tensor([1, 5, 3, 3])
z = paddle.floor_divide(x, y)
print(z)  # [2, 0, 2, 2]
import paddle

x = paddle.to_tensor([2, 3, 8, 7])
y = paddle.to_tensor([1, 5, 3, 3])
z = paddle.remainder(x, y)
print(z)  # [0, 3, 2, 1]
import paddle

input = paddle.to_tensor([[1,2],[3,4],[5,6]])
index = paddle.to_tensor([0,1])
output = paddle.gather(input, index, axis=0)
# expected output: [[1,2],[3,4]]
import paddle

x = paddle.to_tensor([[[1, 2], [3, 4], [5, 6]],
                      [[7, 8], [9, 10], [11, 12]]])
index = paddle.to_tensor([[0, 1]])

output = paddle.gather_nd(x, index) #[[3, 4]]
import paddle

x = paddle.to_tensor([1, 2, 3])
y = paddle.to_tensor([1, 3, 2])
result1 = paddle.greater_equal(x, y)
print(result1)  # result1 = [True False True]
import paddle

x = paddle.to_tensor([1, 2, 3])
y = paddle.to_tensor([1, 3, 2])
result1 = paddle.greater_than(x, y)
print(result1)  # result1 = [False False True]
import paddle

inputs = paddle.to_tensor([1, 2, 1])
result = paddle.histogram(inputs, bins=4, min=0, max=3)
print(result) # [0, 2, 1, 0]
import paddle

x = paddle.to_tensor(
    [[1 + 6j, 2 + 5j, 3 + 4j], [4 + 3j, 5 + 2j, 6 + 1j]])
# Tensor(shape=[2, 3], dtype=complex64, place=CUDAPlace(0), stop_gradient=True,
#        [[(1+6j), (2+5j), (3+4j)],
#         [(4+3j), (5+2j), (6+1j)]])

imag_res = paddle.imag(x)
# Tensor(shape=[2, 3], dtype=float32, place=CUDAPlace(0), stop_gradient=True,
#        [[6., 5., 4.],
#         [3., 2., 1.]])

imag_t = x.imag()
# Tensor(shape=[2, 3], dtype=float32, place=CUDAPlace(0), stop_gradient=True,
#        [[6., 5., 4.],
#         [3., 2., 1.]])
import paddle

data = paddle.zeros(shape=[1], dtype='float32')
counter = paddle.increment(data)
# [1.]
import paddle

x = paddle.to_tensor([[1.0, 2.0, 3.0, 4.0],
                      [5.0, 6.0, 7.0, 8.0],
                      [9.0, 10.0, 11.0, 12.0]], dtype='float32')
index = paddle.to_tensor([[0, 1, 2],
                          [1, 2, 3],
                          [0, 0, 0]], dtype='int32')
target = paddle.to_tensor([[100, 200, 300, 400],
                           [500, 600, 700, 800],
                           [900, 1000, 1100, 1200]], dtype='int32')
out_z1 = paddle.index_sample(x, index)
print(out_z1)
#[[1. 2. 3.]
# [6. 7. 8.]
# [9. 9. 9.]]

# Use the index of the maximum value by topk op
# get the value of the element of the corresponding index in other tensors
top_value, top_index = paddle.topk(x, k=2)
out_z2 = paddle.index_sample(target, top_index)
print(top_value)
#[[ 4.  3.]
# [ 8.  7.]
# [12. 11.]]

print(top_index)
#[[3 2]
# [3 2]
# [3 2]]

print(out_z2)
#[[ 400  300]
# [ 800  700]
# [1200 1100]]
import paddle

x = paddle.to_tensor([[1.0, 2.0, 3.0, 4.0],
                      [5.0, 6.0, 7.0, 8.0],
                      [9.0, 10.0, 11.0, 12.0]])
index = paddle.to_tensor([0, 1, 1], dtype='int32')
out_z1 = paddle.index_select(x=x, index=index)
#[[1. 2. 3. 4.]
# [5. 6. 7. 8.]
# [5. 6. 7. 8.]]
out_z2 = paddle.index_select(x=x, index=index, axis=1)
#[[ 1.  2.  2.]
# [ 5.  6.  6.]
# [ 9. 10. 10.]]
import paddle

mat = paddle.to_tensor([[2, 0], [0, 2]], dtype='float32')
inv = paddle.inverse(mat)
print(inv) # [[0.5, 0], [0, 0.5]]
import paddle

input = paddle.rand(shape=[4, 32, 32], dtype='float32')
res = paddle.is_empty(x=input)
print("res:", res)
# ('res:', Tensor: eager_tmp_1
#    - place: CPUPlace
#    - shape: [1]
#    - layout: NCHW
#    - dtype: bool
#    - data: [0])
import paddle

input1 = paddle.rand(shape=[2, 3, 5], dtype='float32')
check = paddle.is_tensor(input1)
print(check)  #True

input3 = [1, 4]
check = paddle.is_tensor(input3)
print(check)  #False
import paddle

x = paddle.to_tensor([float('-inf'), -2, 3.6, float('inf'), 0, float('-nan'), float('nan')])
out = paddle.tensor.isfinite(x)
print(out)  # [False  True  True False  True False False]
import paddle
x = paddle.to_tensor([float('-inf'), -2, 3.6, float('inf'), 0, float('-nan'), float('nan')])
out = paddle.tensor.isinf(x)
print(out)  # [ True False False  True False False False]
import paddle
x = paddle.to_tensor([float('-inf'), -2, 3.6, float('inf'), 0, float('-nan'), float('nan')])
out = paddle.tensor.isnan(x)
print(out)  # [False False False False False  True  True]
import paddle
x = paddle.to_tensor([[1, 2], [3, 4]], dtype='int64')
y = paddle.to_tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9]], dtype='int64')
out = paddle.kron(x, y)
print(out)
#        [[1, 2, 3, 2, 4, 6],
#         [ 4,  5,  6,  8, 10, 12],
#         [ 7,  8,  9, 14, 16, 18],
#         [ 3,  6,  9,  4,  8, 12],
#         [12, 15, 18, 16, 20, 24],
#         [21, 24, 27, 28, 32, 36]])
import paddle

x = paddle.to_tensor([1, 2, 3])
y = paddle.to_tensor([1, 3, 2])
result1 = paddle.less_equal(x, y)
print(result1)  # result1 = [True True False]
import paddle

x = paddle.to_tensor([1, 2, 3])
y = paddle.to_tensor([1, 3, 2])
result1 = paddle.less_than(x, y)
print(result1)  # result1 = [False True False]
import paddle

x = paddle.to_tensor([-0.4, -0.2, 0.1, 0.3])
out = paddle.lgamma(x)
print(out)
# [1.31452441, 1.76149750, 2.25271273, 1.09579802]
import paddle

x = [[2,3,4], [7,8,9]]
x = paddle.to_tensor(x, dtype='float32')
res = paddle.log(x)
# [[0.693147, 1.09861, 1.38629], [1.94591, 2.07944, 2.19722]]
import paddle

# example 1: x is a float
x_i = paddle.to_tensor([[1.0], [10.0]])
res = paddle.log10(x_i) # [[0.], [1.0]]

# example 2: x is float32
x_i = paddle.full(shape=[1], fill_value=10, dtype='float32')
paddle.to_tensor(x_i)
res = paddle.log10(x_i)
print(res) # [1.0]

# example 3: x is float64
x_i = paddle.full(shape=[1], fill_value=10, dtype='float64')
paddle.to_tensor(x_i)
res = paddle.log10(x_i)
print(res) # [1.0]
import paddle

data = paddle.to_tensor([[0], [1]], dtype='float32')
res = paddle.log1p(data)
# [[0.], [0.6931472]]
import paddle

# example 1: x is a float
x_i = paddle.to_tensor([[1.0], [2.0]])
res = paddle.log2(x_i) # [[0.], [1.0]]

# example 2: x is float32
x_i = paddle.full(shape=[1], fill_value=2, dtype='float32')
paddle.to_tensor(x_i)
res = paddle.log2(x_i)
print(res) # [1.0]

# example 3: x is float64
x_i = paddle.full(shape=[1], fill_value=2, dtype='float64')
paddle.to_tensor(x_i)
res = paddle.log2(x_i)
print(res) # [1.0]
import paddle

x = paddle.to_tensor([True])
y = paddle.to_tensor([True, False, True, False])
res = paddle.logical_and(x, y)
print(res) # [True False True False]
import paddle

x = paddle.to_tensor([True, False, True, False])
res = paddle.logical_not(x)
print(res) # [False  True False  True]
import paddle
import numpy as np

x_data = np.array([True, False], dtype=np.bool).reshape(2, 1)
y_data = np.array([True, False, True, False], dtype=np.bool).reshape(2, 2)
x = paddle.to_tensor(x_data)
y = paddle.to_tensor(y_data)
res = paddle.logical_or(x, y)
print(res) # [[ True  True] [ True False]]
import paddle
import numpy as np

x_data = np.array([True, False], dtype=np.bool).reshape([2, 1])
y_data = np.array([True, False, True, False], dtype=np.bool).reshape([2, 2])
x = paddle.to_tensor(x_data)
y = paddle.to_tensor(y_data)
res = paddle.logical_xor(x, y)
print(res) # [[False,  True], [ True, False]]
import paddle

x = paddle.to_tensor([[-1.5, 0., 2.], [3., 1.2, -2.4]])
out1 = paddle.logsumexp(x) # [3.4691226]
out2 = paddle.logsumexp(x, 1) # [2.15317821, 3.15684602]
import paddle

x = paddle.to_tensor([[1.0, 2.0, 3.0, 4.0],
                      [5.0, 6.0, 7.0, 8.0],
                      [9.0, 10.0, 11.0, 12.0]])
mask = paddle.to_tensor([[True, False, False, False],
                         [True, True, False, False],
                         [True, False, False, False]])
out = paddle.masked_select(x, mask)
#[1.0 5.0 6.0 9.0]
import paddle
import numpy as np

# vector * vector
x_data = np.random.random([10]).astype(np.float32)
y_data = np.random.random([10]).astype(np.float32)
x = paddle.to_tensor(x_data)
y = paddle.to_tensor(y_data)
z = paddle.matmul(x, y)
print(z.numpy().shape)
# [1]

# matrix * vector
x_data = np.random.random([10, 5]).astype(np.float32)
y_data = np.random.random([5]).astype(np.float32)
x = paddle.to_tensor(x_data)
y = paddle.to_tensor(y_data)
z = paddle.matmul(x, y)
print(z.numpy().shape)
# [10]

# batched matrix * broadcasted vector
x_data = np.random.random([10, 5, 2]).astype(np.float32)
y_data = np.random.random([2]).astype(np.float32)
x = paddle.to_tensor(x_data)
y = paddle.to_tensor(y_data)
z = paddle.matmul(x, y)
print(z.numpy().shape)
# [10, 5]

# batched matrix * batched matrix
x_data = np.random.random([10, 5, 2]).astype(np.float32)
y_data = np.random.random([10, 2, 5]).astype(np.float32)
x = paddle.to_tensor(x_data)
y = paddle.to_tensor(y_data)
z = paddle.matmul(x, y)
print(z.numpy().shape)
# [10, 5, 5]

# batched matrix * broadcasted matrix
x_data = np.random.random([10, 1, 5, 2]).astype(np.float32)
y_data = np.random.random([1, 3, 2, 5]).astype(np.float32)
x = paddle.to_tensor(x_data)
y = paddle.to_tensor(y_data)
z = paddle.matmul(x, y)
print(z.numpy().shape)
# [10, 3, 5, 5]
import paddle

x = paddle.to_tensor([[1, 2, 3],
                      [1, 4, 9],
                      [1, 8, 27]], dtype='float64')
print(paddle.linalg.matrix_power(x, 2))
# [[6.  , 34. , 102.],
#  [14. , 90. , 282.],
#  [36. , 250., 804.]]

print(paddle.linalg.matrix_power(x, 0))
# [[1., 0., 0.],
#  [0., 1., 0.],
#  [0., 0., 1.]]

print(paddle.linalg.matrix_power(x, -2))
# [[ 12.91666667, -12.75000000,  2.83333333 ],
#  [-7.66666667 ,  8.         , -1.83333333 ],
#  [ 1.80555556 , -1.91666667 ,  0.44444444 ]]
import paddle

# data_x is a Tensor with shape [2, 4]
# the axis is a int element

x = paddle.to_tensor([[0.2, 0.3, 0.5, 0.9],
                      [0.1, 0.2, 0.6, 0.7]])
result1 = paddle.max(x)
print(result1)
#[0.9]
result2 = paddle.max(x, axis=0)
print(result2)
#[0.2 0.3 0.6 0.9]
result3 = paddle.max(x, axis=-1)
print(result3)
#[0.9 0.7]
result4 = paddle.max(x, axis=1, keepdim=True)
print(result4)
#[[0.9]
# [0.7]]

# data_y is a Tensor with shape [2, 2, 2]
# the axis is list

y = paddle.to_tensor([[[1.0, 2.0], [3.0, 4.0]],
                      [[5.0, 6.0], [7.0, 8.0]]])
result5 = paddle.max(y, axis=[1, 2])
print(result5)
#[4. 8.]
result6 = paddle.max(y, axis=[0, 1])
print(result6)
#[7. 8.]
import numpy as np
import paddle

x = paddle.to_tensor([[1, 2], [7, 8]])
y = paddle.to_tensor([[3, 4], [5, 6]])
res = paddle.maximum(x, y)
print(res)
#    [[3, 4],
#     [7, 8]]

x = paddle.to_tensor([[1, 2, 3], [1, 2, 3]])
y = paddle.to_tensor([3, 0, 4])
res = paddle.maximum(x, y)
print(res)
#    [[3, 2, 4],
#     [3, 2, 4]]

x = paddle.to_tensor([2, 3, 5], dtype='float32')
y = paddle.to_tensor([1, np.nan, np.nan], dtype='float32')
res = paddle.maximum(x, y)
print(res)
#    [ 2., nan, nan]

x = paddle.to_tensor([5, 3, np.inf], dtype='float32')
y = paddle.to_tensor([1, -np.inf, 5], dtype='float32')
res = paddle.maximum(x, y)
print(res)
#    [  5.,   3., inf.]
import paddle

x = paddle.to_tensor([[[1., 2., 3., 4.],
                       [5., 6., 7., 8.],
                       [9., 10., 11., 12.]],
                      [[13., 14., 15., 16.],
                       [17., 18., 19., 20.],
                       [21., 22., 23., 24.]]])
out1 = paddle.mean(x)
# [12.5]
out2 = paddle.mean(x, axis=-1)
# [[ 2.5  6.5 10.5]
#  [14.5 18.5 22.5]]
out3 = paddle.mean(x, axis=-1, keepdim=True)
# [[[ 2.5]
#   [ 6.5]
#   [10.5]]
#  [[14.5]
#   [18.5]
#   [22.5]]]
out4 = paddle.mean(x, axis=[0, 2])
# [ 8.5 12.5 16.5]
import paddle

x = paddle.arange(12).reshape([3, 4])
# x is [[0 , 1 , 2 , 3 ],
#       [4 , 5 , 6 , 7 ],
#       [8 , 9 , 10, 11]]

y1 = paddle.median(x)
# y1 is [5.5]

y2 = paddle.median(x, axis=0)
# y2 is [4., 5., 6., 7.]

y3 = paddle.median(x, axis=1)
# y3 is [1.5, 5.5, 9.5]

y4 = paddle.median(x, axis=0, keepdim=True)
# y4 is [[4., 5., 6., 7.]]
import paddle

# x is a tensor with shape [2, 4]
# the axis is a int element
x = paddle.to_tensor([[0.2, 0.3, 0.5, 0.9],
                      [0.1, 0.2, 0.6, 0.7]])
result1 = paddle.min(x)
print(result1)
#[0.1]
result2 = paddle.min(x, axis=0)
print(result2)
#[0.1 0.2 0.5 0.7]
result3 = paddle.min(x, axis=-1)
print(result3)
#[0.2 0.1]
result4 = paddle.min(x, axis=1, keepdim=True)
print(result4)
#[[0.2]
# [0.1]]

# y is a Tensor with shape [2, 2, 2]
# the axis is list
y = paddle.to_tensor([[[1.0, 2.0], [3.0, 4.0]],
                      [[5.0, 6.0], [7.0, 8.0]]])
result5 = paddle.min(y, axis=[1, 2])
print(result5)
#[1. 5.]
result6 = paddle.min(y, axis=[0, 1])
print(result6)
#[1. 2.]
import numpy as np
import paddle

x = paddle.to_tensor([[1, 2], [7, 8]])
y = paddle.to_tensor([[3, 4], [5, 6]])
res = paddle.minimum(x, y)
print(res)
#       [[1, 2],
#        [5, 6]]

x = paddle.to_tensor([[[1, 2, 3], [1, 2, 3]]])
y = paddle.to_tensor([3, 0, 4])
res = paddle.minimum(x, y)
print(res)
#       [[[1, 0, 3],
#         [1, 0, 3]]]

x = paddle.to_tensor([2, 3, 5], dtype='float32')
y = paddle.to_tensor([1, np.nan, np.nan], dtype='float32')
res = paddle.minimum(x, y)
print(res)
#       [ 1., nan, nan]

x = paddle.to_tensor([5, 3, np.inf], dtype='float64')
y = paddle.to_tensor([1, -np.inf, 5], dtype='float64')
res = paddle.minimum(x, y)
print(res)
#       [   1., -inf.,    5.]
import paddle
input = paddle.arange(1, 7).reshape((3, 2)).astype('float32')
mat2 = paddle.arange(1, 9).reshape((2, 4)).astype('float32')
out = paddle.mm(input, mat2)
print(out)
#        [[11., 14., 17., 20.],
#         [23., 30., 37., 44.],
#         [35., 46., 57., 68.]])
import paddle

x = paddle.to_tensor([2, 3, 8, 7])
y = paddle.to_tensor([1, 5, 3, 3])
z = paddle.remainder(x, y)
print(z)  # [0, 3, 2, 1]
import paddle
import numpy as np

# A * B
A_data = np.random.random([3, 4]).astype(np.float32)
B_data = np.random.random([4, 5]).astype(np.float32)
A = paddle.to_tensor(A_data)
B = paddle.to_tensor(B_data)
out = paddle.linalg.multi_dot([A, B])
print(out.numpy().shape)
# [3, 5]

# A * B * C
A_data = np.random.random([10, 5]).astype(np.float32)
B_data = np.random.random([5, 8]).astype(np.float32)
C_data = np.random.random([8, 7]).astype(np.float32)
A = paddle.to_tensor(A_data)
B = paddle.to_tensor(B_data)
C = paddle.to_tensor(C_data)
out = paddle.linalg.multi_dot([A, B, C])
print(out.numpy().shape)
# [10, 7]
import paddle
import numpy as np
img1 = np.array([[1, 2], [3, 4]]).astype(np.float32)
img2 = np.array([[5, 6], [7, 8]]).astype(np.float32)
inputs = [paddle.to_tensor(img1), paddle.to_tensor(img2)]
index = paddle.to_tensor(np.array([[1], [0]]).astype(np.int32))
res = paddle.multiplex(inputs, index)
print(res) # [array([[5., 6.], [3., 4.]], dtype=float32)]
import paddle

x = paddle.to_tensor([[1, 2], [3, 4]])
y = paddle.to_tensor([[5, 6], [7, 8]])
res = paddle.multiply(x, y)
print(res) # [[5, 12], [21, 32]]

x = paddle.to_tensor([[[1, 2, 3], [1, 2, 3]]])
y = paddle.to_tensor([2])
res = paddle.multiply(x, y)
print(res) # [[[2, 4, 6], [2, 4, 6]]]
# x: [M, N], vec: [N]
# paddle.mv(x, vec)  # out: [M]

import numpy as np
import paddle

x_data = np.array([[2, 1, 3], [3, 0, 1]]).astype("float64")
x = paddle.to_tensor(x_data)
vec_data = np.array([3, 5, 1])
vec = paddle.to_tensor(vec_data).astype("float64")
out = paddle.mv(x, vec)
import paddle

paddle.enable_static()

# create a static Variable
x = paddle.static.data(name='x', shape=[3, 2, 1])
# print the dimension of the Variable
print(x.ndim)
import paddle

x = paddle.to_tensor([-0.4, -0.2, 0.1, 0.3])
out = paddle.neg(x)
print(out)
# [0.4 0.2 -0.1 -0.3]
import paddle

x1 = paddle.to_tensor([[1.0, 0.0, 0.0],
                       [0.0, 2.0, 0.0],
                       [0.0, 0.0, 3.0]])
x2 = paddle.to_tensor([0.0, 1.0, 0.0, 3.0])
out_z1 = paddle.nonzero(x1)
print(out_z1)
#[[0 0]
# [1 1]
# [2 2]]
out_z1_tuple = paddle.nonzero(x1, as_tuple=True)
for out in out_z1_tuple:
    print(out)
#[[0]
# [1]
# [2]]
#[[0]
# [1]
# [2]]
out_z2 = paddle.nonzero(x2)
print(out_z2)
#[[1]
# [3]]
out_z2_tuple = paddle.nonzero(x2, as_tuple=True)
for out in out_z2_tuple:
    print(out)
#[[1]
# [3]]
import paddle
import numpy as np
shape=[2, 3, 4]
np_input = np.arange(24).astype('float32') - 12
np_input = np_input.reshape(shape)
x = paddle.to_tensor(np_input)
#[[[-12. -11. -10.  -9.] [ -8.  -7.  -6.  -5.] [ -4.  -3.  -2.  -1.]]
# [[  0.   1.   2.   3.] [  4.   5.   6.   7.] [  8.   9.  10.  11.]]]

# compute frobenius norm along last two dimensions.
out_fro = paddle.norm(x, p='fro', axis=[0,1])
# out_fro.numpy() [17.435596 16.911535 16.7332   16.911535]

# compute 2-order vector norm along last dimension.
out_pnorm = paddle.norm(x, p=2, axis=-1)
#out_pnorm.numpy(): [[21.118711  13.190906   5.477226]
#                    [ 3.7416575 11.224972  19.131126]]

# compute 2-order  norm along [0,1] dimension.
out_pnorm = paddle.norm(x, p=2, axis=[0,1])
#out_pnorm.numpy(): [17.435596 16.911535 16.7332   16.911535]

# compute inf-order  norm
out_pnorm = paddle.norm(x, p=np.inf)
#out_pnorm.numpy()  = [12.]
out_pnorm = paddle.norm(x, p=np.inf, axis=0)
#out_pnorm.numpy(): [[12. 11. 10. 9.] [8. 7. 6. 7.] [8. 9. 10. 11.]]

# compute -inf-order  norm
out_pnorm = paddle.norm(x, p=-np.inf)
#out_pnorm.numpy(): [0.]
out_pnorm = paddle.norm(x, p=-np.inf, axis=0)
#out_pnorm.numpy(): [[0. 1. 2. 3.] [4. 5. 6. 5.] [4. 3. 2. 1.]]
import paddle

x = paddle.to_tensor([1, 2, 3])
y = paddle.to_tensor([1, 3, 2])
result1 = paddle.not_equal(x, y)
print(result1)  # result1 = [False True True]
import paddle

x = paddle.full(shape=[4, 5, 7], fill_value=0, dtype='int32')
numel = paddle.numel(x) # 140
import paddle

x = paddle.to_tensor([1, 2, 3], dtype='float32')

# example 1: y is a float or int
res = paddle.pow(x, 2)
print(res)
# Tensor(shape=[3], dtype=float32, place=CUDAPlace(0), stop_gradient=True,
#        [1., 4., 9.])
res = paddle.pow(x, 2.5)
print(res)
# Tensor(shape=[3], dtype=float32, place=CUDAPlace(0), stop_gradient=True,
#        [1.         , 5.65685415 , 15.58845711])

# example 2: y is a Tensor
y = paddle.to_tensor([2], dtype='float32')
res = paddle.pow(x, y)
print(res)
# Tensor(shape=[3], dtype=float32, place=CUDAPlace(0), stop_gradient=True,
#        [1., 4., 9.])
import paddle

# the axis is a int element
x = paddle.to_tensor([[0.2, 0.3, 0.5, 0.9],
                      [0.1, 0.2, 0.6, 0.7]])
out1 = paddle.prod(x)
# [0.0002268]

out2 = paddle.prod(x, -1)
# [0.027  0.0084]

out3 = paddle.prod(x, 0)
# [0.02 0.06 0.3  0.63]

out4 = paddle.prod(x, 0, keepdim=True)
# [[0.02 0.06 0.3  0.63]]

out5 = paddle.prod(x, 0, dtype='int64')
# [0 0 0 0]

# the axis is list
y = paddle.to_tensor([[[1.0, 2.0], [3.0, 4.0]],
                      [[5.0, 6.0], [7.0, 8.0]]])
out6 = paddle.prod(y, [0, 1])
# [105. 384.]

out7 = paddle.prod(y, (1, 2))
# [  24. 1680.]
import paddle

x = paddle.to_tensor([[1.0, 2.0], [3.0, 4.0], [5.0, 6.0]]).astype('float64')
q, r = paddle.linalg.qr(x)
print (q)
print (r)

# Q = [[-0.16903085,  0.89708523],
#      [-0.50709255,  0.27602622],
#      [-0.84515425, -0.34503278]])

# R = [[-5.91607978, -7.43735744],
#      [ 0.        ,  0.82807867]])

# one can verify : X = Q * R ;
import paddle

input = paddle.rand((3, 100, 100))
rank = paddle.rank(input)
print(rank)
# 3
import paddle

x = paddle.to_tensor(
    [[1 + 6j, 2 + 5j, 3 + 4j], [4 + 3j, 5 + 2j, 6 + 1j]])
# Tensor(shape=[2, 3], dtype=complex64, place=CUDAPlace(0), stop_gradient=True,
#        [[(1+6j), (2+5j), (3+4j)],
#         [(4+3j), (5+2j), (6+1j)]])

real_res = paddle.real(x)
# Tensor(shape=[2, 3], dtype=float32, place=CUDAPlace(0), stop_gradient=True,
#        [[1., 2., 3.],
#         [4., 5., 6.]])

real_t = x.real()
# Tensor(shape=[2, 3], dtype=float32, place=CUDAPlace(0), stop_gradient=True,
#        [[1., 2., 3.],
#         [4., 5., 6.]])
import paddle

x = paddle.to_tensor([-0.4, -0.2, 0.1, 0.3])
out = paddle.reciprocal(x)
print(out)
# [-2.5        -5.         10.          3.33333333]
import paddle

x = paddle.to_tensor([2, 3, 8, 7])
y = paddle.to_tensor([1, 5, 3, 3])
z = paddle.remainder(x, y)
print(z)  # [0, 3, 2, 1]
import numpy as np
import paddle

x = paddle.rand([2, 4, 6], dtype="float32")
positive_four = paddle.full([1], 4, "int32")

out = paddle.reshape(x, [-1, 0, 3, 2])
print(out)
# the shape is [2,4,3,2].

out = paddle.reshape(x, shape=[positive_four, 12])
print(out)
# the shape of out_2 is [4, 12].

shape_tensor = paddle.to_tensor(np.array([8, 6]).astype("int32"))
out = paddle.reshape(x, shape=shape_tensor)
print(out)
# the shape is [8, 6].
# out shares data with x in dygraph mode
x[0, 0, 0] = 10.
print(out[0, 0])
# the value is [10.]
import paddle
import numpy as np

image_shape=(3, 2, 2)
x = np.arange(image_shape[0] * image_shape[1] * image_shape[2]).reshape(image_shape)
x = x.astype('float32')
img = paddle.to_tensor(x)
tmp = paddle.flip(img, [0,1])
print(tmp) # [[[10,11],[8, 9]], [[6, 7],[4, 5]], [[2, 3],[0, 1]]]

out = paddle.flip(tmp,-1)
print(out) # [[[11,10],[9, 8]], [[7, 6],[5, 4]], [[3, 2],[1, 0]]]
import paddle

x = paddle.to_tensor([[1.0, 2.0, 3.0],
                      [4.0, 5.0, 6.0],
                      [7.0, 8.0, 9.0]])
out_z1 = paddle.roll(x, shifts=1)
print(out_z1)
#[[9. 1. 2.]
# [3. 4. 5.]
# [6. 7. 8.]]
out_z2 = paddle.roll(x, shifts=1, axis=0)
print(out_z2)
#[[7. 8. 9.]
# [1. 2. 3.]
# [4. 5. 6.]]
import paddle

x = paddle.to_tensor([-0.5, -0.2, 0.6, 1.5])
out = paddle.round(x)
print(out)
# [-1. -0.  1.  2.]
import paddle

x = paddle.to_tensor([0.1, 0.2, 0.3, 0.4])
out = paddle.rsqrt(x)
print(out)
# [3.16227766 2.23606798 1.82574186 1.58113883]
# scale as a float32 number
import paddle

data = paddle.randn(shape=[2,3], dtype='float32')
res = paddle.scale(data, scale=2.0, bias=1.0)
# scale with parameter scale as a Tensor
import paddle

data = paddle.randn(shape=[2, 3], dtype='float32')
factor = paddle.to_tensor([2], dtype='float32')
res = paddle.scale(data, scale=factor, bias=1.0)
import numpy as np
#input:
x = np.array([[1, 1], [2, 2], [3, 3]])
index = np.array([2, 1, 0, 1])
# shape of updates should be the same as x
# shape of updates with dim > 1 should be the same as input
updates = np.array([[1, 1], [2, 2], [3, 3], [4, 4]])
overwrite = False
# calculation:
if not overwrite:
    for i in range(len(index)):
        x[index[i]] = np.zeros((2))
for i in range(len(index)):
    if (overwrite):
        x[index[i]] = updates[i]
    else:
        x[index[i]] += updates[i]
# output:
out = np.array([[3, 3], [6, 6], [1, 1]])
out.shape # [3, 2]
import paddle

x = paddle.to_tensor([[1, 1], [2, 2], [3, 3]], dtype='float32')
index = paddle.to_tensor([2, 1, 0, 1], dtype='int64')
updates = paddle.to_tensor([[1, 1], [2, 2], [3, 3], [4, 4]], dtype='float32')

output1 = paddle.scatter(x, index, updates, overwrite=False)
# [[3., 3.],
#  [6., 6.],
#  [1., 1.]]

output2 = paddle.scatter(x, index, updates, overwrite=True)
# CPU device:
# [[3., 3.],
#  [4., 4.],
#  [1., 1.]]
# GPU device maybe have two results because of the repeated numbers in index
# result 1:
# [[3., 3.],
#  [4., 4.],
#  [1., 1.]]
# result 2:
# [[3., 3.],
#  [2., 2.],
#  [1., 1.]]
import paddle
import numpy as np

index_data = np.array([[1, 1],
                       [0, 1],
                       [1, 3]]).astype(np.int64)
index = paddle.to_tensor(index_data)
updates = paddle.rand(shape=[3, 9, 10], dtype='float32')
shape = [3, 5, 9, 10]

output = paddle.scatter_nd(index, updates, shape)
import paddle
import numpy as np

x = paddle.rand(shape=[3, 5, 9, 10], dtype='float32')
updates = paddle.rand(shape=[3, 9, 10], dtype='float32')
index_data = np.array([[1, 1],
                       [0, 1],
                       [1, 3]]).astype(np.int64)
index = paddle.to_tensor(index_data)
output = paddle.scatter_nd_add(x, index, updates)
shard_size = (index_num + nshards - 1) // nshards
v = v - shard_id * shard_size if shard_id * shard_size <= v < (shard_id+1) * shard_size else ignore_value
import paddle
label = paddle.to_tensor([[16], [1]], "int64")
shard_label = paddle.shard_index(input=label,
                                 index_num=20,
                                 nshards=2,
                                 shard_id=0)
print(shard_label)
# [[-1], [1]]
import paddle

x = paddle.to_tensor([3.0, 0.0, -2.0, 1.7], dtype='float32')
out = paddle.sign(x=x)
print(out)  # [1.0, 0.0, -1.0, 1.0]
import paddle

x = paddle.to_tensor([-0.4, -0.2, 0.1, 0.3])
out = paddle.sin(x)
print(out)
# [-0.38941834 -0.19866933  0.09983342  0.29552021]
import paddle

x = paddle.to_tensor([-0.4, -0.2, 0.1, 0.3])
out = paddle.sinh(x)
print(out)
# [-0.41075233 -0.201336    0.10016675  0.30452029]
import paddle

input = paddle.rand(shape=[4, 5, 6], dtype='float32')
# example 1:
# attr starts is a list which doesn't contain tensor.
axes = [0, 1, 2]
starts = [-3, 0, 2]
ends = [3, 2, 4]
sliced_1 = paddle.slice(input, axes=axes, starts=starts, ends=ends)
# sliced_1 is input[0:3, 0:2, 2:4].

# example 2:
# attr starts is a list which contain tensor.
minus_3 = paddle.full([1], -3, "int32")
sliced_2 = paddle.slice(input, axes=axes, starts=[minus_3, 0, 2], ends=ends)
# sliced_2 is input[0:3, 0:2, 2:4].
import paddle

x = paddle.to_tensor([[[5,8,9,5],
                       [0,0,1,7],
                       [6,9,2,4]],
                      [[5,2,4,2],
                       [4,7,7,9],
                       [1,7,0,6]]],
                     dtype='float32')
out1 = paddle.sort(x=x, axis=-1)
out2 = paddle.sort(x=x, axis=0)
out3 = paddle.sort(x=x, axis=1)
print(out1)
#[[[5. 5. 8. 9.]
#  [0. 0. 1. 7.]
#  [2. 4. 6. 9.]]
# [[2. 2. 4. 5.]
#  [4. 7. 7. 9.]
#  [0. 1. 6. 7.]]]
print(out2)
#[[[5. 2. 4. 2.]
#  [0. 0. 1. 7.]
#  [1. 7. 0. 4.]]
# [[5. 8. 9. 5.]
#  [4. 7. 7. 9.]
#  [6. 9. 2. 6.]]]
print(out3)
#[[[0. 0. 1. 4.]
#  [5. 8. 2. 5.]
#  [6. 9. 9. 7.]]
# [[1. 2. 0. 2.]
#  [4. 7. 4. 6.]
#  [5. 7. 7. 9.]]]
import paddle

# x is a Tensor of shape [3, 9, 5]
x = paddle.rand([3, 9, 5])

out0, out1, out2 = paddle.split(x, num_or_sections=3, axis=1)
print(out0.shape)  # [3, 3, 5]
print(out1.shape)  # [3, 3, 5]
print(out2.shape)  # [3, 3, 5]

out0, out1, out2 = paddle.split(x, num_or_sections=[2, 3, 4], axis=1)
print(out0.shape)  # [3, 2, 5]
print(out1.shape)  # [3, 3, 5]
print(out2.shape)  # [3, 4, 5]

out0, out1, out2 = paddle.split(x, num_or_sections=[2, 3, -1], axis=1)
print(out0.shape)  # [3, 2, 5]
print(out1.shape)  # [3, 3, 5]
print(out2.shape)  # [3, 4, 5]

# axis is negative, the real axis is (rank(x) + axis)=1
out0, out1, out2 = paddle.split(x, num_or_sections=3, axis=-2)
print(out0.shape)  # [3, 3, 5]
print(out1.shape)  # [3, 3, 5]
print(out2.shape)  # [3, 3, 5]
import paddle

x = paddle.to_tensor([0.1, 0.2, 0.3, 0.4])
out = paddle.sqrt(x)
print(out)
# [0.31622777 0.4472136  0.54772256 0.63245553]
import paddle

x = paddle.to_tensor([-0.4, -0.2, 0.1, 0.3])
out = paddle.square(x)
print(out)
# [0.16 0.04 0.01 0.09]
import paddle

x = paddle.rand([5, 1, 10])
output = paddle.squeeze(x, axis=1)

print(x.shape)  # [5, 1, 10]
print(output.shape)  # [5, 10]

# output shares data with x in dygraph mode
x[0, 0, 0] = 10.
print(output[0, 0]) # [10.]
import paddle

x1 = paddle.to_tensor([[1.0, 2.0]])
x2 = paddle.to_tensor([[3.0, 4.0]])
x3 = paddle.to_tensor([[5.0, 6.0]])
out = paddle.stack([x1, x2, x3], axis=0)
print(out.shape)  # [3, 1, 2]
print(out)
# [[[1., 2.]],
#  [[3., 4.]],
#  [[5., 6.]]]
import paddle

x = paddle.to_tensor([1.0, 2.0, 3.0, 4.0])
out = paddle.stanh(x, scale_a=0.67, scale_b=1.72) # [1.00616539, 1.49927628, 1.65933108, 1.70390463]
import paddle

x = paddle.to_tensor([[1.0, 2.0, 3.0], [1.0, 4.0, 5.0]])
out1 = paddle.std(x)
# [1.63299316]
out2 = paddle.std(x, axis=1)
# [1.       2.081666]
import paddle
x = paddle.zeros(shape=[3,4,5,6], dtype="float32")
# example 1:
# attr starts is a list which doesn't contain Tensor.
axes = [1, 2, 3]
starts = [-3, 0, 2]
ends = [3, 2, 4]
strides_1 = [1, 1, 1]
strides_2 = [1, 1, 2]
sliced_1 = paddle.strided_slice(x, axes=axes, starts=starts, ends=ends, strides=strides_1)
# sliced_1 is x[:, 1:3:1, 0:2:1, 2:4:1].
# example 2:
# attr starts is a list which contain tensor Tensor.
minus_3 = paddle.full(shape=[1], fill_value=-3, dtype='int32')
sliced_2 = paddle.strided_slice(x, axes=axes, starts=[minus_3, 0, 2], ends=ends, strides=strides_2)
# sliced_2 is x[:, 1:3:1, 0:2:1, 2:4:2].
import numpy as np
import paddle

x = paddle.to_tensor([[1, 2], [7, 8]])
y = paddle.to_tensor([[5, 6], [3, 4]])
res = paddle.subtract(x, y)
print(res)
#       [[-4, -4],
#        [4, 4]]

x = paddle.to_tensor([[[1, 2, 3], [1, 2, 3]]])
y = paddle.to_tensor([1, 0, 4])
res = paddle.subtract(x, y)
print(res)
#       [[[ 0,  2, -1],
#         [ 0,  2, -1]]]

x = paddle.to_tensor([2, np.nan, 5], dtype='float32')
y = paddle.to_tensor([1, 4, np.nan], dtype='float32')
res = paddle.subtract(x, y)
print(res)
#       [ 1., nan, nan]

x = paddle.to_tensor([5, np.inf, -np.inf], dtype='float64')
y = paddle.to_tensor([1, 4, 5], dtype='float64')
res = paddle.subtract(x, y)
print(res)
#       [   4.,  inf., -inf.]
import paddle

# x is a Tensor with following elements:
#    [[0.2, 0.3, 0.5, 0.9]
#     [0.1, 0.2, 0.6, 0.7]]
# Each example is followed by the corresponding output tensor.
x = paddle.to_tensor([[0.2, 0.3, 0.5, 0.9],
                      [0.1, 0.2, 0.6, 0.7]])
out1 = paddle.sum(x)  # [3.5]
out2 = paddle.sum(x, axis=0)  # [0.3, 0.5, 1.1, 1.6]
out3 = paddle.sum(x, axis=-1)  # [1.9, 1.6]
out4 = paddle.sum(x, axis=1, keepdim=True)  # [[1.9], [1.6]]

# y is a Tensor with shape [2, 2, 2] and elements as below:
#      [[[1, 2], [3, 4]],
#      [[5, 6], [7, 8]]]
# Each example is followed by the corresponding output tensor.
y = paddle.to_tensor([[[1, 2], [3, 4]],
                      [[5, 6], [7, 8]]])
out5 = paddle.sum(y, axis=[1, 2]) # [10, 26]
out6 = paddle.sum(y, axis=[0, 1]) # [16, 20]

# x is a Tensor with following elements:
#    [[True, True, True, True]
#     [False, False, False, False]]
# Each example is followed by the corresponding output tensor.
x = paddle.to_tensor([[True, True, True, True],
                      [False, False, False, False]])
out7 = paddle.sum(x)  # [4]
out8 = paddle.sum(x, axis=0)  # [1, 1, 1, 1]
out9 = paddle.sum(x, axis=1)  # [4, 0]
import paddle
x = paddle.ones(shape=[2, 3], dtype='int32')
x_transposed = paddle.t(x)
print(x_transposed.shape)
# [3, 2]
import paddle

x = paddle.to_tensor([-0.4, -0.2, 0.1, 0.3])
out = paddle.tanh(x)
print(out)
# [-0.37994896 -0.19737532  0.09966799  0.29131261]
import paddle

data_type = 'float64'

# For two 2-d tensor x and y, the case axes=0 is equivalent to outer product.
# Note that tensordot supports empty axis sequence, so all the axes=0, axes=[], axes=[[]], and axes=[[],[]] are equivalent cases.
x = paddle.arange(4, dtype=data_type).reshape([2, 2])
y = paddle.arange(4, dtype=data_type).reshape([2, 2])
z = paddle.tensordot(x, y, axes=0)
# z = [[[[0., 0.],
#        [0., 0.]],
#
#       [[0., 1.],
#        [2., 3.]]],
#
#
#      [[[0., 2.],
#        [4., 6.]],
#
#       [[0., 3.],
#        [6., 9.]]]]


# For two 1-d tensor x and y, the case axes=1 is equivalent to inner product.
x = paddle.arange(10, dtype=data_type)
y = paddle.arange(10, dtype=data_type)
z1 = paddle.tensordot(x, y, axes=1)
z2 = paddle.dot(x, y)
# z1 = z2 = [285.]


# For two 2-d tensor x and y, the case axes=1 is equivalent to matrix multiplication.
x = paddle.arange(6, dtype=data_type).reshape([2, 3])
y = paddle.arange(12, dtype=data_type).reshape([3, 4])
z1 = paddle.tensordot(x, y, axes=1)
z2 = paddle.matmul(x, y)
# z1 = z2 =  [[20., 23., 26., 29.],
#             [56., 68., 80., 92.]]


# When axes is a 1-d int list, x and y will be contracted along the same given axes.
# Note that axes=[1, 2] is equivalent to axes=[[1, 2]], axes=[[1, 2], []], axes=[[1, 2], [1]], and axes=[[1, 2], [1, 2]].
x = paddle.arange(24, dtype=data_type).reshape([2, 3, 4])
y = paddle.arange(36, dtype=data_type).reshape([3, 3, 4])
z = paddle.tensordot(x, y, axes=[1, 2])
# z =  [[506. , 1298., 2090.],
#       [1298., 3818., 6338.]]


# When axes is a list containing two 1-d int list, the first will be applied to x and the second to y.
x = paddle.arange(60, dtype=data_type).reshape([3, 4, 5])
y = paddle.arange(24, dtype=data_type).reshape([4, 3, 2])
z = paddle.tensordot(x, y, axes=([1, 0], [0, 1]))
# z =  [[4400., 4730.],
#       [4532., 4874.],
#       [4664., 5018.],
#       [4796., 5162.],
#       [4928., 5306.]]


# Thanks to the support of axes expansion, axes=[[0, 1, 3, 4], [1, 0, 3, 4]] can be abbreviated as axes= [[0, 1, 3, 4], [1, 0]].
x = paddle.arange(720, dtype=data_type).reshape([2, 3, 4, 5, 6])
y = paddle.arange(720, dtype=data_type).reshape([3, 2, 4, 5, 6])
z = paddle.tensordot(x, y, axes=[[0, 1, 3, 4], [1, 0]])
# z = [[23217330., 24915630., 26613930., 28312230.],
#      [24915630., 26775930., 28636230., 30496530.],
#      [26613930., 28636230., 30658530., 32680830.],
#      [28312230., 30496530., 32680830., 34865130.]]
import paddle

data = paddle.to_tensor([1, 2, 3], dtype='int32')
out = paddle.tile(data, repeat_times=[2, 1])
np_out = out.numpy()
# [[1, 2, 3], [1, 2, 3]]

out = paddle.tile(data, repeat_times=[2, 2])
np_out = out.numpy()
# [[1, 2, 3, 1, 2, 3], [1, 2, 3, 1, 2, 3]]

repeat_times = paddle.to_tensor([2, 1], dtype='int32')
out = paddle.tile(data, repeat_times=repeat_times)
np_out = out.numpy()
# [[1, 2, 3], [1, 2, 3]]
import paddle

tensor_1 = paddle.to_tensor([1, 4, 5, 7])
value_1, indices_1 = paddle.topk(tensor_1, k=1)
print(value_1)
# [7]
print(indices_1)
# [3]
tensor_2 = paddle.to_tensor([[1, 4, 5, 7], [2, 6, 2, 5]])
value_2, indices_2 = paddle.topk(tensor_2, k=1)
print(value_2)
# [[7]
#  [6]]
print(indices_2)
# [[3]
#  [1]]
value_3, indices_3 = paddle.topk(tensor_2, k=1, axis=-1)
print(value_3)
# [[7]
#  [6]]
print(indices_3)
# [[3]
#  [1]]
value_4, indices_4 = paddle.topk(tensor_2, k=1, axis=0)
print(value_4)
# [[2 6 5 7]]
print(indices_4)
# [[1 1 0 0]]
import paddle

case1 = paddle.randn([2, 3])
case2 = paddle.randn([3, 10, 10])
case3 = paddle.randn([3, 10, 5, 10])
data1 = paddle.trace(case1) # data1.shape = [1]
data2 = paddle.trace(case2, offset=1, axis1=1, axis2=2) # data2.shape = [3]
data3 = paddle.trace(case3, offset=-3, axis1=1, axis2=-1) # data2.shape = [3, 5]
import paddle

x = paddle.randn([2, 3, 4])
x_transposed = paddle.transpose(x, perm=[1, 0, 2])
print(x_transposed.shape)
# [3L, 2L, 4L]
import paddle

input = paddle.rand([2,2],'float32')
print(input)
# Tensor(shape=[2, 2], dtype=float32, place=CUDAPlace(0), stop_gradient=True,
#         [[0.02331470, 0.42374918],
#         [0.79647720, 0.74970269]])

output = paddle.trunc(input)
print(output)
# Tensor(shape=[2, 2], dtype=float32, place=CUDAPlace(0), stop_gradient=True,
#         [[0., 0.],
#         [0., 0.]]))
import paddle
import numpy as np
# input is a variable which shape is [3, 4, 5]
np_input = np.random.rand(3, 4, 5).astype('float32')
input = paddle.to_tensor(np_input)
[x0, x1, x2] = paddle.unbind(input, axis=0)
# x0.shape [4, 5]
# x1.shape [4, 5]
# x2.shape [4, 5]
[x0, x1, x2, x3] = paddle.unbind(input, axis=1)
# x0.shape [3, 5]
# x1.shape [3, 5]
# x2.shape [3, 5]
# x3.shape [3, 5]
import paddle
# example:
x = paddle.ones(shape=[3, 4])
x.uniform_()
print(x)
# [[ 0.84524226,  0.6921872,   0.56528175,  0.71690357], # random
#  [-0.34646994, -0.45116323, -0.09902662, -0.11397249], # random
#  [ 0.433519,    0.39483607, -0.8660099,   0.83664286]] # random
import paddle

x = paddle.to_tensor([2, 3, 3, 1, 5, 3])
unique = paddle.unique(x)
np_unique = unique.numpy() # [1 2 3 5]
_, indices, inverse, counts = paddle.unique(x, return_index=True, return_inverse=True, return_counts=True)
np_indices = indices.numpy() # [3 0 1 4]
np_inverse = inverse.numpy() # [1 2 2 0 3 2]
np_counts = counts.numpy() # [1 1 3 1]

x = paddle.to_tensor([[2, 1, 3], [3, 0, 1], [2, 1, 3]])
unique = paddle.unique(x)
np_unique = unique.numpy() # [0 1 2 3]

unique = paddle.unique(x, axis=0)
np_unique = unique.numpy()
# [[2 1 3]
#  [3 0 1]]
import paddle

x = paddle.to_tensor([1, 1, 2, 2, 3, 1, 1, 2])
output = paddle.unique_consecutive(x) #
np_output = output.numpy() # [1 2 3 1 2]
_, inverse, counts = paddle.unique_consecutive(x, return_inverse=True, return_counts=True)
np_inverse = inverse.numpy() # [0 0 1 1 2 3 3 4]
np_counts = inverse.numpy() # [2 2 1 2 1]

x = paddle.to_tensor([[2, 1, 3], [3, 0, 1], [2, 1, 3], [2, 1, 3]])
output = paddle.unique_consecutive(x, axis=0) #
np_output = output.numpy() # [2 1 3 0 1 2 1 3 2 1 3]

x = paddle.to_tensor([[2, 1, 3], [3, 0, 1], [2, 1, 3], [2, 1, 3]])
output = paddle.unique_consecutive(x, axis=0) #
np_output = output.numpy()
# [[2 1 3]
#  [3 0 1]
#  [2 1 3]]
import paddle

x = paddle.rand([5, 10])
print(x.shape)  # [5, 10]

out1 = paddle.unsqueeze(x, axis=0)
print(out1.shape)  # [1, 5, 10]

out2 = paddle.unsqueeze(x, axis=[0, 2])
print(out2.shape)  # [1, 5, 1, 10]

axis = paddle.to_tensor([0, 1, 2])
out3 = paddle.unsqueeze(x, axis=axis)
print(out3.shape)  # [1, 1, 1, 5, 10]

# out1, out2, out3 share data with x in dygraph mode
x[0, 0] = 10.
print(out1[0, 0, 0]) # [10.]
print(out2[0, 0, 0, 0]) # [10.]
print(out3[0, 0, 0, 0, 0]) # [10.]
import paddle
x = paddle.ones(name='x', shape=[2, 3, 5], dtype='float32')  # create a tensor with shape=[2, 3, 5]
y = paddle.unstack(x, axis=1)  # unstack with second axis, which results 3 tensors with shape=[2, 5]
import paddle

x = paddle.to_tensor([[1.0, 2.0, 3.0], [1.0, 4.0, 5.0]])
out1 = paddle.var(x)
# [2.66666667]
out2 = paddle.var(x, axis=1)
# [1.         4.33333333]
import paddle

x = paddle.to_tensor([0.9383, 0.1983, 3.2, 1.2])
y = paddle.to_tensor([1.0, 1.0, 1.0, 1.0])
out = paddle.where(x>1, x, y)

print(out)
#out: [1.0, 1.0, 3.2, 1.2]
import paddle

paddle.enable_static()

data = paddle.static.data(name="data", shape=[3, 32, 32], dtype="float32")

fc = paddle.static.nn.fc(x=data,
                         size=1000,
                         weight_attr=paddle.static.WeightNormParamAttr(
                             dim=None,
                             name='weight_norm_param',
                             initializer=paddle.nn.initializer.Constant(1.0),
                             learning_rate=1.0,
                             regularizer=paddle.regularizer.L2Decay(0.1),
                             trainable=True,
                             do_model_average=False,
                             need_clip=True))
# required: xpu

import paddle
import paddle.static as static

paddle.enable_static()
xpu_places = static.xpu_places()
import paddle
include_dir = paddle.sysconfig.get_include()
import paddle
include_dir = paddle.sysconfig.get_lib()
import paddle
import numpy as np

original_tensor = paddle.ones([2, 2])
print("original tensor's dtype is: {}".format(original_tensor.dtype))
new_tensor = original_tensor.astype('float32')
print("new tensor's dtype is: {}".format(new_tensor.dtype))
import paddle
x = paddle.to_tensor(5., stop_gradient=False)
for i in range(5):
    y = paddle.pow(x, 4.0)
    y.backward()
    print("{}: {}".format(i, x.grad))
# 0: [500.]
# 1: [1000.]
# 2: [1500.]
# 3: [2000.]
# 4: [2500.]

x.clear_grad()
print("{}".format(x.grad))
# 0.

grad_tensor=paddle.to_tensor(2.)
for i in range(5):
    y = paddle.pow(x, 4.0)
    y.backward(grad_tensor)
    print("{}: {}".format(i, x.grad))
# 0: [1000.]
# 1: [2000.]
# 2: [3000.]
# 3: [4000.]
# 4: [5000.]
import paddle

tensor = paddle.to_tensor([0, 1, 2, 3, 4])

tensor.fill_(0)
print(tensor.tolist())   #[0, 0, 0, 0, 0]
.. code-block:: python
    import paddle
    x = paddle.ones((4, 3)) * 2
    x.fill_diagonal_(1.0)
    print(x.tolist())   #[[1.0, 2.0, 2.0], [2.0, 1.0, 2.0], [2.0, 2.0, 1.0], [2.0, 2.0, 2.0]]
import paddle

x = paddle.ones((4, 3)) * 2
y = paddle.ones((3,))
nx = x.fill_diagonal_tensor(y)
print(nx.tolist())   #[[1.0, 2.0, 2.0], [2.0, 1.0, 2.0], [2.0, 2.0, 1.0], [2.0, 2.0, 2.0]]
import paddle

x = paddle.ones((4, 3)) * 2
y = paddle.ones((3,))
x.fill_diagonal_tensor_(y)
print(x.tolist())   #[[1.0, 2.0, 2.0], [2.0, 1.0, 2.0], [2.0, 2.0, 1.0], [2.0, 2.0, 2.0]]
import paddle

x = paddle.to_tensor(5., stop_gradient=False)
y = paddle.pow(x, 4.0)
y.backward()
print("grad of x: {}".format(x.gradient()))
# [500.]
import paddle

x = paddle.to_tensor(1)
print(x.item())             #1
print(type(x.item()))       #<class 'int'>

x = paddle.to_tensor(1.0)
print(x.item())             #1.0
print(type(x.item()))       #<class 'float'>

x = paddle.to_tensor(True)
print(x.item())             #True
print(type(x.item()))       #<class 'bool'>

x = paddle.to_tensor(1+1j)
print(x.item())             #(1+1j)
print(type(x.item()))       #<class 'complex'>

x = paddle.to_tensor([[1.1, 2.2, 3.3]])
print(x.item(2))            #3.3
print(x.item(0, 2))         #3.3
import paddle

# hook function return None
def print_hook_fn(grad):
    print(grad)

# hook function return Tensor
def double_hook_fn(grad):
    grad = grad * 2
    return grad

x = paddle.to_tensor([0., 1., 2., 3.], stop_gradient=False)
y = paddle.to_tensor([4., 5., 6., 7.], stop_gradient=False)
z = paddle.to_tensor([1., 2., 3., 4.])

# one Tensor can register multiple hooks
h = x.register_hook(print_hook_fn)
x.register_hook(double_hook_fn)

w = x + y
# register hook by lambda function
w.register_hook(lambda grad: grad * 2)

o = z.matmul(w)
o.backward()
# print_hook_fn print content in backward
# Tensor(shape=[4], dtype=float32, place=CUDAPlace(0), stop_gradient=False,
#        [2., 4., 6., 8.])

print("w.grad:", w.grad) # w.grad: [1. 2. 3. 4.]
print("x.grad:", x.grad) # x.grad: [ 4.  8. 12. 16.]
print("y.grad:", y.grad) # y.grad: [2. 4. 6. 8.]

# remove hook
h.remove()
import paddle.fluid as fluid
from paddle.fluid.dygraph.base import to_variable
from paddle.fluid.dygraph import Linear
import numpy as np

data = np.ones([3, 1024], dtype='float32')
with fluid.dygraph.guard():
    linear = fluid.dygraph.Linear(1024, 4)
    t = to_variable(data)
    linear(t)  # call with default weight
    custom_weight = np.random.randn(1024, 4).astype("float32")
    linear.weight.set_value(custom_weight)  # change existing weight
    out = linear(t)  # call with different weight
import paddle
# example:
x = paddle.ones(shape=[3, 4])
x.uniform_()
print(x)
# [[ 0.84524226,  0.6921872,   0.56528175,  0.71690357], # random
#  [-0.34646994, -0.45116323, -0.09902662, -0.11397249], # random
#  [ 0.433519,    0.39483607, -0.8660099,   0.83664286]] # random
import paddle

tensor = paddle.to_tensor([0, 1, 2, 3, 4])

tensor.zero_()
print(tensor.tolist())   #[0, 0, 0, 0, 0]
import paddle
from paddle.text.datasets import Conll05st

class SimpleNet(paddle.nn.Layer):
    def __init__(self):
        super(SimpleNet, self).__init__()

    def forward(self, pred_idx, mark, label):
        return paddle.sum(pred_idx), paddle.sum(mark), paddle.sum(label)


conll05st = Conll05st()

for i in range(10):
    pred_idx, mark, label= conll05st[i][-3:]
    pred_idx = paddle.to_tensor(pred_idx)
    mark = paddle.to_tensor(mark)
    label = paddle.to_tensor(label)

    model = SimpleNet()
    pred_idx, mark, label= model(pred_idx, mark, label)
    print(pred_idx.numpy(), mark.numpy(), label.numpy())
from paddle.text.datasets import Conll05st

conll05st = Conll05st()
word_dict, predicate_dict, label_dict = conll05st.get_dict()
from paddle.text.datasets import Conll05st

conll05st = Conll05st()
emb_file = conll05st.get_embedding()
import paddle
from paddle.text.datasets import Imdb

class SimpleNet(paddle.nn.Layer):
    def __init__(self):
        super(SimpleNet, self).__init__()

    def forward(self, doc, label):
        return paddle.sum(doc), label


imdb = Imdb(mode='train')

for i in range(10):
    doc, label = imdb[i]
    doc = paddle.to_tensor(doc)
    label = paddle.to_tensor(label)

    model = SimpleNet()
    image, label = model(doc, label)
    print(doc.numpy().shape, label.numpy().shape)
import paddle
from paddle.text.datasets import Imikolov

class SimpleNet(paddle.nn.Layer):
    def __init__(self):
        super(SimpleNet, self).__init__()

    def forward(self, src, trg):
        return paddle.sum(src), paddle.sum(trg)


imikolov = Imikolov(mode='train', data_type='SEQ', window_size=2)

for i in range(10):
    src, trg = imikolov[i]
    src = paddle.to_tensor(src)
    trg = paddle.to_tensor(trg)

    model = SimpleNet()
    src, trg = model(src, trg)
    print(src.numpy().shape, trg.numpy().shape)
import paddle
from paddle.text.datasets import Movielens

class SimpleNet(paddle.nn.Layer):
    def __init__(self):
        super(SimpleNet, self).__init__()

    def forward(self, category, title, rating):
        return paddle.sum(category), paddle.sum(title), paddle.sum(rating)


movielens = Movielens(mode='train')

for i in range(10):
    category, title, rating = movielens[i][-3:]
    category = paddle.to_tensor(category)
    title = paddle.to_tensor(title)
    rating = paddle.to_tensor(rating)

    model = SimpleNet()
    category, title, rating = model(category, title, rating)
    print(category.numpy().shape, title.numpy().shape, rating.numpy().shape)
import paddle
from paddle.text.datasets import UCIHousing

class SimpleNet(paddle.nn.Layer):
    def __init__(self):
        super(SimpleNet, self).__init__()

    def forward(self, feature, target):
        return paddle.sum(feature), target

paddle.disable_static()

uci_housing = UCIHousing(mode='train')

for i in range(10):
    feature, target = uci_housing[i]
    feature = paddle.to_tensor(feature)
    target = paddle.to_tensor(target)

    model = SimpleNet()
    feature, target = model(feature, target)
    print(feature.numpy().shape, target.numpy())
import paddle
paddle.seed(102)
batch_size, seq_len, num_tags = 2, 4, 3
emission = paddle.rand((batch_size, seq_len, num_tags), dtype='float32')
length = paddle.randint(1, seq_len + 1, [batch_size])
tags = paddle.randint(0, num_tags, [batch_size, seq_len])
transition = paddle.rand((num_tags, num_tags), dtype='float32')
scores, path = paddle.text.viterbi_decode(emission, transition, length, False) # scores: [3.37089300, 1.56825531], path: [[1, 0, 0], [1, 1, 0]]
import paddle
paddle.seed(102)
batch_size, seq_len, num_tags = 2, 4, 3
emission = paddle.rand((batch_size, seq_len, num_tags), dtype='float32')
length = paddle.randint(1, seq_len + 1, [batch_size])
tags = paddle.randint(0, num_tags, [batch_size, seq_len])
transition = paddle.rand((num_tags, num_tags), dtype='float32')
decoder = paddle.text.ViterbiDecoder(transition, include_bos_eos_tag=False)
scores, path = decoder(emission, length) # scores: [3.37089300, 1.56825531], path: [[1, 0, 0], [1, 1, 0]]
import paddle

class MyLayer(paddle.nn.Layer):
    def __init__(self):
        super(MyLayer, self).__init__()
        self._linear = paddle.nn.Linear(1, 1)
        w_tmp = self.create_parameter([1,1])
        self.add_parameter("w_tmp", w_tmp)

    def forward(self, input):
        return self._linear(input)

mylayer = MyLayer()
for name, param in mylayer.named_parameters():
    print(name, param)      # will print w_tmp,_linear.weight,_linear.bias
import paddle

class MySequential(paddle.nn.Layer):
    def __init__(self, *layers):
        super(MySequential, self).__init__()
        if len(layers) > 0 and isinstance(layers[0], tuple):
            for name, layer in layers:
                self.add_sublayer(name, layer)
        else:
            for idx, layer in enumerate(layers):
                self.add_sublayer(str(idx), layer)

    def forward(self, input):
        for layer in self._sub_layers.values():
            input = layer(input)
        return input

fc1 = paddle.nn.Linear(10, 3)
fc2 = paddle.nn.Linear(3, 10, bias_attr=False)
model = MySequential(fc1, fc2)
for prefix, layer in model.named_sublayers():
    print(prefix, layer)
import paddle
import paddle.nn as nn

net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2))

def init_weights(layer):
    if type(layer) == nn.Linear:
        print('before init weight:', layer.weight.numpy())
        new_weight = paddle.full(shape=layer.weight.shape, dtype=layer.weight.dtype, fill_value=0.9)
        layer.weight.set_value(new_weight)
        print('after init weight:', layer.weight.numpy())

net.apply(init_weights)

print(net.state_dict())
import numpy as np
import paddle

linear = paddle.nn.Linear(10, 3)
value = np.array([0]).astype("float32")
buffer = paddle.to_tensor(value)
linear.register_buffer("buf_name", buffer, persistable=True)

print(linear.buffers())     # == print([linear.buf_name])
import paddle

linear1 = paddle.nn.Linear(10, 3)
linear2 = paddle.nn.Linear(3, 10, bias_attr=False)
model = paddle.nn.Sequential(linear1, linear2)

layer_list = list(model.children())

print(layer_list)   # [<paddle.nn.layer.common.Linear object at 0x7f7b8113f830>, <paddle.nn.layer.common.Linear object at 0x7f7b8113f950>]
import paddle
import numpy as np

value = np.arange(26).reshape(2, 13).astype("float32")
a = paddle.to_tensor(value)
linear = paddle.nn.Linear(13, 5)
adam = paddle.optimizer.Adam(learning_rate=0.01,
                            parameters=linear.parameters())
out = linear(a)
out.backward()
adam.step()
linear.clear_gradients()
import paddle

class MyLayer(paddle.nn.Layer):
    def __init__(self):
        super(MyLayer, self).__init__()
        self._linear = paddle.nn.Linear(1, 1)
        w_tmp = self.create_parameter([1,1])
        self.add_parameter("w_tmp", w_tmp)

    def forward(self, input):
        return self._linear(input)

mylayer = MyLayer()
for name, param in mylayer.named_parameters():
    print(name, param)      # will print w_tmp,_linear.weight,_linear.bias
import paddle

class MyLinear(paddle.nn.Layer):
    def __init__(self,
                in_features,
                out_features):
        super(MyLinear, self).__init__()
        self.linear = paddle.nn.Linear( 10, 10)

        self.back_var = self.create_tensor(name = "linear_tmp_0", dtype=self._dtype)

    def forward(self, input):
        out = self.linear(input)
        paddle.assign( out, self.back_var)

        return out
import paddle

class MyLinear(paddle.nn.Layer):
    def __init__(self,
                in_features,
                out_features):
        super(MyLinear, self).__init__()
        self.linear = paddle.nn.Linear( 10, 10)

        self.back_var = self.create_variable(name = "linear_tmp_0", dtype=self._dtype)

    def forward(self, input):
        out = self.linear(input)
        paddle.assign( out, self.back_var)

        return out
import paddle

class MyLayer(paddle.nn.Layer):
    def __init__(self):
        super(MyLayer, self).__init__()
        self._linear = paddle.nn.Linear(1, 1)
        self._dropout = paddle.nn.Dropout(p=0.5)

    def forward(self, input):
        temp = self._linear(input)
        temp = self._dropout(temp)
        return temp

x = paddle.randn([10, 1], 'float32')
mylayer = MyLayer()
mylayer.eval()  # set mylayer._dropout to eval mode
out = mylayer(x)
print(out)
import paddle

class LinearNet(paddle.nn.Layer):
    def __init__(self):
        super(LinearNet, self).__init__(name_scope = "demo_linear_net")
        self._linear = paddle.nn.Linear(1, 1)

    def forward(self, x):
        return self._linear(x)

linear_net = LinearNet()
print(linear_net.full_name())   # demo_linear_net_0
import paddle

emb = paddle.nn.Embedding(10, 10)

state_dict = emb.state_dict()
paddle.save(state_dict, "paddle_dy.pdparams")
para_state_dict = paddle.load("paddle_dy.pdparams")
emb.set_state_dict(para_state_dict)
import numpy as np
import paddle

fc1 = paddle.nn.Linear(10, 3)
buffer1 = paddle.to_tensor(np.array([0]).astype("float32"))
# register a tensor as buffer by specific `persistable`
fc1.register_buffer("buf_name_1", buffer1, persistable=True)

fc2 = paddle.nn.Linear(3, 10)
buffer2 = paddle.to_tensor(np.array([1]).astype("float32"))
# register a buffer by assigning an attribute with Tensor.
# The `persistable` can only be False by this way.
fc2.buf_name_2 = buffer2

model = paddle.nn.Sequential(fc1, fc2)

# get all named buffers
for name, buffer in model.named_buffers():
    print(name, buffer)
import paddle

linear1 = paddle.nn.Linear(10, 3)
linear2 = paddle.nn.Linear(3, 10, bias_attr=False)
model = paddle.nn.Sequential(linear1, linear2)
for prefix, layer in model.named_children():
    print(prefix, layer)
    # ('0', <paddle.nn.layer.common.Linear object at 0x7fb61ed85830>)
    # ('1', <paddle.nn.layer.common.Linear object at 0x7fb61ed85950>)
import paddle

fc1 = paddle.nn.Linear(10, 3)
fc2 = paddle.nn.Linear(3, 10, bias_attr=False)
model = paddle.nn.Sequential(fc1, fc2)
for name, param in model.named_parameters():
    print(name, param)
import paddle

fc1 = paddle.nn.Linear(10, 3)
fc2 = paddle.nn.Linear(3, 10, bias_attr=False)
model = paddle.nn.Sequential(fc1, fc2)
for prefix, layer in model.named_sublayers():
    print(prefix, layer)

import numpy as np
import paddle

linear = paddle.nn.Linear(10, 3)
value = np.array([0]).astype("float32")
buffer = paddle.to_tensor(value)
linear.register_buffer("buf_name", buffer, persistable=True)

# get the buffer by attribute.
print(linear.buf_name)
import paddle
import numpy as np

# the forward_post_hook change the output of the layer: output = output * 2
def forward_post_hook(layer, input, output):
    # user can use layer, input and output for information statistis tasks

    # change the output
    return output * 2

linear = paddle.nn.Linear(13, 5)

# register the hook
forward_post_hook_handle = linear.register_forward_post_hook(forward_post_hook)

value1 = np.arange(26).reshape(2, 13).astype("float32")
in1 = paddle.to_tensor(value1)

out0 = linear(in1)

# remove the hook
forward_post_hook_handle.remove()

out1 = linear(in1)

# hook change the linear's output to output * 2, so out0 is equal to out1 * 2.
assert (out0.numpy() == (out1.numpy()) * 2).any()
import paddle
import numpy as np

# the forward_post_hook change the input of the layer: input = input * 2
def forward_pre_hook(layer, input):
    # user can use layer and input for information statistis tasks

    # change the input
    input_return = (input[0] * 2)
    return input_return

linear = paddle.nn.Linear(13, 5)

# register the hook
forward_pre_hook_handle = linear.register_forward_pre_hook(forward_pre_hook)

value0 = np.arange(26).reshape(2, 13).astype("float32")
in0 = paddle.to_tensor(value0)
out0 = linear(in0)

# remove the hook
forward_pre_hook_handle.remove()

value1 = value0 * 2
in1 = paddle.to_tensor(value1)
out1 = linear(in1)

# hook change the linear's input to input * 2, so out0 is equal to out1.
assert (out0.numpy() == out1.numpy()).any()
import paddle

emb = paddle.nn.Embedding(10, 10)

state_dict = emb.state_dict()
paddle.save(state_dict, "paddle_dy.pdparams")
para_state_dict = paddle.load("paddle_dy.pdparams")
emb.set_state_dict(para_state_dict)
import paddle

emb = paddle.nn.Embedding(10, 10)

state_dict = emb.state_dict()
paddle.save(state_dict, "paddle_dy.pdparams")
para_state_dict = paddle.load("paddle_dy.pdparams")
emb.set_state_dict(para_state_dict)
import paddle

emb = paddle.nn.Embedding(10, 10)

state_dict = emb.state_dict()
paddle.save( state_dict, "paddle_dy.pdparams")
import paddle

class MyLayer(paddle.nn.Layer):
    def __init__(self):
        super(MyLayer, self).__init__()
        self._linear = paddle.nn.Linear(1, 1)
        self._dropout = paddle.nn.Dropout(p=0.5)

    def forward(self, input):
        temp = self._linear(input)
        temp = self._dropout(temp)
        return temp

mylayer = MyLayer()
print(mylayer.sublayers())  # [<paddle.nn.layer.common.Linear object at 0x7f44b58977d0>, <paddle.nn.layer.common.Dropout object at 0x7f44b58978f0>]
# required: skip
import paddle

linear=paddle.nn.Linear(2, 2)
linear.weight
#Parameter containing:
#Tensor(shape=[2, 2], dtype=float32, place=CUDAPlace(0), stop_gradient=False,
#       [[-0.32770029,  0.38653070],
#        [ 0.46030545,  0.08158520]])

linear.to(dtype='float64')
linear.weight
#Tenor(shape=[2, 2], dtype=float64, place=CUDAPlace(0), stop_gradient=False,
#       [[-0.32770029,  0.38653070],
#        [ 0.46030545,  0.08158520]])

linear.to(device='cpu')
linear.weight
#Tensor(shape=[2, 2], dtype=float64, place=CPUPlace, stop_gradient=False,
#       [[-0.32770029,  0.38653070],
#        [ 0.46030545,  0.08158520]])
linear.to(device=paddle.CUDAPinnedPlace(), blocking=False)
linear.weight
#Tensor(shape=[2, 2], dtype=float64, place=CUDAPinnedPlace, stop_gradient=False,
#       [[-0.04989364, -0.56889004],
#        [ 0.33960250,  0.96878713]])
import paddle

emb = paddle.nn.Embedding(10, 10)

state_dict = emb.to_static_state_dict()
paddle.save( state_dict, "paddle_dy.pdparams")
import paddle

class MyLayer(paddle.nn.Layer):
    def __init__(self):
        super(MyLayer, self).__init__()
        self._linear = paddle.nn.Linear(1, 1)
        self._dropout = paddle.nn.Dropout(p=0.5)

    def forward(self, input):
        temp = self._linear(input)
        temp = self._dropout(temp)
        return temp

x = paddle.randn([10, 1], 'float32')
mylayer = MyLayer()
mylayer.eval()  # set mylayer._dropout to eval mode
out = mylayer(x)
mylayer.train()  # set mylayer._dropout to train mode
out = mylayer(x)
import paddle
from paddle.text.datasets import WMT14

class SimpleNet(paddle.nn.Layer):
    def __init__(self):
        super(SimpleNet, self).__init__()

    def forward(self, src_ids, trg_ids, trg_ids_next):
        return paddle.sum(src_ids), paddle.sum(trg_ids), paddle.sum(trg_ids_next)

wmt14 = WMT14(mode='train', dict_size=50)

for i in range(10):
    src_ids, trg_ids, trg_ids_next = wmt14[i]
    src_ids = paddle.to_tensor(src_ids)
    trg_ids = paddle.to_tensor(trg_ids)
    trg_ids_next = paddle.to_tensor(trg_ids_next)

    model = SimpleNet()
    src_ids, trg_ids, trg_ids_next = model(src_ids, trg_ids, trg_ids_next)
    print(src_ids.numpy(), trg_ids.numpy(), trg_ids_next.numpy())
from paddle.text.datasets import WMT14
wmt14 = WMT14(mode='train', dict_size=50)
src_dict, trg_dict = wmt14.get_dict()
import paddle
from paddle.text.datasets import WMT16

class SimpleNet(paddle.nn.Layer):
    def __init__(self):
        super(SimpleNet, self).__init__()

    def forward(self, src_ids, trg_ids, trg_ids_next):
        return paddle.sum(src_ids), paddle.sum(trg_ids), paddle.sum(trg_ids_next)

paddle.disable_static()

wmt16 = WMT16(mode='train', src_dict_size=50, trg_dict_size=50)

for i in range(10):
    src_ids, trg_ids, trg_ids_next = wmt16[i]
    src_ids = paddle.to_tensor(src_ids)
    trg_ids = paddle.to_tensor(trg_ids)
    trg_ids_next = paddle.to_tensor(trg_ids_next)

    model = SimpleNet()
    src_ids, trg_ids, trg_ids_next = model(src_ids, trg_ids, trg_ids_next)
    print(src_ids.numpy(), trg_ids.numpy(), trg_ids_next.numpy())
from paddle.text.datasets import WMT16
wmt16 = WMT16(mode='train', src_dict_size=50, trg_dict_size=50)
en_dict = wmt16.get_dict('en')
import paddle.fluid as fluid

# any version >= 0.1.0 is acceptable.
fluid.require_version('0.1.0')

# if 0.1.0 <= version <= 10.0.0, it is acceptable.
fluid.require_version(min_version='0.1.0', max_version='10.0.0')
import paddle

paddle.utils.run_check()
# Running verify PaddlePaddle program ...
# W1010 07:21:14.972093  8321 device_context.cc:338] Please NOTE: device: 0, CUDA Capability: 70, Driver API Version: 11.0, Runtime API Version: 10.1
# W1010 07:21:14.979770  8321 device_context.cc:346] device: 0, cuDNN Version: 7.6.
# PaddlePaddle works well on 1 GPU.
# PaddlePaddle works well on 8 GPUs.
# PaddlePaddle is installed successfully! Let's start deep learning with PaddlePaddle now.
import paddle

paddle.version.cuda()
# '10.2'
import paddle

paddle.version.cudnn()
# '7.6.5'
from paddle.vision import get_image_backend

backend = get_image_backend()
print(backend)
import numpy as np
from PIL import Image
from paddle.vision import image_load, set_image_backend

fake_img = Image.fromarray((np.random.random((32, 32, 3)) * 255).astype('uint8'))

path = 'temp.png'
fake_img.save(path)

set_image_backend('pil')

pil_img = image_load(path).convert('RGB')

# should be PIL.Image.Image
print(type(pil_img))

# use opencv as backend
# set_image_backend('cv2')

# np_img = image_load(path)
# # should get numpy.ndarray
# print(type(np_img))
import os
import shutil
import tempfile
import numpy as np
from PIL import Image

from paddle.vision import DatasetFolder
from paddle.vision import set_image_backend

set_image_backend('pil')

def make_fake_dir():
    data_dir = tempfile.mkdtemp()

    for i in range(2):
        sub_dir = os.path.join(data_dir, 'class_' + str(i))
        if not os.path.exists(sub_dir):
            os.makedirs(sub_dir)
        for j in range(2):
            fake_img = Image.fromarray((np.random.random((32, 32, 3)) * 255).astype('uint8'))
            fake_img.save(os.path.join(sub_dir, str(j) + '.png'))
    return data_dir

temp_dir = make_fake_dir()

pil_data_folder = DatasetFolder(temp_dir)

for items in pil_data_folder:
    break

# should get PIL.Image.Image
print(type(items[0]))

# use opencv as backend
# set_image_backend('cv2')

# cv2_data_folder = DatasetFolder(temp_dir)

# for items in cv2_data_folder:
#     break

# should get numpy.ndarray
# print(type(items[0]))

shutil.rmtree(temp_dir)
